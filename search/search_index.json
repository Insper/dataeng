{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"Engenharia de Dados (2025/2)","text":"<p>Seja bem-vindo ao material do curso de Engenharia de Dados (2025/2). Aqui voc\u00ea encontrar\u00e1 todo o material de apoio para o curso, incluindo links para a entrega das atividades pr\u00e1ticas.</p>"},{"location":"about/#sobre-este-curso","title":"Sobre este Curso","text":"<p>Engenharia de dados \u00e9 a \u00e1rea respons\u00e1vel por projetar, construir e manter sistemas e pipelines que coletam, armazenam, processam e disponibilizam dados de forma eficiente, segura e escal\u00e1vel. Este ser\u00e1 o nosso foco de estudo durante o curso!</p>"},{"location":"about/#links-importantes","title":"Links importantes","text":"<ul> <li>Blackboard: utilizado principalmente para o envio de notifica\u00e7\u00f5es (e-mails).</li> </ul> <ul> <li>Calend\u00e1rio: calend\u00e1rio do Insper (pode estar desatualizado).</li> </ul>"},{"location":"about/#horarios","title":"Hor\u00e1rios","text":"<ul> <li>Segunda-feira: 16:30 - 18:30</li> <li>Quarta-feira: 14:15 - 16:15</li> <li>Plant\u00e3o de d\u00favidas quarta-feira das 12:35 \u00e0s 14:05 (Teams)</li> </ul> <p>Nossos encontros ser\u00e3o presenciais. A primeira aula ser\u00e1 dia 11/08 e a \u00faltima aula ser\u00e1 dia 10/11, mas poder\u00e3o haver entregas ap\u00f3s esta data.</p>"},{"location":"about/#requirementstxt","title":"Requirements.txt!","text":"<p>Para que voc\u00ea aproveite ao m\u00e1ximo as atividades do curso, \u00e9 obrigat\u00f3rio ter conhecimento em:</p> <ul> <li>Bancos de dados relacionais e n\u00e3o relacionais</li> <li>Programa\u00e7\u00e3o avan\u00e7ada</li> <li>Computa\u00e7\u00e3o em nuvem</li> <li>Git</li> <li>Ferramentas de linha de comando</li> </ul> <p>As aulas exigem bastante autonomia. \u00c9 recomend\u00e1vel j\u00e1 ter cursado as disciplinas do Insper de: - Megadados - Computa\u00e7\u00e3o em nuvem (se ENGCOMP) - Sprint sessions (se BCC)</p>"},{"location":"about/#dinamica-das-aulas","title":"Din\u00e2mica das aulas","text":"<p>No inicio de cada aula, poder\u00e1 ocorrer uma se\u00e7\u00e3o expositiva, para introduzir os conceitos que ser\u00e3o trabalhados. Em seguida, seguiremos de forma predominantemente ativa, com a utiliza\u00e7\u00e3o de active handouts. \u00c9 esperado que voc\u00ea interaja com o material para consolidar o aprendizado:</p> <ul> <li>Ler e explorar o material;</li> <li>Executar atividades;</li> <li>Testar e validar respostas (escritas e de c\u00f3digo);</li> <li>Refletir e anotar.</li> </ul>"},{"location":"about/#entregas","title":"Entregas","text":"<p>Os alunos precisar\u00e3o entregar algumas atividades:</p> <ul> <li><code>APS</code>: atividades pr\u00e1ticas desenvolvidas durante e ap\u00f3s as aulas.</li> <li><code>INT</code>: algumas aulas ser\u00e3o focadas em entrevistas t\u00e9cnicas para vagas em engenharia de dados. Ser\u00e1 exigida prepara\u00e7\u00e3o pr\u00e9via dos alunos, que ir\u00e3o se entrevistar durante essas aulas.</li> <li><code>PRO</code>: nas \u00faltimas aulas, os alunos dever\u00e3o aplicar o conhecimento adquirido em um projeto aberto envolvendo temas correlatos ao curso.</li> </ul>"},{"location":"about/#provas","title":"Provas","text":"<p>As atividades de entrevista (<code>INT</code>) s\u00e3o como provas. Fora isto, n\u00e3o teremos provas, nem aulas durante as semanas de avalia\u00e7\u00e3o intermedi\u00e1ria e final do calend\u00e1rio do Insper.</p>"},{"location":"about/#nota-final","title":"Nota final","text":"<p>A nota final \u00e9 calculada com a seguinte f\u00f3rmula</p> <pre><code>NF = 0.35*APS + 0.25*INT + 0.40*PRO\n</code></pre> <p>Algumas condi\u00e7\u00f5es s\u00e3o necess\u00e1rias para aprova\u00e7\u00e3o:</p> <ul> <li><code>NF</code> maior que ou igual a <code>5.0</code>.</li> <li>M\u00e9dia maior que ou igual a <code>5.0</code> em APS, INT e PRO.</li> <li>No m\u00e1ximo duas atividades (entrevistas + APS) com nota zero ou n\u00e3o entregues.</li> </ul> <p>Se algum desses crit\u00e9rios n\u00e3o for atendido, o aluno ser\u00e1 reprovado na disciplina!</p>"},{"location":"contributions/","title":"Contribui\u00e7\u00f5es","text":"<p>Obrigado a todos que contribu\u00edram com o curso de Data Engineering!</p> <ul> <li>2025/2:<ul> <li>Maciel: Cria\u00e7\u00e3o do curso de Data Engineering.</li> </ul> </li> </ul>"},{"location":"deadlines/","title":"Prazos","text":"<p>Cada atividade detalha como sua entrega deve ser feita. Geralmente, basta fazer o commit no GitHub.</p> <p>O prazo considera o final do dia (23:59:59), a menos que especificado de outra forma.</p> Data de In\u00edcio Atividade Prazo"},{"location":"classes/01-intro/intro/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/01-intro/intro/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Nos \u00faltimos dez anos, a quantidade de dados gerados cresceu de forma exponencial, ultrapassando 30 mil gigabytes por segundo, e esse ritmo continua a acelerar (DDIA).</p> <p>Info</p> <p>As refer\u00eancias de cada aula sempre estar\u00e3o listadas ao final da \u00faltima p\u00e1gina da aula.</p> <p>Aposto que voc\u00ea j\u00e1 cursou outras disciplina com a palavra \"Dado\" no t\u00edtulo! Ent\u00e3o, tente relembrar seus aprendizados e responder o exerc\u00edcio na sequ\u00eancia.</p> <p>Exercise</p> <p>Defina, com suas palavras, o conceito de \"Dado\".</p> Submit <p>Answer</p> <p>Um dado \u00e9 a representa\u00e7\u00e3o bruta e n\u00e3o processada de um fato, evento ou conceito. Ele \u00e9 a unidade mais b\u00e1sica, isolada, que por si s\u00f3 n\u00e3o carrega um significado completo ou contexto. Pense no dado como a mat\u00e9ria-prima de todo o conhecimento.</p> <p>Em um contexto de computa\u00e7\u00e3o, dados podem ser:</p> <ul> <li>Valores num\u00e9ricos: \"150\", \"3.14\"</li> <li>Caracteres: \"a\", \"Z\", \"@\"</li> <li>Strings: \"Jos\u00e9 da Silva\", \"endere\u00e7o@email.com\"</li> <li>S\u00edmbolos: como os de um c\u00f3digo de barras.</li> </ul> <p>De forma isolada, um dado como <code>\"150\"</code> n\u00e3o nos diz nada. Ele pode ser uma idade, um valor monet\u00e1rio, uma quantidade ou qualquer outra coisa. Ele \u00e9 apenas um registro sem um prop\u00f3sito claro.</p> <p>Esses dados s\u00e3o extremamente diversos, abrangendo desde conte\u00fados produzidos por usu\u00e1rios \u2014 como postagens em blogs, tweets, intera\u00e7\u00f5es em redes sociais e fotografias \u2014 at\u00e9 registros de sistemas e sensores.</p> A diferen\u00e7a entre dado e informa\u00e7\u00e3o <p>A informa\u00e7\u00e3o surge quando os dados s\u00e3o processados, organizados e contextualizados, ganhando um significado que permite a tomada de decis\u00f5es ou a constru\u00e7\u00e3o de conhecimento. A informa\u00e7\u00e3o \u00e9 o resultado da interpreta\u00e7\u00e3o dos dados.</p> <p>A principal distin\u00e7\u00e3o \u00e9 que a informa\u00e7\u00e3o responde a perguntas como \"quem?\", \"o qu\u00ea?\", \"onde?\", \"quando?\" e \"por que?\".</p> <p>Vamos usar um exemplo pr\u00e1tico para ilustrar a diferen\u00e7a:</p> <ul> <li>Dados: \"Jo\u00e3o\", \"30\", \"Rua das Palmeiras, 100\", \"2024-08-09\"</li> <li>Informa\u00e7\u00e3o: \"Jo\u00e3o, que tem 30 anos, mora na Rua das Palmeiras, 100. Essas informa\u00e7\u00f5es foram registradas em 9 de agosto de 2024.\"</li> </ul> <p>Neste caso, a informa\u00e7\u00e3o foi criada ao combinar e interpretar os dados brutos. O dado \"30\" se tornou a \"idade\" de \"Jo\u00e3o\", e o conjunto de dados formou um registro coerente e \u00fatil.</p> <p>Em resumo, a rela\u00e7\u00e3o entre os dois conceitos pode ser vista como um ciclo:</p> <ol> <li>Dados s\u00e3o coletados.</li> <li>Dados s\u00e3o processados e organizados.</li> <li>O resultado \u00e9 informa\u00e7\u00e3o, que tem um significado claro.</li> <li>Essa informa\u00e7\u00e3o, ao ser utilizada e interpretada, gera conhecimento.</li> </ol> <p>Nesse contexto, as empresas data-driven buscam ativamente coletar, processar e analisar esses dados brutos para transform\u00e1-los em informa\u00e7\u00f5es estrat\u00e9gicas.</p> <p>O objetivo principal \u00e9 utilizar essa intelig\u00eancia para otimizar opera\u00e7\u00f5es, tomar decis\u00f5es mais precisas e orientadas por evid\u00eancias, prever tend\u00eancias de mercado, personalizar a experi\u00eancia do cliente e, em \u00faltima an\u00e1lise, impulsionar o crescimento e a inova\u00e7\u00e3o.</p> <p>Info</p> <p>Todo neg\u00f3cio, seja ele consciente disso ou n\u00e3o, requer an\u00e1lise de dados.</p> <p>\u00c9 neste contexto, de prover capacidade de extrair valor dos dados, que surgem a engenharia de dados e ci\u00eancia de dados.</p>"},{"location":"classes/01-intro/intro/#o-que-e-engenharia-de-dados","title":"O Que \u00e9 Engenharia de Dados?","text":"<p>Embora hoje o termo seja amplamente utilizado, ainda existe confus\u00e3o sobre o que realmente significa engenharia de dados. Na pr\u00e1tica, ela existe desde que empresas come\u00e7aram a usar dados para an\u00e1lises preditivas, relat\u00f3rios e estudos descritivos, mas ganhou destaque com a ascens\u00e3o da ci\u00eancia de dados a partir de 2010.</p> <p>Defini\u00e7\u00e3o</p> <p>Vamos utilizar a seguinte defini\u00e7\u00e3o para Engenharia de Dados:</p> <p>Engenharia de Dados \u00e9 a disciplina t\u00e9cnica e pr\u00e1tica que se dedica ao design, constru\u00e7\u00e3o e manuten\u00e7\u00e3o de sistemas e infraestrutura que possibilitam a coleta, processamento, an\u00e1lise e armazenamento de grandes volumes de dados. Ela abrange atividades como extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga (ETL), al\u00e9m de garantir a integridade, acessibilidade, relev\u00e2ncia e escalabilidade dos dados. O objetivo \u00e9 assegurar que os dados estejam limpos, estruturados e prontos para serem utilizados por cientistas de dados e analistas em processos anal\u00edticos, preditivos e de tomada de decis\u00e3o.</p> <p>Ou de forma mais simples:</p> <p>A engenharia de dados \u00e9 o campo dedicado ao fluxo, processamento e administra\u00e7\u00e3o de dados, garantindo sua organiza\u00e7\u00e3o e utiliza\u00e7\u00e3o eficiente.</p> Defini\u00e7\u00e3o conforme a bibliografia oficial <p>Esta \u00e9 a defini\u00e7\u00e3o conforme o livro Fundamentals of Data Engineering (que iremos referenciar como FDE), uma das bibliograficas principais do curso:</p> <p>Info</p> <p>As refer\u00eancias de cada aula sempre estar\u00e3o listadas ao final da \u00faltima p\u00e1gina da aula.</p> <p>\"A engenharia de dados envolve a cria\u00e7\u00e3o, implementa\u00e7\u00e3o e manuten\u00e7\u00e3o de sistemas e fluxos de trabalho que transformam dados brutos em informa\u00e7\u00f5es de alta qualidade e consistentes. Essas informa\u00e7\u00f5es s\u00e3o essenciais para apoiar atividades como an\u00e1lise de dados e aprendizado de m\u00e1quina. Essa \u00e1rea abrange a integra\u00e7\u00e3o de seguran\u00e7a, gerenciamento de dados, DataOps, arquitetura de dados, orquestra\u00e7\u00e3o de processos e engenharia de software. O engenheiro de dados \u00e9 respons\u00e1vel por gerenciar o ciclo de vida dos dados, desde a captura das fontes de dados at\u00e9 sua disponibiliza\u00e7\u00e3o para os mais diversos casos de uso, como an\u00e1lise e machine learning.\"</p> <p>Sendo assim, o principal papel de um engenheiro de dados \u00e9 construir e manter os sistemas que coletam, armazenam e preparam grandes volumes de dados para uso.</p>"},{"location":"classes/01-intro/intro/#aplicacoes-intensivas-em-dados","title":"Aplica\u00e7\u00f5es Intensivas em Dados","text":"<p>Os sistemas ou aplica\u00e7\u00f5es modernos s\u00e3o tipicamente intensivos em dados, n\u00e3o em processamento. O poder bruto da CPU raramente \u00e9 o fator limitante. Os maiores desafios s\u00e3o o volume de dados, sua complexidade e a velocidade com que mudam.</p> <p>Essas aplica\u00e7\u00f5es s\u00e3o constru\u00eddas com blocos padronizados que fornecem funcionalidades essenciais: armazenar dados para recupera\u00e7\u00e3o posterior (bancos de dados), lembrar resultados de opera\u00e7\u00f5es complexas (caches), permitir buscas e filtros (\u00edndices de busca), enviar mensagens entre processos (processamento de streams) e processar grandes volumes acumulados (processamento em lote).</p>"},{"location":"classes/01-intro/intro/#a-complexidade-da-escolha","title":"A Complexidade da Escolha","text":"<p>Embora esses sistemas de dados sejam abstra\u00e7\u00f5es bem-sucedidas que usamos constantemente, a realidade n\u00e3o \u00e9 simples. Existem diversos sistemas de banco com caracter\u00edsticas diferentes porque aplica\u00e7\u00f5es t\u00eam requisitos distintos. H\u00e1 v\u00e1rias abordagens para cache, m\u00faltiplas formas de construir \u00edndices de busca. Ao desenvolver uma aplica\u00e7\u00e3o, ainda precisamos descobrir quais ferramentas e abordagens s\u00e3o mais apropriadas para cada tarefa, e pode ser desafiador combinar ferramentas quando uma \u00fanica n\u00e3o resolve tudo sozinha.</p>"},{"location":"classes/01-intro/intro/#nosso-objetivo","title":"Nosso objetivo","text":"<p>Neste curso, nosso objetivo \u00e9 desmistificar a engenharia de dados, preparando voc\u00ea para os desafios e oportunidades profissionais da \u00e1rea. Nosso foco \u00e9 garantir que, ao final do curso, voc\u00ea saiba tanto a teoria quanto aplicar as ferramentas e processos de infraestrutura de dados na pr\u00e1tica, implementando solu\u00e7\u00f5es escal\u00e1veis e eficientes que atendam \u00e0s necessidades de diferentes organiza\u00e7\u00f5es.</p>"},{"location":"classes/01-intro/processar/","title":"Explorar Alternativas","text":""},{"location":"classes/01-intro/processar/#explorar-alternativas","title":"Explorar Alternativas","text":"<p>Agora que voc\u00ea j\u00e1 consegue pelo menos observar quais arquivos est\u00e3o dispon\u00edveis, iremos avan\u00e7ar at\u00e9 que os dados dispon\u00edveis no S3 possam ser analisados pela empresa.</p> <p>Info</p> <p>J\u00e1 sabemos que eles cont\u00e9m informa\u00e7\u00f5es sobre esta\u00e7\u00f5es de bicicleta em S\u00e3o Francisco.</p> <p>Exercise</p> <p>Qual o formato dos arquivos dispon\u00edveis no S3? Voc\u00ea considera este formato adequado? Justifique.</p> <pre><code>$ aws s3 ls s3://dataeng-warmup --recursive --profile dataeng-warmup\n</code></pre> Submit <p>Answer</p> <p>O formato dos arquivos dispon\u00edveis no S3 \u00e9 CSV. Esse formato \u00e9 utilizado para an\u00e1lise de dados e suportado por diversas ferramentas de an\u00e1lise. No entanto, pode n\u00e3o ser o mais eficiente em termos de armazenamento e desempenho para grandes volumes de dados.</p> <p>Um formato alternativo que poderia ser considerado \u00e9 o Parquet. O Parquet \u00e9 um formato de arquivo colunar que oferece melhor compress\u00e3o e desempenho em consultas, especialmente em cen\u00e1rios de big data.</p> <p>Exercise</p> <p>Uma outra vantagem do Parquet \u00e9 a fixa\u00e7\u00e3o de schema dos dados. Por que isto \u00e9 importante?</p> Submit <p>Answer</p> <p>Isto \u00e9 importante porque garante que todos os dados sejam armazenados de forma consistente, facilitando a valida\u00e7\u00e3o e a an\u00e1lise.</p> <p>Al\u00e9m disso, a fixa\u00e7\u00e3o de schema permite que as ferramentas de processamento de dados otimizem suas opera\u00e7\u00f5es, resultando em melhor desempenho e efici\u00eancia.</p> <p>Sua pr\u00f3xima tarefa ser\u00e1:</p> <ol> <li>Ler todos os CSVs</li> <li>Fixar um schema adequado</li> <li>Salvar no S3 em formato Parquet no path <code>s3://dataeng-warmup/data_processed/insper_username/file.parquet</code> onde:<ol> <li><code>insper_username</code> deve ser substitu\u00eddo pelo seu nome de usu\u00e1rio do Insper.</li> <li><code>file</code> deve ser substitu\u00eddo pelo nome do arquivo que est\u00e1 sendo processado.</li> </ol> </li> </ol> <p>Antes de come\u00e7ar a realizar as tarefas, leia um pouco mais do handout!</p>"},{"location":"classes/01-intro/processar/#ferramenta","title":"Ferramenta","text":"<p>Para realizar as tarefas, duas alternativas foram propostas pela empresa: - Pandas, - Polars.</p> <p>Exercise</p> <p>Imagino que voc\u00ea j\u00e1 conhe\u00e7a o <code>pandas</code>. Ela seria uma boa escolha para a tarefa?</p> Submit <p>Answer</p> <p>Se os arquivos fossem pequenos, seria. No entanto, para arquivos grandes, o <code>pandas</code> pode ter problemas de desempenho e consumo de mem\u00f3ria.</p> <p>Confira o tamanho dos arquivos dispon\u00edveis no S3 e considere o uso do <code>polars</code>.</p> <p>Exercise</p> <p>Fa\u00e7a uma pesquisa breve sobre o <code>polars</code> e compare com o <code>pandas</code>.</p> <p>Voc\u00ea pode solicitar alguns exemplos de c\u00f3digo para ilustrar as diferen\u00e7as entre as duas bibliotecas.</p> Mark as done <p>Exercise</p> <p>Crie um diret\u00f3rio para a aula e inicie um projeto Python nele.</p> <pre><code>$ mkdir aula01\n$ cd aula01\n</code></pre> Mark as done <p>Para conseguirmos interagir com o S3 utilizando Python, precisamos definir as credenciais de acesso. Isto ser\u00e1 realizado no arquivo <code>.env</code>. Veja um exemplo abaixo:</p> <pre><code>AWS_ACCESS_KEY_ID=your_access_key\nAWS_SECRET_ACCESS_KEY=your_secret_key\nAWS_REGION=us-east-1\n</code></pre> <p>Exercise</p> <p>Crie o arquivo <code>.env</code> na raiz do seu projeto e adicione as credenciais de acesso ao S3 fornecidas pelo professor.</p> Mark as done <p>Answer</p> <p>Pergunte ao professor caso n\u00e3o saiba onde encontrar esta informa\u00e7\u00e3o!</p> <p>Uma outra boa pr\u00e1tica \u00e9 criar um ambiente virtual para aula. Voc\u00ea pode fazer isso utilizando o <code>venv</code>, <code>conda</code>, <code>uv</code> ou outra ferramenta de sua prefer\u00eancia.</p> <p>Exercise</p> <p>Crie ou ative seu ambiente virtual</p> Mark as done <p>\u00c9 adequado criar um arquivo <code>requirements.txt</code> para gerenciar as depend\u00eancias do projeto.</p> <p>Exercise</p> <p>Crie o arquivo <code>requirements.txt</code> na raiz do seu projeto e adicione as seguintes bibliotecas nele:</p> <pre><code>pandas==2.3.1\npolars==1.32.2\npython-dotenv==1.1.1\ns3fs==0.4.2\n</code></pre> Mark as done <p>Exercise</p> <p>Instale com:</p> <p>Aten\u00e7\u00e3o!</p> <p>Lembre de ativar o ambiente virtual!</p> <pre><code>$ pip install -r requirements.txt\n</code></pre> Mark as done <p>Utilize os seguintes c\u00f3digos base para realizar a leitura dos arquivos CSV:</p> <p>Exercise</p> <p>Analise os c\u00f3digos da sequ\u00eancia e garanta que entendeu o que est\u00e1 acontecendo.</p> Mark as done PandasPolars <pre><code>import os\nimport s3fs\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Definir path do arquivo a ser lido\ncsv_path = \"dataeng-warmup/data_raw/station.csv\"\n\naws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\naws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\naws_region = os.getenv(\"AWS_REGION\")\n\nfs = s3fs.S3FileSystem(\n    key=aws_access_key, secret=aws_secret_access_key, client_kwargs={\"region_name\": aws_region}\n)\n\n# CSV\nwith fs.open(csv_path, \"rb\") as f:\n    df = pd.read_csv(f)\n\ndf.head(2)\n</code></pre> <pre><code>import os\nimport s3fs\nimport polars as pl\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Definir path do arquivo a ser lido\ncsv_path = \"dataeng-warmup/data_raw/station.csv\"\n\naws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\naws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\naws_region = os.getenv(\"AWS_REGION\")\n\nfs = s3fs.S3FileSystem(\n    key=aws_access_key, secret=aws_secret_access_key, client_kwargs={\"region_name\": aws_region}\n)\n\n# Observe que estammos utilizando Lazy Evaluation\nwith fs.open(csv_path, \"rb\") as f:\n    df = pl.scan_csv(f)\n\ndf.head(2).collect()\n</code></pre> <p>Exercise</p> <p>O que \u00e9 lazy evaluation?</p> Submit <p>Answer</p> <p>Lazy evaluation \u00e9 uma t\u00e9cnica onde a avalia\u00e7\u00e3o de uma express\u00e3o \u00e9 adiada at\u00e9 que seu valor seja realmente necess\u00e1rio. Isso pode ajudar a melhorar o desempenho e a efici\u00eancia, especialmente ao trabalhar com grandes volumes de dados.</p> <p>No contexto do <code>polars</code>, a lazy evaluation permite que o sistema otimize a execu\u00e7\u00e3o de opera\u00e7\u00f5es em um DataFrame, agrupando e minimizando o trabalho necess\u00e1rio para produzir o resultado final.</p> <p>Exercise</p> <p>Fa\u00e7a uma vers\u00e3o do c\u00f3digo para ler o DataFrame utilizando <code>polars</code> sem lazy evaluation.</p> Mark as done <p>Answer</p> <p>Altere de <code>pl.scan_csv(f)</code> para <code>pl.read_csv(f)</code> e remova o '.collect()`.</p> <p>Exercise</p> <p>Fa\u00e7a a transforma\u00e7\u00e3o necess\u00e1rias nos dados para que o schema seja fixado corretamente.</p> <p>Para fixar o schema, voc\u00ea deve garantir que os tipos de dados das colunas estejam corretos e que n\u00e3o haja valores ausentes ou inconsistentes.</p> <p>Utilize as fun\u00e7\u00f5es de transforma\u00e7\u00e3o do <code>polars</code> para ajustar os dados conforme necess\u00e1rio.</p> Mark as done <p>Answer</p> <p>Nesta etapa, pesquise sobre as fun\u00e7\u00f5es de transforma\u00e7\u00e3o do <code>polars</code>.</p> <p>Exercise</p> <p>Para cada arquivo bruto (<code>raw</code>) dispon\u00edvel no S3, crie um arquivo <code>parquet</code> no S3 com o mesmo nome, mas no diret\u00f3rio <code>data_processed/insper_username/</code>.</p> <p>O arquivo Parquet deve ser criado a partir do DataFrame processado.</p> <p>Utilize este c\u00f3digo como base:</p> <p>Aten\u00e7\u00e3o!</p> <p>O c\u00f3digo base n\u00e3o faz as transforma\u00e7\u00f5es necess\u00e1rias!</p> <p>Defina seu <code>insper_username</code> e o <code>parquet_path</code> corretamente. </p> <pre><code>import os\nimport s3fs\nimport polars as pl\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Definir path do arquivo a ser exportado\ninsper_username = \"\"\nbucket_name = \"dataeng-warmup\"\nparquet_path = f\"{bucket_name}/data_processed/{insper_username}/station.parquet\"\n\naws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\naws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\naws_region = os.getenv(\"AWS_REGION\")\n\nfs = s3fs.S3FileSystem(\n    key=os.getenv(\"AWS_ACCESS_KEY_ID\"), secret=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n)\n\n# Aqui, o `df` \u00e9 o DataFrame do Polars que iremos exportar\n# N\u00e3o utilize collect se n\u00e3o estiver utilizando lazy evaluation\nwith fs.open(parquet_path, mode=\"wb\") as f:\n    df.collect().write_parquet(f)\n</code></pre> Mark as done <p>Exercise</p> <p>Ap\u00f3s exportar, confira que voc\u00ea consegue ler o arquivo <code>parquet</code> criado.</p> <p>Utilize este c\u00f3digo como base:</p> <pre><code>import os\nimport s3fs\nimport polars as pl\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Definir path do arquivo a ser lido\ninsper_username = \"pereira\"\nbucket_name = \"dataeng-warmup\"\nparquet_path = f\"{bucket_name}/data_processed/{insper_username}/station.parquet\"\n\naws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\naws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\naws_region = os.getenv(\"AWS_REGION\")\n\nfs = s3fs.S3FileSystem(\n    key=aws_access_key, secret=aws_secret_access_key, client_kwargs={\"region_name\": aws_region}\n)\n\nwith fs.open(parquet_path, \"rb\") as f:\n    # df = pl.read_parquet(f) # Sem lazy evaluation\n    df = pl.scan_parquet(f) # Com lazy evaluation\n\n# df.head(2) # Sem lazy evaluation\ndf.head(2).collect() # Com lazy evaluation\n</code></pre> Mark as done <p>Exercise</p> <p>Ap\u00f3s exportar e garantir que consegue ler o arquivo <code>parquet</code> criado, confira que voc\u00ea consegue listar os arquivos no S3. Voc\u00ea deve visualizar, na lista de arquivos, os parquets que voc\u00ea criou e os parquets criados por seus colegas.</p> <pre><code>$ aws s3 ls s3://dataeng-warmup --recursive --profile dataeng-warmup\n</code></pre> Mark as done"},{"location":"classes/01-intro/processar/#exemplo-de-analise","title":"Exemplo de an\u00e1lise","text":"<p>Agora que voc\u00ea exportou e leu os arquivos <code>parquet</code>, pode realizar an\u00e1lises sobre os dados. Por exemplo, voc\u00ea pode calcular estat\u00edsticas descritivas, criar visualiza\u00e7\u00f5es ou aplicar modelos de machine learning.</p> <p>Exercise</p> <p>Sua tarefa \u00e9, utilizando <code>polars</code> e o hist\u00f3rico de <code>status</code> das esta\u00e7\u00f5es (quantas bicicletas est\u00e3o dispon\u00edveis em cada esta\u00e7\u00e3o ao longo do tempo e quantos docks de armazenamento de bicicletas est\u00e3o livres), responder:</p> <p>\"Quais as dez esta\u00e7\u00f5es que, em m\u00e9dia, considerando os intantes de tempo, possuem percentualmente menos bicicletas dispon\u00edveis em rela\u00e7\u00e3o ao n\u00famero total de docks de armazenamento?\"</p> Mark as done"},{"location":"classes/01-intro/processar/#questoes-finais","title":"Quest\u00f5es finais","text":"<p>Exercise</p> <p>Segundo o esquema deste warm up, onde os dados s\u00e3o armazenados?</p> No computador dos analistas De forma centralizada no S3 Submit <p>Answer</p> <p>Os dados s\u00e3o armazenados de forma centralizada no S3, permitindo que todos os membros da equipe acessem e trabalhem com os mesmos conjuntos de dados.</p> <p>Exercise</p> <p>Segundo o esquema deste warm up, onde os dados s\u00e3o processados?</p> No computador dos analistas No S3 Submit <p>Exercise</p> <p>Como voc\u00ea analisa esta organiza\u00e7\u00e3o? Quais as vantagens e desvantagens dos dados serem processados no computador dos analistas?</p> Submit <p>Answer</p> <p>\u2705 Vantagens:</p> <ul> <li>Flexibilidade: Os analistas podem escolher as ferramentas e ambientes que melhor atendem \u00e0s suas necessidades.</li> <li>Autonomia: Cada analista pode trabalhar de forma independente, sem depender de uma infraestrutura centralizada.</li> </ul> <p>\u274c Desvantagens:</p> <ul> <li>Consist\u00eancia: Pode haver varia\u00e7\u00f5es nos resultados devido a diferentes ambientes e configura\u00e7\u00f5es.</li> <li>Escalabilidade: Processar grandes volumes de dados localmente pode ser limitado pela capacidade do hardware dos analistas e pela largura de banda da rede.</li> <li>Colabora\u00e7\u00e3o: A falta de um ambiente centralizado pode dificultar a colabora\u00e7\u00e3o e o compartilhamento de resultados entre os membros da equipe.</li> </ul>"},{"location":"classes/01-intro/profissionais/","title":"Ciclo de vida e Profissionais","text":""},{"location":"classes/01-intro/profissionais/#ciclo-de-vida-e-profissionais","title":"Ciclo de vida e Profissionais","text":""},{"location":"classes/01-intro/profissionais/#ciclo-de-vida-da-engenharia-de-dados","title":"Ciclo de vida da engenharia de dados","text":"<p>O ciclo de vida da engenharia de dados representa um processo cont\u00ednuo e interconectado que transforma dados brutos em valor para a organiza\u00e7\u00e3o (adaptado de FDE):</p> <pre><code>flowchart LR\n\n    %% Plataforma principal\n    subgraph PD[Plataforma de Dados]\n        direction LR\n\n        G[Gera\u00e7\u00e3o]\n\n        %% Linha horizontal de entrada + pipeline\n        subgraph LINE[Armazenamento]\n        direction LR\n        I[Ingest\u00e3o]\n        T[Transforma\u00e7\u00e3o]\n        S[Disponibiliza\u00e7\u00e3o]\n        I --&gt; T --&gt; S\n        end\n\n        %% Sa\u00eddas diretas (\u00e0 direita)\n        ML[Aprendizado de M\u00e1quina]\n        AN[An\u00e1lises]\n        REP[Dashboards]\n\n        G --&gt; I\n        S --&gt; ML\n        S --&gt; AN\n        S --&gt; REP\n    end\n\n    %% Estilos adaptados para light e dark mode\n    classDef gen fill:#64748b,stroke:#475569,color:#ffffff,stroke-width:2px;\n    classDef stage fill:#0891b2,stroke:#0e7490,color:#ffffff,stroke-width:2px;\n    classDef trans fill:#8b5cf6,stroke:#7c3aed,color:#ffffff,stroke-width:2px;\n    classDef serve fill:#10b981,stroke:#059669,color:#ffffff,stroke-width:2px;\n    classDef out fill:#f59e0b,stroke:#d97706,color:#ffffff,stroke-width:2px;\n\n    class G gen;\n    class I stage;\n    class T trans;\n    class S serve;\n    class ML,AN,REP out;</code></pre> <p>Este fluxo inicia com a gera\u00e7\u00e3o de dados em diversas fontes (sistemas transacionais, sensores, APIs, logs) e passa por tr\u00eas etapas fundamentais dentro da plataforma de dados:</p> <ul> <li>ingest\u00e3o (coleta e captura),</li> <li>transforma\u00e7\u00e3o (limpeza, estrutura\u00e7\u00e3o e enriquecimento)</li> <li>e disponibiliza\u00e7\u00e3o (armazenamento otimizado para consumo).</li> </ul> <p>O produto final alimenta desde modelos de machine learning at\u00e9 dashboards executivos e an\u00e1lises explorat\u00f3rias, criando um ecossistema onde dados se tornam a base para decis\u00f5es estrat\u00e9gicas e inova\u00e7\u00e3o.</p>"},{"location":"classes/01-intro/profissionais/#engenharia-de-dados-versus-ciencia-de-dados","title":"Engenharia de dados versus Ci\u00eancia de dados","text":"<p>As tarefas necess\u00e1rias para tornar a extra\u00e7\u00e3o de valor a partir de dados poss\u00edvel ser\u00e1 realizada por diferentes profissionais. Por mais que a engenharia de dados possua interse\u00e7\u00e3o com as \u00e1reas de ci\u00eancia de dados e an\u00e1lise, ela n\u00e3o deve ser confundida como uma sub\u00e1rea direta dessas disciplinas.</p> <p>Info</p> <p>Embora trabalhem de forma complementar, cada uma possui objetivos e compet\u00eancias distintas.</p> <p>Considere a hierarquia de necessidades da ci\u00eancia de dados (adaptado de FDE):</p> <pre><code>%%{init: {\n  \"flowchart\": {\n    \"wrap\": false,\n    \"htmlLabels\": false,\n    \"useMaxWidth\": true,\n    \"curve\": \"linear\",\n    \"rankSpacing\": 15,\n    \"nodeSpacing\": 8\n  }\n}}%%\nflowchart TB\n  classDef top fill:#6366f1,stroke:#4f46e5,color:#ffffff,font-size:11px,stroke-width:2px;\n  classDef l2  fill:#8b5cf6,stroke:#7c3aed,color:#ffffff,font-size:11px,stroke-width:2px;\n  classDef l3  fill:#a855f7,stroke:#9333ea,color:#ffffff,font-size:11px,stroke-width:2px;\n  classDef l4  fill:#0891b2,stroke:#0e7490,color:#ffffff,font-size:11px,stroke-width:2px;\n  classDef l5  fill:#10b981,stroke:#059669,color:#ffffff,font-size:11px,stroke-width:2px;\n  classDef l6  fill:#22c55e,stroke:#16a34a,color:#ffffff,font-size:11px,stroke-width:2px;\n\n  A[\"IA, Aprendizado Profundo\"]:::top\n  B[\"Testes A/B, Experimenta\u00e7\u00e3o, Algoritmos de ML Simples\"]:::l2\n  C[\"An\u00e1lises, M\u00e9tricas, Segmentos, Agrega\u00e7\u00f5es, Features, Dados de Treinamento\"]:::l3\n  D[\"Limpeza, Detec\u00e7\u00e3o de Anomalias, Prepara\u00e7\u00e3o\"]:::l4\n  E[\"Fluxo de Dados Confi\u00e1vel, Infraestrutura, Pipelines, ETL, Armazenamento Estruturado e N\u00e3o Estruturado\"]:::l5\n  F[\"Instrumenta\u00e7\u00e3o, Logs, Sensores, Dados Externos, Conte\u00fado Gerado por Usu\u00e1rios\"]:::l6\n\n  A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F</code></pre> <p>Apesar do interesse de muitos cientistas de dados em criar e aprimorar modelos de Machine Learning, grande parte de seu tempo (entre 70% e 80%) \u00e9 consumida nas tr\u00eas etapas da base pir\u00e2mide. </p> <p>Por n\u00e3o serem, em geral, especializados na constru\u00e7\u00e3o de sistemas de dados para produ\u00e7\u00e3o, acabam realizando essas atividades de maneira n\u00e3o otimizada.</p> <p>Recruta-se Engenheiros(as) de dados!</p> <p>Sem base s\u00f3lida, o trabalho de ci\u00eancia de dados tende a ser ineficiente e limitado pelo tempo gasto em tarefas de prepara\u00e7\u00e3o.</p> <p>Assim, podemos considerar que engenharia de dados est\u00e1 localizada nas etapas prim\u00e1rias do fluxo de trabalho com dados: \u00e9 respons\u00e1vel por coletar, organizar, transformar e disponibilizar dados de forma confi\u00e1vel e escal\u00e1vel, fornecendo insumos de alta qualidade para que cientistas de dados possam gerar an\u00e1lises, treinarr modelos de aprendizado de m\u00e1quina e identificar insights estrat\u00e9gicos.</p> <pre><code>flowchart LR\n    style Fontes fill:#10b981,stroke:#059669,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Engenharia fill:#8b5cf6,stroke:#7c3aed,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Ciencia fill:#f59e0b,stroke:#d97706,color:#ffffff,rx:5,ry:5,stroke-width:2px\n\n    Fontes[Dados de&lt;br&gt;v\u00e1rias fontes] --&gt; Engenharia[Engenharia de dados] --&gt; Ciencia[Ci\u00eancia de dados&lt;br&gt;e an\u00e1lise]</code></pre> <p>Aten\u00e7\u00e3o!</p> <p>Sem essa base s\u00f3lida, o trabalho de ci\u00eancia de dados tende a ser ineficiente e limitado pelo tempo gasto em tarefas de prepara\u00e7\u00e3o.</p>"},{"location":"classes/01-intro/profissionais/#profissionais-de-dados","title":"Profissionais de dados","text":"<p>Para facilitar a compreens\u00e3o da \u00e1rea de Engenharia de Dados, vamos explorar os principais pap\u00e9is e responsabilidades dos profissionais que atuam nesse campo:</p> <p>N\u00e3o seja t\u00e3o r\u00edgido!</p> <p>Esta lista serve como um guia geral para as principais atua\u00e7\u00f5es na \u00e1rea de dados.</p> <p>Na pr\u00e1tica, as responsabilidades de cada profissional podem se sobrepor e variar bastante de empresa para empresa.</p> <ul> <li> <p>Engenheiro de Dados (Data Engineer): Este profissional \u00e9 respons\u00e1vel por projetar, construir e manter os sistemas e infraestruturas que coletam, armazenam e processam grandes volumes de dados. \u00c9 ele quem garante que os dados estejam dispon\u00edveis e prontos para serem utilizados pelos analistas e cientistas.</p> </li> <li> <p>Arquiteto de Dados (Data Architect): O Arquiteto de Dados \u00e9 o respons\u00e1vel por desenhar a estrat\u00e9gia e a arquitetura geral de dados da organiza\u00e7\u00e3o. Ele define como os dados ser\u00e3o armazenados, integrados e consumidos, garantindo que a infraestrutura seja escal\u00e1vel, segura e eficiente.</p> </li> </ul> <p>Aten\u00e7\u00e3o</p> <p>Os dois pr\u00f3ximos cargos s\u00e3o bastante focados em dar vis\u00e3o sobre o estado atual dos dados da empresa (geralmente sem envolver constru\u00e7\u00e3o de modelos, predi\u00e7\u00e3o).</p> <ul> <li> <p>Analista de Dados (Data Analyst): Focado na an\u00e1lise explorat\u00f3ria de dados, o Analista de Dados coleta, limpa e interpreta conjuntos de dados para identificar tend\u00eancias, padr\u00f5es e insights. Ele utiliza ferramentas como SQL e Excel para criar relat\u00f3rios e dashboards que ajudam na visualiza\u00e7\u00e3o dos resultados.</p> </li> <li> <p>Analista de BI (Business Intelligence Analyst): Este profissional \u00e9 especializado em transformar dados brutos em informa\u00e7\u00f5es \u00fateis para os neg\u00f3cios. Ele constr\u00f3i dashboards, relat\u00f3rios e visualiza\u00e7\u00f5es que permitem que l\u00edderes e gestores monitorem o desempenho da empresa e tomem decis\u00f5es mais assertivas.</p> </li> </ul> <p>Aten\u00e7\u00e3o</p> <p>Os dois pr\u00f3ximos cargos s\u00e3o bastante focados em Machine Learning.</p> <ul> <li> <p>Engenheiro de Machine Learning (Machine Learning Engineer): Respons\u00e1vel por desenvolver, treinar e otimizar modelos de machine learning. Este profissional tem um perfil mais t\u00e9cnico e focado na parte de cria\u00e7\u00e3o do modelo, utilizando t\u00e9cnicas estat\u00edsticas e programa\u00e7\u00e3o para resolver problemas complexos.</p> </li> <li> <p>Cientista de Dados (Data Scientist): O Cientista de Dados utiliza t\u00e9cnicas estat\u00edsticas, programa\u00e7\u00e3o e aprendizado de m\u00e1quina para extrair insights valiosos dos dados. Sua principal fun\u00e7\u00e3o \u00e9 construir modelos preditivos (ML) e algoritmos que ajudam a resolver problemas complexos e a tomar decis\u00f5es estrat\u00e9gicas.</p> </li> </ul> <p>Aten\u00e7\u00e3o</p> <p>O pr\u00f3ximo cargo \u00e9 bastante focado em tornar a opera\u00e7\u00e3o de machine learning mais eficiente. Os modelos produzidos pelos cientistas de dados viram produtos e s\u00e3o monitorados e gerenciados em produ\u00e7\u00e3o pelo Engenheiro de MLOps.</p> <ul> <li> <p>Engenheiro de MLOps (MLOps Engineer): Este profissional atua como uma ponte entre o desenvolvimento de modelos de machine learning e o ambiente de produ\u00e7\u00e3o. Ele automatiza, implanta e monitora os modelos, garantindo que funcionem de forma confi\u00e1vel, escal\u00e1vel e eficiente em um ambiente real. Ele aplica os princ\u00edpios de DevOps para o mundo do machine learning.</p> </li> <li> <p>Engenheiro de Software (Software Engineer): Embora n\u00e3o seja um profissional exclusivo da \u00e1rea de dados, o Engenheiro de Software desempenha um papel crucial. Ele \u00e9 respons\u00e1vel por desenvolver aplica\u00e7\u00f5es e sistemas robustos e eficientes, muitas vezes colaborando com equipes de dados para integrar modelos de machine learning e pipelines de dados nas solu\u00e7\u00f5es de software da empresa.</p> </li> <li> <p>Database Administrator (DBA): O DBA \u00e9 respons\u00e1vel por gerenciar, manter e otimizar sistemas de gerenciamento de bancos de dados (SGBDs). Ele garante a seguran\u00e7a, a integridade e o desempenho dos bancos de dados, sendo fundamental para o armazenamento e acesso aos dados.</p> </li> <li> <p>Engenheiro de Seguran\u00e7a de Dados (Data Security Engineer): O Engenheiro de Seguran\u00e7a de Dados protege os dados da organiza\u00e7\u00e3o. Ele implementa medidas de seguran\u00e7a como criptografia, controle de acesso e auditorias, garantindo que os dados estejam seguros e em conformidade com regulamenta\u00e7\u00f5es.</p> </li> <li> <p>Especialista em Governan\u00e7a de Dados: Foca no gerenciamento e controle de qualidade dos dados, garantindo conformidade com pol\u00edticas internas e regulamenta\u00e7\u00f5es externas.</p> </li> </ul>"},{"location":"classes/01-intro/profissionais/#exercicios","title":"Exerc\u00edcios","text":"<p>Vamos utilizar os exerc\u00edcios a seguir para verificar os conhecimentos adquiridos sobre os cargos na \u00e1rea de dados.</p> <p>Importante</p> <p>Chame o professor caso tenha d\u00favidas!</p> <p>Question</p> <p>Uma empresa precisa projetar e implementar toda a infraestrutura para coleta, armazenamento e processamento de dados vindos de m\u00faltiplas fontes, garantindo que os dados estejam sempre dispon\u00edveis para an\u00e1lise. Qual profissional \u00e9 mais adequado para liderar esta tarefa?</p> Engenheiro de Dados Cientista de Dados Analista de BI Database Administrator Submit <p>Answer</p> <p>O Engenheiro de Dados \u00e9 respons\u00e1vel por projetar, construir e manter os sistemas e infraestruturas que coletam, armazenam e processam grandes volumes de dados.</p> <p>Question</p> <p>O CEO de uma startup est\u00e1 planejando a estrat\u00e9gia de dados da empresa para os pr\u00f3ximos 5 anos. Ele precisa definir como os dados ser\u00e3o armazenados, integrados e consumidos, garantindo escalabilidade e efici\u00eancia. Qual profissional deve ser consultado?</p> Engenheiro de Dados Arquiteto de Dados Analista de Dados Engenheiro de MLOps Submit <p>Answer</p> <p>O Arquiteto de Dados \u00e9 respons\u00e1vel por desenhar a estrat\u00e9gia e a arquitetura geral de dados da organiza\u00e7\u00e3o, definindo como os dados ser\u00e3o armazenados, integrados e consumidos.</p> <p>Question</p> <p>Uma empresa precisa migrar seu sistema legado de dados para a nuvem, implementando uma arquitetura moderna que inclua data lakes, data warehouses e ferramentas de processamento distribu\u00eddo. Al\u00e9m disso, \u00e9 necess\u00e1rio garantir que os pipelines de dados sejam resilientes a falhas e possam se recuperar automaticamente. Qual \u00e9 a principal responsabilidade do Engenheiro de Dados neste projeto?</p> Definir a estrat\u00e9gia geral de arquitetura de dados Criar dashboards para monitorar a migra\u00e7\u00e3o Implementar e manter a infraestrutura t\u00e9cnica de dados Analisar os dados migrados para validar qualidade Submit <p>Answer</p> <p>O Engenheiro de Dados \u00e9 respons\u00e1vel por implementar tecnicamente a infraestrutura de dados, construir pipelines resilientes, configurar ferramentas de processamento distribu\u00eddo e garantir que os sistemas funcionem de forma confi\u00e1vel e escal\u00e1vel na nuvem.</p> <p>Question</p> <p>Uma empresa de e-commerce quer identificar padr\u00f5es de comportamento dos clientes atrav\u00e9s de an\u00e1lise explorat\u00f3ria dos dados de vendas. Eles precisam criar relat\u00f3rios e dashboards para visualizar tend\u00eancias. Qual profissional \u00e9 mais adequado?</p> Analista de Dados Engenheiro de Machine Learning Engenheiro de Seguran\u00e7a de Dados Especialista em Governan\u00e7a de Dados Submit <p>Answer</p> <p>O Analista de Dados \u00e9 focado na an\u00e1lise explorat\u00f3ria de dados, coletando, limpando e interpretando conjuntos de dados para identificar tend\u00eancias, padr\u00f5es e insights.</p> <p>Question</p> <p>Um modelo de Machine Learning j\u00e1 foi desenvolvido e testado em ambiente de desenvolvimento. Agora precisa ser colocado em produ\u00e7\u00e3o com monitoramento automatizado. Qual profissional \u00e9 respons\u00e1vel por esta tarefa?</p> Cientista de Dados Engenheiro de Machine Learning Engenheiro de MLOps Engenheiro de Software Submit <p>Answer</p> <p>O Engenheiro de MLOps atua como ponte entre o desenvolvimento de modelos e o ambiente de produ\u00e7\u00e3o, automatizando, implantando e monitorando os modelos em produ\u00e7\u00e3o.</p> <p>O Engenheiro de Machine Learning tem um papel focado no desenvolvimento e treinamento de modelos de machine learning. Ele \u00e9 respons\u00e1vel por selecionar algoritmos, ajustar os modelos e garantir que eles tenham um bom desempenho durante o treinamento. No entanto, ao passar para o ambiente de produ\u00e7\u00e3o e necessitar de monitoramento e automa\u00e7\u00e3o, o Engenheiro de MLOps assume a responsabilidade, pois ele \u00e9 especializado na implanta\u00e7\u00e3o, automa\u00e7\u00e3o e monitoramento dos modelos em ambientes de produ\u00e7\u00e3o.</p> <p>Portanto, enquanto o Engenheiro de Machine Learning pode atuar na fase de desenvolvimento, o Engenheiro de MLOps \u00e9 o profissional que realmente cuida da integra\u00e7\u00e3o e manuten\u00e7\u00e3o cont\u00ednua do modelo em produ\u00e7\u00e3o.</p> <p>Question</p> <p>Uma empresa detectou que seus bancos de dados est\u00e3o com performance baixa e precisa otimizar consultas, gerenciar backups e garantir a integridade dos dados. Qual profissional \u00e9 mais adequado?</p> Engenheiro de Dados Arquiteto de Dados Analista de BI Database Administrator (DBA) Submit <p>Answer</p> <p>O DBA \u00e9 respons\u00e1vel por gerenciar, manter e otimizar sistemas de gerenciamento de bancos de dados, garantindo seguran\u00e7a, integridade e desempenho.</p> <p>Question</p> <p>Uma organiza\u00e7\u00e3o precisa construir um modelo preditivo para prever a rotatividade de funcion\u00e1rios usando t\u00e9cnicas de machine learning e an\u00e1lise estat\u00edstica avan\u00e7ada. Qual profissional deve liderar este projeto?</p> Analista de Dados Cientista de Dados Analista de BI Engenheiro de Dados Submit <p>Answer</p> <p>O Cientista de Dados utiliza t\u00e9cnicas estat\u00edsticas, programa\u00e7\u00e3o e aprendizado de m\u00e1quina para construir modelos preditivos e algoritmos que resolvem problemas complexos.</p> <p>Question</p> <p>Uma empresa de streaming recebe dados de milh\u00f5es de usu\u00e1rios em tempo real (cliques, visualiza\u00e7\u00f5es, curtidas) e precisa processar esses dados continuamente para alimentar recomenda\u00e7\u00f5es instant\u00e2neas. A infraestrutura atual n\u00e3o consegue lidar com o volume e a velocidade dos dados. Qual profissional \u00e9 fundamental para resolver este problema?</p> Engenheiro de Dados Analista de Dados Database Administrator Cientista de Dados Submit <p>Answer</p> <p>O Engenheiro de Dados \u00e9 respons\u00e1vel por projetar e construir pipelines de dados em tempo real, implementar tecnologias de streaming e garantir que a infraestrutura possa processar grandes volumes de dados com baixa lat\u00eancia.</p> <p>Question</p> <p>O gerente de log\u00edstica de uma grande empresa sente que est\u00e1 perdido em rela\u00e7\u00e3o aos n\u00fameros de sua \u00e1rea. Ele deseja obter, em um dashboard, n\u00fameros sobre o neg\u00f3cio: quantidade de pedidos entregues, quantidade de clientes atendidos por estado, valor total dos pedidos entregues, quantidade de reclama\u00e7\u00f5es. Considerando que os dados j\u00e1 est\u00e3o dispon\u00edveis, qual o profissional mais adequado para desenvolver a solu\u00e7\u00e3o e atender esta demanda?</p> Cientista de Dados Engenheiro de MLOps Analista de Dados Submit <p>Answer</p> <p>Tanto o Analista de Dados quando o Analista de BI seriam prov\u00e1veis profissionais adequados para a fun\u00e7\u00e3o!</p> <p>Question</p> <p>Uma empresa descobriu que dados sens\u00edveis de clientes podem estar expostos e precisa implementar criptografia, controles de acesso e auditorias de seguran\u00e7a. Qual profissional \u00e9 mais adequado?</p> Database Administrator Especialista em Governan\u00e7a de Dados Engenheiro de Seguran\u00e7a de Dados Arquiteto de Dados Submit <p>Answer</p> <p>O Engenheiro de Seguran\u00e7a de Dados protege os dados da organiza\u00e7\u00e3o implementando medidas como criptografia, controle de acesso e auditorias.</p> <p>Question</p> <p>Verdadeiro ou Falso: Um Engenheiro de Machine Learning tem como principal responsabilidade monitorar modelos em produ\u00e7\u00e3o.</p> Verdadeiro Falso Submit <p>Answer</p> <p>Falso. O Engenheiro de Machine Learning \u00e9 respons\u00e1vel por desenvolver, treinar e otimizar modelos de machine learning. Quem monitora modelos em produ\u00e7\u00e3o \u00e9 o Engenheiro de MLOps.</p> <p>Question</p> <p>Verdadeiro ou Falso: Analistas de Dados e Analistas de BI t\u00eam focos similares em dar vis\u00e3o sobre o estado atual dos dados da empresa (mais an\u00e1lise explorat\u00f3ria, menos predi\u00e7\u00e3o).</p> Verdadeiro Falso Submit <p>Answer</p> <p>Verdadeiro. Ambos os profissionais s\u00e3o focados em dar vis\u00e3o sobre o estado atual dos dados da empresa, geralmente sem envolver constru\u00e7\u00e3o de modelos ou proje\u00e7\u00f5es complexas.</p> <p>Question</p> <p>O servidor centralizado de dados da empresa est\u00e1 sobrecarregado porque diferentes equipes criam suas pr\u00f3prias extra\u00e7\u00f5es de dados, resultando em processos duplicados e inconsist\u00eancias. \u00c9 necess\u00e1rio criar um pipeline centralizado que extraia dados de m\u00faltiplas fontes (bancos transacionais, APIs, arquivos CSV), transforme-os segundo regras de neg\u00f3cio e os carregue no servidor centralizado de dados de forma automatizada. Qual profissional deve liderar esta iniciativa?</p> Arquiteto de Dados Engenheiro de Dados Analista de BI Especialista em Governan\u00e7a de Dados Submit <p>Answer</p> <p>O Engenheiro de Dados \u00e9 respons\u00e1vel por construir e manter pipelines ETL/ELT, automatizar processos de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga de dados, garantindo que os dados fluam de forma eficiente e confi\u00e1vel entre diferentes sistemas.</p>"},{"location":"classes/01-intro/profissionais/#referencias","title":"Refer\u00eancias","text":"<ul> <li>FDE. Reis, J., Housley, M. (2022). Fundamentals of Data Engineering: Plan and Build Robust Data Systems. Estados Unidos: O'Reilly Media.</li> <li>DDIA. Kleppmann, M. (2017). Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. Estados Unidos: O'Reilly Media.</li> <li>BDPBP. Warren, J., Marz, N. (2015). Big Data: Principles and Best Practices of Scalable Realtime Data Systems. Estados Unidos: Manning.</li> </ul>"},{"location":"classes/01-intro/warmup/","title":"Warm Up","text":""},{"location":"classes/01-intro/warmup/#habilidades-do-engenheiro-de-dados","title":"Habilidades do Engenheiro de Dados","text":"<p>O trabalho do engenheiro de dados envolve atuar em \u00e1reas fundamentais como seguran\u00e7a, gest\u00e3o e arquitetura de dados, DataOps  e engenharia de software.</p> <p>\u00c9 essencial compreender como as ferramentas de dados se encaixam ao longo do ciclo de vida da engenharia de dados, desde a origem das informa\u00e7\u00f5es at\u00e9 o momento em que s\u00e3o utilizadas para gerar valor por analistas e cientistas de dados.</p> <p>Relembrando o ciclo de vida:</p> <pre><code>flowchart LR\n\n    %% Plataforma principal\n    subgraph PD[Plataforma de Dados]\n        direction LR\n\n        G[Gera\u00e7\u00e3o]\n\n        %% Linha horizontal de entrada + pipeline\n        subgraph LINE[Armazenamento]\n        direction LR\n        I[Ingest\u00e3o]\n        T[Transforma\u00e7\u00e3o]\n        S[Disponibiliza\u00e7\u00e3o]\n        I --&gt; T --&gt; S\n        end\n\n        %% Sa\u00eddas diretas (\u00e0 direita)\n        ML[Aprendizado de M\u00e1quina]\n        AN[An\u00e1lises]\n        REP[Dashboards]\n\n        G --&gt; I\n        S --&gt; ML\n        S --&gt; AN\n        S --&gt; REP\n    end\n\n    %% Estilos adaptados para light e dark mode\n    classDef gen fill:#64748b,stroke:#475569,color:#ffffff,stroke-width:2px;\n    classDef stage fill:#0891b2,stroke:#0e7490,color:#ffffff,stroke-width:2px;\n    classDef trans fill:#8b5cf6,stroke:#7c3aed,color:#ffffff,stroke-width:2px;\n    classDef serve fill:#10b981,stroke:#059669,color:#ffffff,stroke-width:2px;\n    classDef out fill:#f59e0b,stroke:#d97706,color:#ffffff,stroke-width:2px;\n\n    class G gen;\n    class I stage;\n    class T trans;\n    class S serve;\n    class ML,AN,REP out;</code></pre> <p>Al\u00e9m disso, o engenheiro de dados precisa equilibrar constantemente fatores como custo, agilidade, escalabilidade, simplicidade, reutiliza\u00e7\u00e3o e interoperabilidade, escolhendo as solu\u00e7\u00f5es mais adequadas para cada contexto.</p> <p>At\u00e9 pouco tempo atr\u00e1s, o trabalho do engenheiro de dados envolvia o dom\u00ednio de um conjunto limitado, por\u00e9m robusto, de tecnologias como Hadoop e Spark.</p> <p>Essas ferramentas exigiam habilidades avan\u00e7adas (engenharia de software, redes, computa\u00e7\u00e3o distribu\u00edda, armazenamento) e grande parte das atividades era voltada \u00e0 gest\u00e3o e manuten\u00e7\u00e3o de clusters, ao controle de recursos e \u00e0 implementa\u00e7\u00e3o de pipelines e rotinas de transforma\u00e7\u00e3o de dados.</p> <p>Atualmente, as ferramentas modernas tornaram o cen\u00e1rio menos complexo e mais \u00e1gil, permitindo arquiteturas de dados que evoluem conforme novas tend\u00eancias surgem.</p> <p>Embora precise ter no\u00e7\u00f5es de an\u00e1lise, machine learning e cria\u00e7\u00e3o de relat\u00f3rios, o engenheiro de dados n\u00e3o \u00e9 o respons\u00e1vel direto por essas tarefas.</p> <pre><code>flowchart LR\n    style Custo fill:#0891b2,stroke:#0e7490,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Agilidade fill:#0891b2,stroke:#0e7490,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Escalabilidade fill:#0891b2,stroke:#0e7490,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Simplicidade fill:#0891b2,stroke:#0e7490,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Reuso fill:#0891b2,stroke:#0e7490,color:#ffffff,rx:5,ry:5,stroke-width:2px\n    style Interoperabilidade fill:#0891b2,stroke:#0e7490,color:#ffffff,rx:5,ry:5,stroke-width:2px\n\n    Custo[Custo] --- Agilidade[Agilidade] --- Escalabilidade[Escalabilidade] --- Simplicidade[Simplicidade] --- Reuso[Reuso] --- Interoperabilidade[Interoperabilidade]</code></pre>"},{"location":"classes/01-intro/warmup/#warm-up_1","title":"Warm Up","text":"<p>Para esquentarmos para o restante do semestre e come\u00e7armos a por em pr\u00e1tica os conceitos que estamos aprendendo, vamos propor uma atividade pr\u00e1tica que envolva um primeiro contato com ferramentas de an\u00e1lise de dados para um projeto espec\u00edfico.</p> <p>A ideia \u00e9 que voc\u00eas, enquanto testam as ferramentas, pesquisem suas caracter\u00edsticas, vantagens e desvantagens, considerando aspectos como facilidade de uso, integra\u00e7\u00e3o com outras tecnologias, custo e suporte da comunidade.</p>"},{"location":"classes/01-intro/warmup/#local-de-armazenamento-dos-arquivos","title":"Local de Armazenamento dos arquivos","text":"<p>Os arquivos est\u00e3o armazenados no S3. O S3 \u00e9 um servi\u00e7o de armazenamento de objetos da Amazon Web Services (AWS) que oferece alta durabilidade, escalabilidade e seguran\u00e7a para dados. Pense nele como um Dropbox ou Google Drive para dados est\u00e1ticos em grande escala.</p> <p>Os arquivos podem ser acessados atrav\u00e9s de URLs espec\u00edficas, e o S3 oferece recursos como versionamento, controle de acesso e integra\u00e7\u00e3o com outras ferramentas da AWS.</p>"},{"location":"classes/01-intro/warmup/#aws-cli-command-line-interface","title":"AWS CLI - Command Line Interface","text":"<p>A AWS CLI (Command Line Interface) \u00e9 uma ferramenta que permite gerenciar servi\u00e7os da AWS atrav\u00e9s da linha de comando. Com ela, voc\u00ea pode executar comandos para criar, modificar e excluir recursos na AWS, facilitando a automa\u00e7\u00e3o de tarefas e a integra\u00e7\u00e3o com scripts.</p>"},{"location":"classes/01-intro/warmup/#instalacao","title":"Instala\u00e7\u00e3o","text":"<p>Clique Aqui para instalar o AWS CLI.</p>"},{"location":"classes/01-intro/warmup/#credenciais-de-acesso","title":"Credenciais de acesso","text":"<p>Para acessar os dados no S3, voc\u00ea precisar\u00e1 de credenciais da AWS. Essas credenciais geralmente consistem em uma chave de acesso (Access Key) e uma chave secreta (Secret Key). </p> <p>Info</p> <p>Pergunte ao professor onde obter as credenciais de acesso ao S3.</p>"},{"location":"classes/01-intro/warmup/#configuracao","title":"Configura\u00e7\u00e3o","text":"<p>Configure a regi\u00e3o e as credenciais fornecidas pelo professor.</p> <pre><code>$ aws configure --profile dataeng-warmup\nAWS Access Key ID [None]: ????????????\nAWS Secret Access Key [None]: ????????????????????????????????\nDefault region name [None]: us-east-1\nDefault output format [None]: \n</code></pre> <p></p>"},{"location":"classes/01-intro/warmup/#definir-perfil","title":"Definir perfil","text":"<p>Para definir um perfil padr\u00e3o, use:</p> <p>Aten\u00e7\u00e3o</p> <p>Tudo bem caso n\u00e3o consiga definir um perfil padr\u00e3o.</p> <p>Nos comandos de CLI, sempre utilize o par\u00e2metro <code>--profile</code> para especificar o perfil a ser utilizado.</p> LinuxWindows CMD (Prompt de Comando)Windows PowerShell <pre><code>$ export AWS_PROFILE=dataeng-warmup\n</code></pre> <p> </p> <pre><code>$ set AWS_PROFILE=dataeng-warmup\n</code></pre> <p> </p> <pre><code>$ $env:AWS_PROFILE=\"dataeng-warmup\"\n</code></pre> <p> </p>"},{"location":"classes/01-intro/warmup/#exemplo-listar-conteudo-do-bucket-s3","title":"Exemplo: listar conte\u00fado do bucket S3","text":"<p>Agora voc\u00ea pode usar o AWS CLI para criar, listar ou remover recursos. Por exemplo, para listar os objetos no bucket S3 que utilizaremos na aula:</p> <pre><code>$ aws s3 ls s3://dataeng-warmup --recursive --profile dataeng-warmup\n</code></pre> <p>Voc\u00ea deve obter algo como:</p> <pre><code>2025-08-10 17:30:06          0 data_processed/\n2025-08-10 17:27:42          0 data_raw/\n2025-08-10 17:28:04       5647 data_raw/station.csv\n2025-08-10 17:34:31 1989696383 data_raw/status.csv\n2025-08-10 17:28:04   80208848 data_raw/trip.csv\n2025-08-10 17:28:05     438063 data_raw/weather.csv\n</code></pre> <p>Eles representam informa\u00e7\u00f5es sobre esta\u00e7\u00f5es de bicicleta em S\u00e3o Francisco.</p>"},{"location":"classes/02-docker-filas/docker/","title":"Docker","text":"<p>Vamos programar o exemplo proposto e ver o funcionamento de filas na pr\u00e1tica.</p>"},{"location":"classes/02-docker-filas/docker/#docker","title":"Docker","text":"<p>Para esta aula, ser\u00e1 necess\u00e1rio ter o Docker instalado.</p> <p>Docker \u00e9 uma plataforma que permite criar, empacotar e executar aplica\u00e7\u00f5es de forma isolada em cont\u00eaineres, garantindo portabilidade e consist\u00eancia entre ambientes.</p> <p>Ele facilita o desenvolvimento, a distribui\u00e7\u00e3o e a implanta\u00e7\u00e3o, eliminando problemas de incompatibilidade de configura\u00e7\u00e3o.</p> <p>Voc\u00ea pode verificar se o Docker est\u00e1 instalado executando o seguinte comando no terminal:</p> <pre><code>$ docker --version\n</code></pre> <p>Se o Docker n\u00e3o estiver instalado, siga as instru\u00e7\u00f5es na documenta\u00e7\u00e3o oficial para instal\u00e1-lo.</p>"},{"location":"classes/02-docker-filas/filas/","title":"Exemplo: Sensores","text":"<p>Suponha um cen\u00e1rio onde sensores de temperatura s\u00e3o utilizados em uma f\u00e1brica. Esses sensores enviam dados de temperatura em tempo real para um sistema central.</p> <pre><code>graph TD\n    %% N\u00f3 central\n    C([Endpoint de Ingest\u00e3o])\n\n    %% Sensores distribu\u00eddos\n    S1(((Sensor A)))\n    S2(((Sensor B)))\n    S3(((Sensor C)))\n    S4(((Sensor D)))\n    S5(((Sensor E)))\n    S6(((Sensor F)))\n    S7(((Sensor G)))\n    S8(((Sensor H)))\n\n    %% Liga\u00e7\u00f5es\n    S1 --&gt; C\n    S2 --&gt; C\n    S3 --&gt; C\n    S4 --&gt; C\n    S5 --&gt; C\n    S6 --&gt; C\n    S7 --&gt; C\n    S8 --&gt; C</code></pre> <p>O endpoint de ingest\u00e3o \u00e9 respons\u00e1vel por receber os dados dos sensores e encaminh\u00e1-los para tratamento adequado (transforma\u00e7\u00e3o e disponibiliza\u00e7\u00e3o).</p> <p>Exemplo de Mensagem</p> <p>Exemplo de mensagem enviada por um sensor:</p> <pre><code>{\n    \"sensor_id\": \"S1\",\n    \"timestamp\": \"2023-10-01T12:00:00Z\",\n    \"temperature\": 22.5\n}\n</code></pre> <p>Neste cen\u00e1rio, o modelo de ingest\u00e3o \u00e9 push-based. Como vantagens para este modelo, podemos considerar:</p> <ol> <li> <p>Dados em tempo real:</p> <ul> <li>Sensores podem enviar leituras assim que elas s\u00e3o coletadas, garantindo baixa lat\u00eancia.</li> <li>Em casos como detec\u00e7\u00e3o de superaquecimento, tempo de resposta r\u00e1pido \u00e9 essencial.</li> </ul> </li> <li> <p>Efici\u00eancia energ\u00e9tica e de rede:</p> <ul> <li>O sensor s\u00f3 transmite quando tem dados, economizando energia (importante para IoT).</li> <li>Evita tr\u00e1fego desnecess\u00e1rio de polling cont\u00ednuo.</li> </ul> </li> <li> <p>Simplicidade de implementa\u00e7\u00e3o:</p> <ul> <li>Sensores n\u00e3o precisam implementar l\u00f3gica complexa para gerenciar conex\u00f5es ou estados.</li> <li>O envio de dados \u00e9 feito de forma simples e direta.</li> </ul> </li> <li> <p>Armazenamento nos sensores:</p> <ul> <li>Os sensores n\u00e3o precisam implementar l\u00f3gica de armazenamento, pois os dados s\u00e3o enviados em tempo real para o endpoint de ingest\u00e3o.</li> </ul> </li> </ol> <p>Requisitos!</p> <p>Em um cen\u00e1rio real, a escolha pelo modelo de ingest\u00e3o deve considerar o cen\u00e1rio atual da empresa e os requisitos de projeto.</p>"},{"location":"classes/02-docker-filas/filas/#questao-importante","title":"Quest\u00e3o importante","text":"<p>Assim, o endpoint de ingest\u00e3o precisa ser algum servi\u00e7o que consiga receber as mensagens e lidar com a taxa de transmiss\u00e3o dos sensores.</p> <p>Como cada sensor ir\u00e1 realizar uma conex\u00e3o para envio das mensagens, precisamos garantir que o sistema seja escal\u00e1vel e capaz de lidar com picos de tr\u00e1fego.</p> <p>Perigo!</p> <p>Caso o endpoint de ingest\u00e3o n\u00e3o consiga lidar com a taxa de transmiss\u00e3o dos sensores, podemos enfrentar problemas como:</p> <ul> <li>Perda de dados: Mensagens podem ser descartadas se o sistema estiver sobrecarregado. O sensor pode n\u00e3o esperar o endpoint processar a mensagem (timeout).</li> <li>Atrasos: O processamento de dados pode ser retardado, afetando a an\u00e1lise em tempo real.</li> <li>Lock dos sensores: Sensores podem ficar bloqueados, impedindo o envio de novas mensagens (espera at\u00e9 que ocorra garantia de entrega da mensagem).</li> </ul>"},{"location":"classes/02-docker-filas/filas/#filas","title":"Filas","text":"<p>As filas s\u00e3o uma solu\u00e7\u00e3o eficaz para lidar com a ingest\u00e3o de dados em cen\u00e1rios de alta taxa de transmiss\u00e3o, como o dos sensores de temperatura.</p> <p>Elas ir\u00e3o atuar como intermedi\u00e1rias entre os produtores de dados (sensores) e os consumidores (servi\u00e7os de processamento).</p> <p>Ao inv\u00e9s do endpoint de ingest\u00e3o receber as mensagens diretamente dos sensores, os sensores ir\u00e3o publicar suas mensagens em uma fila. Os servi\u00e7os de processamento, quando tiverem disponibilidade, ir\u00e3o consumir as mensagens dessa fila.</p> <p>Este \u00e9 o modelo proposto:</p> <pre><code>flowchart LR\n    S1[Sensor Temp 1] --&gt;|Publish| B[Broker/Fila]\n    S2[Sensor Temp 2] --&gt;|Publish| B\n    S3[Sensor Temp N] --&gt;|Publish| B\n    B --&gt;|Consume| P[Pipeline de Processamento]\n    %% Data Lake/Warehouse\n    P --&gt; D[Servi\u00e7o de Armazenamento - Disponibiliza\u00e7\u00e3o]</code></pre> <p>Benef\u00edcios</p> <p>Com o uso de filas, ganharemos em:</p> <ol> <li> <p>Escalabilidade:</p> <ul> <li>Adicionar mais sensores n\u00e3o sobrecarrega um job central de coleta; cada sensor publica no seu ritmo.</li> <li>A fila far\u00e1 o gerenciamento da entrega para o pipeline de processamento.</li> </ul> </li> <li> <p>Desacoplamento:</p> <ul> <li>Se o processador (pipeline de processamento) estiver indispon\u00edvel por alguns segundos, as mensagens podem ficar na fila e serem processadas depois.</li> </ul> </li> </ol> <p>Dica!</p> <p>Desacoplamento \u00e9 o princ\u00edpio de projetar sistemas de forma que seus componentes funcionem de maneira independente, reduzindo depend\u00eancias diretas entre eles.</p> <p>Isso permite que partes do pipeline (como ingest\u00e3o, processamento e armazenamento) possam evoluir, ser escaladas ou substitu\u00eddas sem afetar drasticamente as demais.</p> <p>T\u00e9cnicas como uso de filas, APIs e contratos bem definidos entre servi\u00e7os ajudam a alcan\u00e7ar esse isolamento, trazendo mais resili\u00eancia, flexibilidade e facilidade de manuten\u00e7\u00e3o \u00e0s arquiteturas de dados.</p> <p>Mas toda tecnologia tem suas complexidades e trade-offs! O uso de filas, por exemplo, pode introduzir lat\u00eancias adicionais e requer um gerenciamento cuidadoso para evitar problemas como o ac\u00famulo de mensagens n\u00e3o processadas.</p> <p>Considere se o uso de filas n\u00e3o ir\u00e1 trazer mais complexidade do que benef\u00edcio ao fluxo de dados. Isso acontece, por exemplo, em pipelines simples e diretos, onde um componente chama outro imediatamente e n\u00e3o h\u00e1 necessidade de desacoplamento ou de lidar com varia\u00e7\u00f5es de carga.</p> <p>Evite uso de filas se:</p> <ul> <li>N\u00e3o h\u00e1 varia\u00e7\u00f5es de carga / pico de tr\u00e1fego</li> <li>Volume de dados pequeno e previs\u00edvel</li> <li>N\u00e3o h\u00e1 m\u00faltiplos produtores ou consumidores</li> <li>A resposta precisa ser imediata e n\u00e3o h\u00e1 toler\u00e2ncia para processamento ass\u00edncrono.</li> </ul> <p>Nestes casos, a sobrecarga operacional de gerenciar e monitorar uma fila pode ser desnecess\u00e1ria!</p>"},{"location":"classes/02-docker-filas/programar-pipeline/","title":"Programar Pipeline","text":""},{"location":"classes/02-docker-filas/programar-pipeline/#programar-pipeline","title":"Programar Pipeline","text":"<p>Iremos programar consumidores para processar as mensagens enviadas para o RabbitMQ.</p> <p>Desta forma, o fluxo estar\u00e1 completo. As mensagens s\u00e3o produzidas por sensores e consumidas por pipelines de processamento.</p> <pre><code>graph LR\n    subgraph Produtores\n        P1[Sensor 1]\n        P2[Sensor 2]\n        P3[Sensor 3]\n    end\n\n    subgraph Fila\n        direction LR\n        F1[(Msg 1)]\n        F2[(Msg 2)]\n        F3[(Msg n)]\n\n    end\n\n    subgraph Consumidores\n        C1[Pipeline 1]\n        C2[Pipeline 2]\n    end\n\n    P1 --&gt; Fila\n    P2 --&gt; Fila\n    P3 --&gt; Fila\n    F1 --- F2\n    F2 --- F3\n    Fila --&gt; C1\n    Fila --&gt; C2</code></pre> <p>Vamos criar <code>pipeline-proc.py</code> contendo um consumidor para processar as mensagens do RabbitMQ:</p> <pre><code>import json\nimport random\nimport time\nimport sys\nimport os\nimport pika\nfrom datetime import datetime\n\n# Configura\u00e7\u00f5es do RabbitMQ via vari\u00e1veis de ambiente\nRABBITMQ_HOST = os.getenv('RABBITMQ_HOST', 'rabbitmq')\nRABBITMQ_PORT = int(os.getenv('RABBITMQ_PORT', '5672'))\nRABBITMQ_USER = os.getenv('RABBITMQ_USER', '')\nRABBITMQ_PASS = os.getenv('RABBITMQ_PASS', '')\nQUEUE_NAME = os.getenv('QUEUE_NAME', '')\n\ndef conectar_rabbitmq():\n    \"\"\"Estabelece conex\u00e3o com RabbitMQ\"\"\"\n    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)\n    parameters = pika.ConnectionParameters(\n        host=RABBITMQ_HOST,\n        port=RABBITMQ_PORT,\n        credentials=credentials\n    )\n\n    try:\n        connection = pika.BlockingConnection(parameters)\n        channel = connection.channel()\n\n        # Declara a fila (garante que existe)\n        channel.queue_declare(queue=QUEUE_NAME, durable=True)\n\n        return connection, channel\n    except Exception as e:\n        print(f\"Erro ao conectar com RabbitMQ: {e}\", flush=True)\n        return None, None\n\ndef processar_mensagem(ch, method, properties, body):\n    \"\"\"Processa uma mensagem recebida do RabbitMQ\"\"\"\n    try:\n        # Decodifica a mensagem\n        mensagem = body.decode('utf-8')\n        dados = json.loads(mensagem)\n\n        # Imprime a mensagem formatada\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(f\"[{timestamp}] Mensagem recebida:\", flush=True)\n        print(f\"  Sensor ID: {dados.get('id', 'N/A')}\", flush=True)\n        print(f\"  Data: {dados.get('data', 'N/A')}\", flush=True)\n        print(f\"  Temperatura: {dados.get('temperatura', 'N/A')}\u00b0C\", flush=True)\n        print(f\"  Mensagem completa: {mensagem}\", flush=True)\n        print(\"-\" * 50, flush=True)\n\n        # Simula processamento com tempo aleat\u00f3rio entre 0.5 e 5 segundos\n        tempo_processamento = random.uniform(0.5, 5.0)\n        print(f\"Processando por {tempo_processamento:.2f} segundos...\", flush=True)\n        time.sleep(tempo_processamento)\n\n        # Confirma o processamento da mensagem\n        ch.basic_ack(delivery_tag=method.delivery_tag)\n        print(\"Mensagem processada com sucesso!\\n\", flush=True)\n\n    except json.JSONDecodeError as e:\n        print(f\"Erro ao decodificar JSON: {e}\", flush=True)\n        print(f\"Mensagem recebida: {body}\", flush=True)\n        # Mesmo com erro, confirma a mensagem para n\u00e3o ficar em loop\n        ch.basic_ack(delivery_tag=method.delivery_tag)\n\n    except Exception as e:\n        print(f\"Erro ao processar mensagem: {e}\", flush=True)\n        # Em caso de erro, rejeita a mensagem sem reprocessar\n        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)\n\ndef wait_for_rabbitmq():\n    \"\"\"Aguarda RabbitMQ ficar dispon\u00edvel\"\"\"\n    max_retries = 30\n    retry_count = 0\n\n    while retry_count &lt; max_retries:\n        try:\n            connection, channel = conectar_rabbitmq()\n            if connection is not None and channel is not None:\n                connection.close()\n                print(\"RabbitMQ est\u00e1 dispon\u00edvel!\", flush=True)\n                return True\n        except Exception as e:\n            pass\n\n        retry_count += 1\n        print(f\"Aguardando RabbitMQ... tentativa {retry_count}/{max_retries}\", flush=True)\n        time.sleep(2)\n\n    print(\"RabbitMQ n\u00e3o ficou dispon\u00edvel ap\u00f3s 60 segundos\", flush=True)\n    return False\n\ndef main():\n    print(\"=== Pipeline de Processamento de Sensores ===\", flush=True)\n\n    # Aguarda RabbitMQ ficar dispon\u00edvel\n    if not wait_for_rabbitmq():\n        print(\"Encerrando: RabbitMQ n\u00e3o est\u00e1 dispon\u00edvel\", flush=True)\n        sys.exit(1)\n\n    # Conecta ao RabbitMQ\n    connection, channel = conectar_rabbitmq()\n    if connection is None or channel is None:\n        print(\"Falha ao estabelecer conex\u00e3o com RabbitMQ\", flush=True)\n        sys.exit(1)\n\n    try:\n        # Configura o consumidor\n        channel.basic_qos(prefetch_count=1)  # Processa uma mensagem por vez\n        channel.basic_consume(\n            queue=QUEUE_NAME,\n            on_message_callback=processar_mensagem\n        )\n\n        print(f\"Aguardando mensagens da fila '{QUEUE_NAME}'. Para sair, pressione CTRL+C\", flush=True)\n        print(\"=\" * 60, flush=True)\n\n        # Inicia o consumo\n        channel.start_consuming()\n\n    except KeyboardInterrupt:\n        print(\"\\nInterrompendo o processamento...\", flush=True)\n        channel.stop_consuming()\n\n    except Exception as e:\n        print(f\"Erro durante o consumo: {e}\", flush=True)\n\n    finally:\n        # Fecha a conex\u00e3o\n        try:\n            if connection and not connection.is_closed:\n                connection.close()\n                print(\"Conex\u00e3o com RabbitMQ fechada\", flush=True)\n        except:\n            pass\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Agora, vamos criar um arquivo <code>Dockerfile.pipeline</code> contendo as instru\u00e7\u00f5es para construir a imagem do cont\u00eainer do pipeline:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Instalar depend\u00eancias\nRUN pip install pika\n\n# Copiar arquivos necess\u00e1rios\nCOPY pipeline-proc.py .\nCOPY .env .\n\n# Definir vari\u00e1vel de ambiente para sa\u00edda n\u00e3o bufferizada\nENV PYTHONUNBUFFERED=1\n\n# Executar o script\nCMD [\"python\", \"-u\", \"pipeline-proc.py\"]\n</code></pre> <p>Ser\u00e1 necess\u00e1rio tamb\u00e9m atualizar o arquivo <code>docker-compose.yml</code> para incluir o novo servi\u00e7o do pipeline:</p> <pre><code>services:\n  rabbitmq:\n    image: rabbitmq:3-management\n    container_name: rabbitmq-sensores\n    restart: always\n    ports:\n      - 5672:5672\n      - 15672:15672\n    volumes:\n      - ./rabbitmq:/var/lib/rabbitmq\n    environment:\n      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}\n      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASS}\n    env_file:\n      - .env\n\n  pipeline-processor:\n    build:\n      context: .\n      dockerfile: Dockerfile.pipeline\n    container_name: pipeline-processor\n    restart: always\n    env_file:\n      - .env\n    depends_on:\n      - rabbitmq\n\n  sensor-1:\n    build: .\n    container_name: sensor-python-1\n    restart: always\n    environment:\n      - SENSOR_ID=1\n    env_file:\n      - .env\n    depends_on:\n      - rabbitmq\n\n  sensor-2:\n    build: .\n    container_name: sensor-python-2\n    restart: always\n    environment:\n      - SENSOR_ID=2\n    env_file:\n      - .env\n    depends_on:\n      - rabbitmq\n\n  sensor-3:\n    build: .\n    container_name: sensor-python-3\n    restart: always\n    environment:\n      - SENSOR_ID=3\n    env_file:\n      - .env\n    depends_on:\n      - rabbitmq\n</code></pre> <p>Exercise</p> <p>Atualize os arquivos necess\u00e1rios para que o pipeline processe as mensagens.</p> Mark as done <p>Exercise</p> <p>Reinicie os servi\u00e7os:</p> <pre><code>$ docker compose build\n$ docker compose up -d\n</code></pre> Mark as done <p>Exercise</p> <p>Em diferentes janelas do terminal (abra lado a lado), confira os logs dos sensores e do pipeline:</p> <pre><code>$ docker logs -f sensor-python-1\n$ docker logs -f sensor-python-2\n$ docker logs -f sensor-python-3\n</code></pre> <p> </p> <pre><code>$ docker logs -f pipeline-processor\n</code></pre> Mark as done <p>Exercise</p> <p>Teste outras varia\u00e7\u00f5es:</p> <ul> <li>Alterar a frequ\u00eancia de envio dos sensores.</li> <li>Adicionar mais sensores.</li> <li>Adicione mais unidades da pipeline de processamento.</li> <li>Altere os tempos de consumo da mensagem (que por enquanto simula um tempo de processamento).</li> <li>Monitore o tamanho da fila no painel do RabbitMQ.</li> </ul> Mark as done <p>Exercise</p> <p>Altere o c\u00f3digo do pipeline para que ele armazene as informa\u00e7\u00f5es dos sensores em um banco de dados relacional.</p> <p>Voc\u00ea pode utilizar coisas como:</p> <ul> <li>SQLite ou PostgreSQL (atualize <code>docker-compose.yml</code> para incluir o servi\u00e7o de banco de dados)</li> <li>ORM para facilitar a intera\u00e7\u00e3o com o banco de dados, como o SQLAlchemy.</li> </ul> Mark as done <p>Exercise</p> <p>Crie uma <code>view</code> que exiba as medidas resumo da \u00faltima hora e dados dos sensores.</p> <p>Algo como: - Temperatura m\u00e9dia, m\u00ednima, m\u00e1xima da \u00faltima hora.</p> Mark as done <p>Exercise</p> <p>Refatore o c\u00f3digo do sensor e pipeline.</p> <p>Separe funcionalidades em m\u00f3dulos (arquivos Python distintos) e/ou classes.</p> Mark as done"},{"location":"classes/02-docker-filas/programar-sensores/","title":"Programar Sensores","text":""},{"location":"classes/02-docker-filas/programar-sensores/#programar-sensores","title":"Programar Sensores","text":"<p>Agora iremos programar os sensores para que eles enviem dados para o RabbitMQ.</p> <p>Primeiro, vamos definir uma fun\u00e7\u00e3o que, a cada um segundo, captura a data atual, gera uma temperatura aleat\u00f3ria e devolve um JSON:</p> <pre><code>import json\nimport random\nimport time\nfrom datetime import datetime\n\ndef gerar_dados_sensor():\n    dados = {\n        \"data\": datetime.now().isoformat(),\n        \"temperatura\": random.uniform(20.0, 30.0)\n    }\n    return json.dumps(dados)\n\nwhile True:\n    dados = gerar_dados_sensor()\n    print(dados)\n    time.sleep(1)\n</code></pre> <p>Exercise</p> <p>Copie o c\u00f3digo em um arquivo <code>sensor.py</code> e execute-o. Voc\u00ea ver\u00e1 que a cada segundo um novo JSON \u00e9 impresso no terminal.</p> <pre><code>$ python sensor.py \n{\"data\": \"2025-08-11T23:16:59.428169\", \"temperatura\": 23.8459438494851}\n{\"data\": \"2025-08-11T23:17:00.428268\", \"temperatura\": 24.943463028936346}\n{\"data\": \"2025-08-11T23:17:01.429249\", \"temperatura\": 20.52063245882019}\n{\"data\": \"2025-08-11T23:17:02.430248\", \"temperatura\": 29.410169922982647}\n</code></pre> Mark as done"},{"location":"classes/02-docker-filas/programar-sensores/#executando-o-sensor-em-um-conteiner-docker","title":"Executando o Sensor em um Cont\u00eainer Docker","text":"<p>Para executar o sensor em um cont\u00eainer Docker, crie um arquivo <code>Dockerfile</code> contendo:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Copiar o arquivo Python\nCOPY sensor.py .\n\n# Definir vari\u00e1vel de ambiente para sa\u00edda n\u00e3o bufferizada\nENV PYTHONUNBUFFERED=1\n\n# Executar o script\nCMD [\"python\", \"-u\", \"sensor.py\"]\n</code></pre> <p>Depois, atualize o <code>docker-compose.yml</code> para incluir o servi\u00e7o do sensor:</p> <pre><code>services:\n  rabbitmq:\n    image: rabbitmq:3-management\n    container_name: rabbitmq-sensores\n    restart: always\n    ports:\n      - 5672:5672\n      - 15672:15672\n    volumes:\n      - ./rabbitmq:/var/lib/rabbitmq\n    environment:\n      - RABBITMQ_DEFAULT_USER=admin\n      - RABBITMQ_DEFAULT_PASS=112233\n\n  sensor:\n    build: .\n    container_name: sensor-python\n    restart: always\n    depends_on:\n      - rabbitmq\n</code></pre> <p>Reinicialize os servi\u00e7os:</p> <pre><code>$ docker compose build\n$ docker compose up -d\n</code></pre> <p>E confira os logs com:</p> <pre><code>$ docker logs sensor-python -f\n</code></pre> <p>Exercise</p> <p>Garanta que consegue ver as mensagens com temperatura no terminal do cont\u00eainer do sensor.</p> Mark as done"},{"location":"classes/02-docker-filas/programar-sensores/#multiplos-sensores","title":"M\u00faltiplos sensores","text":"<p>Vamos atualizar o c\u00f3digo do sensor para receber um <code>ID</code>:</p> C\u00f3digo do sensor <pre><code>import json\nimport random\nimport time\nimport sys\nimport os\nfrom datetime import datetime\n\n# Pega o ID do sensor da vari\u00e1vel de ambiente ou usa 1 como padr\u00e3o\nSENSOR_ID = int(os.getenv('SENSOR_ID', '1'))\n\ndef gerar_dados_sensor():\n    dados = {\n        \"id\": SENSOR_ID,\n        \"data\": datetime.now().isoformat(),\n        \"temperatura\": random.uniform(20.0, 30.0) # Temperatura aleat\u00f3ria (Dummy)\n    }\n    return json.dumps(dados)\n\nwhile True:\n    dados = gerar_dados_sensor()\n    print(dados, flush=True)\n    # O tempo de espera \u00e9 baseado no ID do sensor (1 espera menos, 3 espera mais)\n    time.sleep(SENSOR_ID)\n</code></pre> <p>E atualizar o <code>docker-compose.yml</code> para inicializar tr\u00eas sensores:</p> Atualiza\u00e7\u00e3o do <code>docker-compose.yml</code> <pre><code>services:\n    rabbitmq:\n        image: rabbitmq:3-management\n        container_name: rabbitmq-sensores\n        restart: always\n        ports:\n        - 5672:5672\n        - 15672:15672\n        volumes:\n        - ./rabbitmq:/var/lib/rabbitmq\n        environment:\n        - RABBITMQ_DEFAULT_USER=admin\n        - RABBITMQ_DEFAULT_PASS=112233\n\n    sensor-1:\n        build: .\n        container_name: sensor-python-1\n        restart: always\n        environment:\n        - SENSOR_ID=1\n        depends_on:\n        - rabbitmq\n\n    sensor-2:\n        build: .\n        container_name: sensor-python-2\n        restart: always\n        environment:\n        - SENSOR_ID=2\n        depends_on:\n        - rabbitmq\n\n    sensor-3:\n        build: .\n        container_name: sensor-python-3\n        restart: always\n        environment:\n        - SENSOR_ID=3\n        depends_on:\n        - rabbitmq\n</code></pre> <p>Reinicialize os servi\u00e7os:</p> <pre><code>$ docker compose build\n$ docker compose up -d\n</code></pre> <p>E confira os logs com:</p> <pre><code>$ docker logs sensor-python-1\n$ docker logs sensor-python-2\n$ docker logs sensor-python-3\n</code></pre> <p>Exercise</p> <p>Garanta que consegue ver as mensagens com temperatura no terminal do cont\u00eainer do sensor.</p> <p>Note</p> <p>Voc\u00ea pode usar o comando <code>docker logs -f &lt;container_name&gt;</code> para seguir os logs em tempo real.</p> Mark as done"},{"location":"classes/02-docker-filas/programar-sensores/#conectando-tudo","title":"Conectando tudo!","text":"<p>Agora, vamos fazer com que os dados dos sensores sejam enviados para o RabbitMQ.</p> <p>Fila</p> <p>As mensagens com os dados dos sensores ser\u00e3o enviadas para a fila <code>sensor_data</code> no RabbitMQ.</p> <p>L\u00e1, ficar\u00e3o aguardando para serem processadas.</p> <p>Primeiro, vamos atualizar o <code>Dockerfile</code>:</p> C\u00f3digo do <code>Dockerfile</code> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\n# Instalar depend\u00eancias\nRUN pip install pika\n\n# Copiar arquivos necess\u00e1rios\nCOPY sensor.py .\nCOPY .env .\n\n# Definir vari\u00e1vel de ambiente para sa\u00edda n\u00e3o bufferizada\nENV PYTHONUNBUFFERED=1\n\n# Executar o script\nCMD [\"python\", \"-u\", \"sensor.py\"]\n</code></pre> <p>Crie um arquivo para vari\u00e1veis de ambiente.</p> C\u00f3digo do arquivo <code>.env</code> <pre><code># Configura\u00e7\u00f5es do RabbitMQ\nRABBITMQ_HOST=rabbitmq\nRABBITMQ_PORT=5672\nRABBITMQ_USER=admin\nRABBITMQ_PASS=112233\nQUEUE_NAME=sensor_data\n</code></pre> <p>Atualize o c\u00f3digo <code>sensor.py</code>.</p> C\u00f3digo do arquivo <code>sensor.py</code> <pre><code>import json\nimport random\nimport time\nimport sys\nimport os\nimport pika\nfrom datetime import datetime\n\n# Pega o ID do sensor da vari\u00e1vel de ambiente ou usa 1 como padr\u00e3o\nSENSOR_ID = int(os.getenv('SENSOR_ID', '1'))\n\n# Configura\u00e7\u00f5es do RabbitMQ via vari\u00e1veis de ambiente\nRABBITMQ_HOST = os.getenv('RABBITMQ_HOST', 'rabbitmq')\nRABBITMQ_PORT = int(os.getenv('RABBITMQ_PORT', '5672'))\nRABBITMQ_USER = os.getenv('RABBITMQ_USER', 'admin')\nRABBITMQ_PASS = os.getenv('RABBITMQ_PASS', '')\nQUEUE_NAME = os.getenv('QUEUE_NAME', '')\n\ndef conectar_rabbitmq():\n    \"\"\"Estabelece conex\u00e3o com RabbitMQ\"\"\"\n    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)\n    parameters = pika.ConnectionParameters(\n        host=RABBITMQ_HOST,\n        port=RABBITMQ_PORT,\n        credentials=credentials\n    )\n\n    try:\n        connection = pika.BlockingConnection(parameters)\n        channel = connection.channel()\n\n        # Declara a fila (cria se n\u00e3o existir)\n        channel.queue_declare(queue=QUEUE_NAME, durable=True)\n\n        return connection, channel\n    except Exception as e:\n        print(f\"Erro ao conectar com RabbitMQ: {e}\", flush=True)\n        return None, None\n\n\ndef wait_for_rabbitmq():\n    \"\"\"Aguarda RabbitMQ ficar dispon\u00edvel\"\"\"\n    max_retries = 30\n    retry_count = 0\n\n    while retry_count &lt; max_retries:\n        try:\n            connection, channel = conectar_rabbitmq()\n            if connection is not None and channel is not None:\n                connection.close()\n                print(\"RabbitMQ est\u00e1 dispon\u00edvel!\", flush=True)\n                return True\n        except Exception as e:\n            pass\n\n        retry_count += 1\n        print(f\"Aguardando RabbitMQ... tentativa {retry_count}/{max_retries}\", flush=True)\n        time.sleep(2)\n\n    print(\"RabbitMQ n\u00e3o ficou dispon\u00edvel ap\u00f3s 60 segundos\", flush=True)\n    return False\n\ndef gerar_dados_sensor():\n    dados = {\n        \"id\": SENSOR_ID,\n        \"data\": datetime.now().isoformat(),\n        \"temperatura\": random.uniform(20.0, 30.0) # Temperatura aleat\u00f3ria (Dummy)\n    }\n    return json.dumps(dados)\n\ndef send_message(channel, dados):\n    \"\"\"Envia mensagem para RabbitMQ usando canal existente\"\"\"\n    try:\n        # Publica a mensagem na fila\n        channel.basic_publish(\n            exchange='',\n            routing_key=QUEUE_NAME,\n            body=dados,\n            properties=pika.BasicProperties(\n                delivery_mode=2,  # Torna a mensagem persistente\n            )\n        )\n        print(f\"Mensagem enviada para RabbitMQ: {dados}\", flush=True)\n\n    except Exception as e:\n        print(f\"Erro ao enviar mensagem: {e}\", flush=True)\n        raise  # Re-lan\u00e7a a exce\u00e7\u00e3o para que o loop principal possa lidar com ela\n\n\n# Aguarda RabbitMQ ficar dispon\u00edvel antes de come\u00e7ar\nif not wait_for_rabbitmq():\n    print(\"Encerrando: RabbitMQ n\u00e3o est\u00e1 dispon\u00edvel\", flush=True)\n    sys.exit(1)\n\nprint(f\"Sensor {SENSOR_ID} iniciado!\", flush=True)\n\n# Cria conex\u00e3o uma \u00fanica vez no in\u00edcio\nconnection, channel = conectar_rabbitmq()\nif connection is None or channel is None:\n    print(\"Falha ao estabelecer conex\u00e3o inicial com RabbitMQ\", flush=True)\n    sys.exit(1)\n\ntry:\n    while True:\n        dados = gerar_dados_sensor()\n\n        try:\n            send_message(channel, dados)\n        except Exception as e:\n            # Se houver erro na conex\u00e3o, tenta reconectar\n            print(f\"Erro na conex\u00e3o, tentando reconectar: {e}\", flush=True)\n            try:\n                connection.close()\n            except:\n                pass\n\n            connection, channel = conectar_rabbitmq()\n            if connection is None or channel is None:\n                print(\"Falha na reconex\u00e3o. Tentando novamente no pr\u00f3ximo ciclo.\", flush=True)\n            else:\n                # Tenta enviar a mensagem novamente com a nova conex\u00e3o\n                try:\n                    send_message(channel, dados)\n                except Exception as retry_error:\n                    print(f\"Falha ao reenviar mensagem: {retry_error}\", flush=True)\n\n        # O tempo de espera \u00e9 baseado no ID do sensor (1 espera menos, 3 espera mais)\n        time.sleep(SENSOR_ID)\n\nexcept KeyboardInterrupt:\n    print(f\"Sensor {SENSOR_ID} interrompido pelo usu\u00e1rio\", flush=True)\nexcept Exception as e:\n    print(f\"Sensor {SENSOR_ID} teve erro: {e}\", flush=True)\nfinally:\n    # Fecha a conex\u00e3o ao final\n    try:\n        if connection and not connection.is_closed:\n            connection.close()\n            print(\"Conex\u00e3o com RabbitMQ fechada\", flush=True)\n    except:\n        pass\n</code></pre> <p>Tamb\u00e9m ser\u00e1 necess\u00e1rio atualizar o <code>docker-compose.yml</code>:</p> C\u00f3digo do arquivo <code>docker-compose.yml</code> <pre><code>services:\n    rabbitmq:\n        image: rabbitmq:3-management\n        container_name: rabbitmq-sensores\n        restart: always\n        ports:\n        - 5672:5672\n        - 15675:15672\n        volumes:\n        - ./rabbitmq:/var/lib/rabbitmq\n        environment:\n        - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}\n        - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASS}\n        env_file:\n        - .env\n\n    sensor-1:\n        build: .\n        container_name: sensor-python-1\n        restart: always\n        environment:\n        - SENSOR_ID=1\n        env_file:\n        - .env\n        depends_on:\n        - rabbitmq\n\n    sensor-2:\n        build: .\n        container_name: sensor-python-2\n        restart: always\n        environment:\n        - SENSOR_ID=2\n        env_file:\n        - .env\n        depends_on:\n        - rabbitmq\n\n    sensor-3:\n        build: .\n        container_name: sensor-python-3\n        restart: always\n        environment:\n        - SENSOR_ID=3\n        env_file:\n        - .env\n        depends_on:\n        - rabbitmq\n</code></pre> <p>Reinicialize os servi\u00e7os:</p> <pre><code>$ docker compose build\n$ docker compose up -d\n</code></pre> <p>Exercise</p> <p>No painel do RabbitMQ, acesse a aba de filas (Queues) e confira se:</p> <ul> <li>A fila foi criada</li> <li>Est\u00e1 recebendo mensagens</li> </ul> <p></p> Mark as done <p>Dica: consumir mensagens</p> <p>Enquanto n\u00e3o implementamos um consumidor (pipeline de processamento), voc\u00ea pode clicar no nome da fila e acessar a op\u00e7\u00e3o Get Messages para consumir parte das mensagens.</p> <p>Danger</p> <p>Cuidado ao consumir mensagens diretamente do RabbitMQ, pois isso pode afetar o fluxo de dados da sua aplica\u00e7\u00e3o.</p> <p>Automatic ack ir\u00e1 fazer com que as mensagens obtidas sejam consideradas como propriamente consumidas! Se voc\u00ea estiver em um ambiente de produ\u00e7\u00e3o, isto significa que as mensagens ser\u00e3o removidas da fila, mesmo que o processamento n\u00e3o tenha sido conclu\u00eddo com sucesso.</p> <p></p>"},{"location":"classes/02-docker-filas/rabbitmq/","title":"RabbitMQ","text":""},{"location":"classes/02-docker-filas/rabbitmq/#rabbitmq","title":"RabbitMQ","text":"<p>O RabbitMQ \u00e9 um sistema de mensageria de c\u00f3digo aberto que implementa o padr\u00e3o de fila de mensagens.</p> <p>Ele permite que diferentes partes de uma aplica\u00e7\u00e3o se comuniquem de forma ass\u00edncrona pelo protocolo AMQP, enviando mensagens  entre produtores e consumidores.</p> <p>Info</p> <p>No exemplo, as mensagens s\u00e3o os JSONs de temperatura</p> <pre><code>graph LR\n    subgraph Produtores\n        P1[Sensor 1]\n        P2[Sensor 2]\n        P3[Sensor 3]\n    end\n\n    subgraph Fila\n        direction LR\n        F1[(Msg 1)]\n        F2[(Msg 2)]\n        F3[(Msg n)]\n\n    end\n\n    subgraph Consumidores\n        C1[Pipeline 1]\n        C2[Pipeline 2]\n    end\n\n    P1 --&gt; Fila\n    P2 --&gt; Fila\n    P3 --&gt; Fila\n    F1 --- F2\n    F2 --- F3\n    Fila --&gt; C1\n    Fila --&gt; C2</code></pre> AMQP <p>O AMQP (Advanced Message Queuing Protocol) \u00e9 um protocolo aberto e padronizado para comunica\u00e7\u00e3o entre sistemas por meio de mensagens, usado pelo RabbitMQ.</p> <p>Ele define como as mensagens s\u00e3o formatadas, roteadas, entregues e confirmadas, permitindo que aplica\u00e7\u00f5es escritas em diferentes linguagens e rodando em diferentes plataformas troquem dados de forma confi\u00e1vel e desacoplada.</p> <p>O RabbitMQ \u00e9 amplamente utilizado em arquiteturas de microservi\u00e7os, onde a comunica\u00e7\u00e3o entre servi\u00e7os pode ser feita de forma desacoplada e escal\u00e1vel.</p>"},{"location":"classes/02-docker-filas/rabbitmq/#iniciar-o-rabbitmq","title":"Iniciar o RabbitMQ","text":"<p>Sucesso!</p> <p>Para ter sucesso durante o curso, mantenha a organiza\u00e7\u00e3o!</p> <p>Crie um diret\u00f3rio para manter os arquivos da aula!</p> <pre><code>$ mkdir -p ~/aula02\n$ cd ~/aula02\n</code></pre>"},{"location":"classes/02-docker-filas/rabbitmq/#criar-docker-composeyml","title":"Criar <code>docker-compose.yml</code>","text":"<p>Vamos iniciar um servi\u00e7o RabbitMQ usando Docker.</p> <p>Crie um arquivo <code>docker-compose.yml</code> com o seguinte conte\u00fado:</p> <pre><code>services:\n  rabbitmq:\n    image: rabbitmq:3-management\n    container_name: rabbitmq-sensores\n    restart: always\n    ports:\n      - 5672:5672\n      - 15672:15672\n    volumes:\n      - ./rabbitmq:/var/lib/rabbitmq\n    environment:\n      - RABBITMQ_DEFAULT_USER=admin\n      - RABBITMQ_DEFAULT_PASS=112233\n</code></pre> <p>Warning</p> <p>\u00c9 recomend\u00e1vel utilizar uma senha mais forte que <code>112233</code></p> <p>Question</p> <p>Explique o que significa esta se\u00e7\u00e3o do <code>docker-compose.yml</code>:</p> <pre><code>ports:\n  - 5672:5672\n  - 15672:15672\n</code></pre> Submit <p>Answer</p> <p>A se\u00e7\u00e3o ports mapeia as portas do container (interna) para as portas do host (externa, sua m\u00e1quina):</p> <ul> <li> <p><code>5672:5672</code>: Isso mapeia a porta <code>5672</code> do container RabbitMQ para a porta <code>5672</code> na m\u00e1quina host. A porta <code>5672</code> \u00e9 a porta padr\u00e3o para AMQP (Advanced Message Queuing Protocol), que \u00e9 usada para mensageria pelo RabbitMQ.</p> </li> <li> <p><code>15672:15672</code>: Isso mapeia a porta <code>15672</code> do container RabbitMQ para a porta <code>15672</code> na m\u00e1quina host. A porta <code>15672</code> \u00e9 a porta padr\u00e3o para o RabbitMQ Management Plugin, que fornece uma interface web para gerenciar o RabbitMQ.</p> </li> </ul> <p>Dica</p> <p>Caso ocorra um conflito de porta (outros servi\u00e7os que possam estar usando a mesma porta), voc\u00ea pode alterar a porta do host (a primeira) para um n\u00famero diferente, como <code>15673:15672</code>.</p>"},{"location":"classes/02-docker-filas/rabbitmq/#inicie-o-rabbitmq","title":"Inicie o RabbitMQ","text":"<p>Para iniciar o RabbitMQ, execute o seguinte comando no terminal:</p> <pre><code>$ docker compose up -d\n</code></pre> <p>Isso iniciar\u00e1 o RabbitMQ em segundo plano. Voc\u00ea pode verificar se o RabbitMQ est\u00e1 em execu\u00e7\u00e3o acessando a interface de gerenciamento em http://localhost:15672 com o usu\u00e1rio e senha definidos no <code>docker-compose.yml</code>.</p> <p>Para conferir se o RabbitMQ est\u00e1 em execu\u00e7\u00e3o, voc\u00ea pode usar o seguinte comando:</p> <pre><code>$ docker compose ps\n</code></pre> <p>E para acompanhar os logs:</p> <pre><code>$ docker compose logs -f\n</code></pre> Uma outra forma <p>Voc\u00ea pode usar o comando <code>docker logs</code> para visualizar os logs do container diretamente:</p> <pre><code>$ docker logs -f rabbitmq-sensores\n</code></pre> <p>O mesmo para <code>docker ps</code>:</p> <pre><code>$ docker ps\n</code></pre>"},{"location":"classes/02-docker-filas/rabbitmq/#painel-rabbitmq","title":"Painel RabbitMQ","text":"<p>Acesse a interface de gerenciamento do RabbitMQ em http://localhost:15672 com o usu\u00e1rio e senha definidos no <code>docker-compose.yml</code>.</p> <p>Problema</p> <p>Caso voc\u00ea n\u00e3o consiga acessar a interface web, verifique se o container do RabbitMQ est\u00e1 em execu\u00e7\u00e3o e se as portas est\u00e3o corretamente mapeadas.</p> <p></p>"},{"location":"classes/02-docker-filas/relembrar-ciclo/","title":"Relembrar","text":""},{"location":"classes/02-docker-filas/relembrar-ciclo/#relembrar","title":"Relembrar","text":"<p>Em nossa primeira aula, exploramos a defini\u00e7\u00e3o de engenharia de dados e os principais profissionais t\u00e9cnicos da \u00e1rea</p> <p>Um dos principais conceitos discutidos foi o ciclo de vida de engenharia de dados, voc\u00ea se lembra dele?!</p> <p>Exercise</p> <p>Quais eram os principais componentes do ciclo de vida de engenharia de dados?</p> Submit <p>Answer</p> <p>Ingest\u00e3o, Transforma\u00e7\u00e3o e Disponibiliza\u00e7\u00e3o.</p> <pre><code>flowchart LR\n\n%% Plataforma principal\nsubgraph PD[Plataforma de Dados]\n    direction LR\n\n    G[Gera\u00e7\u00e3o]\n\n    %% Linha horizontal de entrada + pipeline\n    subgraph LINE[Armazenamento]\n    direction LR\n    I[Ingest\u00e3o]\n    T[Transforma\u00e7\u00e3o]\n    S[Disponibiliza\u00e7\u00e3o]\n    I --&gt; T --&gt; S\n    end\n\n    %% Sa\u00eddas diretas (\u00e0 direita)\n    ML[Aprendizado de M\u00e1quina]\n    AN[An\u00e1lises]\n    REP[Dashboards]\n\n    G --&gt; I\n    S --&gt; ML\n    S --&gt; AN\n    S --&gt; REP\nend\n\n%% Estilos adaptados para light e dark mode\nclassDef gen fill:#64748b,stroke:#475569,color:#ffffff,stroke-width:2px;\nclassDef stage fill:#0891b2,stroke:#0e7490,color:#ffffff,stroke-width:2px;\nclassDef trans fill:#8b5cf6,stroke:#7c3aed,color:#ffffff,stroke-width:2px;\nclassDef serve fill:#10b981,stroke:#059669,color:#ffffff,stroke-width:2px;\nclassDef out fill:#f59e0b,stroke:#d97706,color:#ffffff,stroke-width:2px;\n\nclass G gen;\nclass I stage;\nclass T trans;\nclass S serve;\nclass ML,AN,REP out;</code></pre> <p>No final da aula, fizemos um warm up, onde fizemos a leitura de dados do servi\u00e7o S3 (Amazon Simple Storage Service) da AWS (Amazon Web Services). Os dados passaram por uma defini\u00e7\u00e3o de schema e posterior escrita no pr\u00f3prio S3.</p> <p>Exercise</p> <p>Como o script final do seu warm up se relaciona com o ciclo de vida de engenharia de dados? Existiam se\u00e7\u00f5es de c\u00f3digo que realizaram tarefas relativas a cada etapa do ciclo?</p> Submit <p>Answer</p> <p>O script percorre as tr\u00eas etapas:</p> <ol> <li> <p>Ingest\u00e3o: realizada ao ler dados do bucket S3.  </p> </li> <li> <p>Transforma\u00e7\u00e3o: ao definir um schema (tipos de dados, nomes de colunas). Mesmo que n\u00e3o haja c\u00e1lculos complexos, normaliza\u00e7\u00f5es ou jun\u00e7\u00f5es, a simples convers\u00e3o e padroniza\u00e7\u00e3o de tipos \u00e9 parte da transforma\u00e7\u00e3o.</p> </li> <li> <p>Disponibiliza\u00e7\u00e3o (Serving): ao escrever no S3 em formato Parquet, disponibilizamos os dados em um formato otimizado para consulta e processamento posterior (exemplo: ferramentas anal\u00edticas).</p> </li> </ol> <p>Nosso warm up da aula passada Serviu como um exemplo pr\u00e1tico de como os dados podem ser ingeridos, transformados e disponibilizados em um fluxo de trabalho simples de engenharia de dados.</p> <p>Entretanto, \u00e9 importante notar que come\u00e7amos com uma representa\u00e7\u00e3o bem simplista.</p> <p>Por exemplo, a etapa de ingest\u00e3o foi representada apenas pela leitura de dados do S3, mas na pr\u00e1tica, ela pode envolver uma variedade de fontes e m\u00e9todos de coleta de dados, como APIs, bancos de dados, arquivos CSV, entre outros. Raramente teremos apenas poucos arquivos e raramente eles ser\u00e3o est\u00e1ticos.</p> <p>Em cen\u00e1rios de Big Data , os dados tendem a ser gerados continuamente, em grandes volumes e a partir de m\u00faltiplas origens, como sensores IoT, registros de transa\u00e7\u00f5es, redes sociais, sistemas corporativos e fluxos de streaming. Al\u00e9m disso, a natureza din\u00e2mica e heterog\u00eanea dessas fontes exige que a ingest\u00e3o seja capaz de lidar com dados estruturados, semiestruturados e n\u00e3o estruturados, frequentemente em tempo real.</p> <p>Os cinco Vs do Big Data:</p> <ol> <li>Volume \u2013 Quantidade massiva de dados gerados.</li> <li>Velocidade \u2013 Rapidez com que os dados s\u00e3o gerados, processados e analisados.</li> <li>Variedade \u2013 Diversidade de formatos e fontes dos dados.</li> <li>Veracidade \u2013 Confiabilidade e qualidade das informa\u00e7\u00f5es.</li> <li>Valor \u2013 Utilidade e relev\u00e2ncia dos dados para gerar insights.</li> </ol> <p>Ao final do curso, nosso objetivo \u00e9 capacitar voc\u00ea a lidar com esses desafios e a construir solu\u00e7\u00f5es de engenharia de dados que sejam escal\u00e1veis, eficientes e capazes de extrair valor real dos dados. Come\u00e7aremos com cen\u00e1rios mais simples e controlados, mas gradualmente avan\u00e7aremos para situa\u00e7\u00f5es mais complexas e realistas.</p> <p>Question</p> <p>Ent\u00e3o a ingest\u00e3o de dados envolver\u00e1, necessariamente, que algum script seja ativado e fa\u00e7a a leitura de dados de alguma fonte (como o S3)?</p> Sim N\u00e3o Submit <p>Answer</p> <p>N\u00e3o, veremos na sequ\u00eancia!</p> <p>Em engenharia de dados, a ingest\u00e3o pode funcionar seguindo um padr\u00e3o:</p> <ul> <li>Push (a ingest\u00e3o \u00e9 chamada): a fonte envia dados para voc\u00ea.</li> <li>Pull (a ingest\u00e3o chama a fonte): ocorre a busca de dados na fonte.</li> <li>H\u00edbrido: mistura os dois.</li> </ul>"},{"location":"classes/02-docker-filas/relembrar-ciclo/#ingestao-push-based","title":"Ingest\u00e3o Push-based","text":"<p>Neste formato de ingest\u00e3o, a fonte empurra eventos/dados para um endpoint seu. Este modelo \u00e9 particularmente bom para cen\u00e1rios de tempo real.</p> <p>Analogia: notifica\u00e7\u00f5es no celular (voc\u00ea n\u00e3o pede, elas chegam).</p> <p>Exemplo</p> <p>Lembra do Webhook configurado para as corre\u00e7\u00f5es das atividades de SisHard?</p> <p>As cria\u00e7\u00f5es de tag batem no endpoint do servidor de corre\u00e7\u00e3o como um JSON.</p>"},{"location":"classes/02-docker-filas/relembrar-ciclo/#ingestao-pull-based","title":"Ingest\u00e3o Pull-based","text":"<p>Segundo este formato, o pipeline ou script consulta/baixa dados periodicamente. Este \u00e9 um formato adequado para processamento em lotes (iremos abordar isso mais adiante) e integra\u00e7\u00f5es com sistemas legados.</p> <p>Analogia: checar e-mail manualmente, uma vez que voc\u00ea vai l\u00e1 e busca.</p> <p>Exemplo</p> <p>Um script di\u00e1rio que l\u00ea um banco de dados e grava no S3</p>"},{"location":"classes/03-data-warehouse/clickhouse/","title":"ClickHouse","text":""},{"location":"classes/03-data-warehouse/clickhouse/#clickhouse","title":"ClickHouse","text":"<p>O ClickHouse \u00e9 um sistema de gerenciamento de banco de dados (SGBD) anal\u00edtico columnar (OLAP) de c\u00f3digo aberto, desenvolvido pela Yandex. \u00c9 otimizado especificamente para consultas anal\u00edticas em tempo real sobre grandes volumes de dados.</p> <p>Info</p> <p>O ClickHouse \u00e9 conhecido por sua excepcional velocidade em consultas anal\u00edticas.</p> <p>Utiliza armazenamento columnar e processamento paralelo massivo (MPP) para alta performance.</p>"},{"location":"classes/03-data-warehouse/clickhouse/#caracteristicas-principais","title":"Caracter\u00edsticas principais","text":"<ul> <li>Columnar storage: Armazena dados por colunas, otimizando consultas anal\u00edticas</li> <li>Compress\u00e3o avan\u00e7ada: Algoritmos de compress\u00e3o espec\u00edficos para cada tipo de dado</li> <li>SQL completo: Suporte robusto ao SQL com extens\u00f5es para an\u00e1lise avan\u00e7ada</li> <li>Distribu\u00eddo: Escalabilidade horizontal nativa com clusters</li> <li>Real-time: Inser\u00e7\u00f5es e consultas em tempo real</li> <li>Formatos diversos: Suporte nativo a Parquet, CSV, JSON e outros formatos</li> </ul>"},{"location":"classes/03-data-warehouse/clickhouse/#preparando-o-ambiente","title":"Preparando o ambiente","text":"<p>Vamos configurar um ambiente com ClickHouse utilizando Docker e trabalhar com os mesmos dados da aula de DuckDB.</p> <p>Exercise</p> <p>Crie um diret\u00f3rio para o projeto:</p> <pre><code>$ mkdir clickhouse\n$ cd clickhouse\n</code></pre> Mark as done <p>Exercise</p> <p>Crie um arquivo <code>clickhouse/docker-compose.yml</code> para configurar o ClickHouse:</p> <p>Aten\u00e7\u00e3o!</p> <p>Caso seja necess\u00e1rio, edite as portas externas!</p> <pre><code>services:\n  clickhouse:\n    image: clickhouse/clickhouse-server:25.7.4.11-alpine\n    container_name: clickhouse-server\n    ports:\n      - \"8123:8123\"\n      - \"9000:9000\"\n    environment:\n      CLICKHOUSE_DB: bike_share_clickhouse\n      CLICKHOUSE_USER: admin\n      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1\n      CLICKHOUSE_PASSWORD: password123\n    volumes:\n      - clickhouse-data:/var/lib/clickhouse\n      - ./dados:/var/lib/clickhouse/user_files\n    ulimits:\n      nofile:\n        soft: 262144\n        hard: 262144\n\nvolumes:\n  clickhouse-data:\n</code></pre> <p>Info!</p> <p>O ClickHouse s\u00f3 permite acesso a arquivos dentro do diret\u00f3rio <code>user_files</code> por quest\u00f5es de seguran\u00e7a.</p> <p>Ao fazer ingest\u00e3o, acessaremos <code>./dados</code> via <code>/var/lib/clickhouse/user_files/</code> dentro do container.</p> <p>Aten\u00e7\u00e3o!</p> <p>Observe que, com a defini\u00e7\u00e3o do volume <code>clickhouse-data</code>, teremos persist\u00eancia dos dados mesmo ap\u00f3s a parada ou remo\u00e7\u00e3o do container.</p> <p>Confira os volumes com (s\u00f3 ir\u00e1 aparecer ap\u00f3s voc\u00ea iniciar o container):</p> <pre><code>$ docker volume ls\n</code></pre> Mark as done <p>Exercise</p> <p>O que a se\u00e7\u00e3o ulimits est\u00e1 configurando?</p> Submit <p>Answer</p> <p>O ulimits est\u00e1 configurando o n\u00famero m\u00e1ximo de arquivos abertos que o ClickHouse pode ter, aumentando o limite padr\u00e3o para 262144.</p> <p>Exercise</p> <p>Crie uma pasta <code>clickhouse/dados</code> e copie os CSVs e Parquets criados na aula de DuckDB.</p> Mark as done <p>Exercise</p> <p>Inicie o container do ClickHouse:</p> <pre><code>$ docker compose up -d\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique se o ClickHouse est\u00e1 rodando e se o volume foi criado:</p> <pre><code>$ docker logs clickhouse-server\n$ docker volume ls\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#interface-de-acesso","title":"Interface de acesso","text":"<p>O ClickHouse oferece m\u00faltiplas interfaces de acesso: HTTP, cliente nativo, e diversas linguagens de programa\u00e7\u00e3o (Java, Python, Go, etc.).</p>"},{"location":"classes/03-data-warehouse/clickhouse/#cliente-http","title":"Cliente HTTP","text":"<p>Exercise</p> <p>Teste a conex\u00e3o via HTTP:</p> <p>Info!</p> <p>Esta chamada retorna a vers\u00e3o do ClickHouse.</p> <pre><code>$ curl 'http://admin:password123@localhost:8123/' -d 'SELECT version()'\n</code></pre> <p>N\u00e3o tenho <code>curl</code></p> <p>Caso n\u00e3o tenha curl, acesse a URL diretamente em seu navegador:</p> <p>http://localhost:8123/?query=SELECT%20version()</p> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#cliente-cli","title":"Cliente CLI","text":"<p>Exercise</p> <p>Acesse o cliente ClickHouse dentro do container:</p> <pre><code>$ docker exec -it clickhouse-server clickhouse-client --user admin --password password123\n</code></pre> <p>Voc\u00ea ver\u00e1 um prompt <code>clickhouse-server :)</code> onde pode executar comandos SQL.</p> Mark as done <p>Exercise</p> <p>Execute a seguinte query para ver informa\u00e7\u00f5es do sistema:</p> <p>Aten\u00e7\u00e3o!</p> <p>As queries (esta e as pr\u00f3ximas) devem ser executadas no terminal que est\u00e1 conectado ao Clickhouse!</p> <p>Aquele que voc\u00ea iniciou com o comando:</p> <pre><code>docker exec -it clickhouse-server clickhouse-client --user admin --password password123\n</code></pre> <p>Query:</p> <pre><code>SELECT \n    name,\n    value\nFROM system.settings \nWHERE name LIKE '%thread%'\nLIMIT 5;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#obtendo-os-dados","title":"Obtendo os dados","text":"<p>Vamos utilizar os mesmos dados de bicicletas de S\u00e3o Francisco que usamos na aula do DuckDB.</p>"},{"location":"classes/03-data-warehouse/clickhouse/#criando-databases-e-tabelas","title":"Criando databases e tabelas","text":""},{"location":"classes/03-data-warehouse/clickhouse/#database","title":"Database","text":"<p>Exercise</p> <p>No cliente ClickHouse, crie um database:</p> <pre><code>CREATE DATABASE IF NOT EXISTS bike_share;\nUSE bike_share;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#importando-dados-csv","title":"Importando dados CSV","text":"<p>Vamos come\u00e7ar criando tabelas e importando os dados CSV.</p> <p>Exercise</p> <p>Crie a tabela <code>station</code>:</p> <pre><code>CREATE TABLE station\n(\n    id UInt32,\n    name String,\n    lat Float64,\n    long Float64,\n    dockcount UInt32,\n    landmark String,\n    installation String\n)\nENGINE = MergeTree()\nORDER BY id;\n</code></pre> Mark as done <p>Exercise</p> <p>Importe os dados das esta\u00e7\u00f5es:</p> <p>Aten\u00e7\u00e3o!</p> <p>Este comando deve ser rodado em outro terminal da sua m\u00e1quina e n\u00e3o no que est\u00e1 conectado como cliente.</p> <p>Ele sup\u00f5e que seu diret\u00f3rio de trabalho atual \u00e9 a pasta <code>clickhouse</code>.</p> <p>Path!</p> <p>Antes de executar o comando, garanta que est\u00e1 no path correto, que \u00e9 a raiz da aula!</p> <p>Nesta pasta, deve haver uma sub-pasta <code>dados</code>.</p> <pre><code>$ docker exec -i clickhouse-server clickhouse-client --user admin --password password123 --query=\"INSERT INTO bike_share.station FORMAT CSVWithNames\" &lt; dados/station.csv\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique os dados importados:</p> <p>Aten\u00e7\u00e3o!</p> <p>Copie, cole e execute os comandos abaixo, um por vez, no terminal que est\u00e1 conectado ao Clickhouse!</p> <pre><code>SHOW TABLES;\nSELECT COUNT(*) FROM station;\nSELECT * FROM station LIMIT 5;\n</code></pre> Mark as done <p>Exercise</p> <p>Acesse http://localhost:8123/dashboard para monitorar o servidor!</p> <p>Aten\u00e7\u00e3o!</p> <p>Informe o usu\u00e1rio e senha!</p> Mark as done <p>Exercise</p> <p>Crie a tabela <code>trip</code> com tipos de dados otimizados:</p> <pre><code>CREATE TABLE trip\n(\n    id UInt64,\n    duration UInt32,\n    start_date DateTime,\n    start_station_name String,\n    start_station_id UInt32,\n    end_date DateTime,\n    end_station_name String,\n    end_station_id UInt32,\n    bike_id UInt32,\n    subscription_type String,\n    zip_code String\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(start_date)\nORDER BY (start_date, start_station_id);\n</code></pre> Mark as done <p>Particionamento</p> <p>O ClickHouse permite particionamento eficiente dos dados. Aqui particionamos por m\u00eas da data de in\u00edcio.</p> <p>Exercise</p> <p>Importe os dados das viagens. Para isso, no cliente ClickHouse, execute a query:</p> <pre><code>INSERT INTO trip\nSELECT \n    toUInt64(id) as id,\n    toUInt32(duration) as duration,\n    parseDateTimeBestEffort(start_date) as start_date,\n    start_station_name,\n    toUInt32(start_station_id) as start_station_id,\n    parseDateTimeBestEffort(end_date) as end_date,\n    end_station_name,\n    toUInt32(end_station_id) as end_station_id,\n    toUInt32(bike_id) as bike_id,\n    subscription_type,\n    zip_code\nFROM file('trip.csv', 'CSVWithNames');\n</code></pre> <p>Magic do ClickHouse</p> <p>A fun\u00e7\u00e3o <code>parseDateTimeBestEffort()</code> consegue interpretar automaticamente formatos de data diversos, incluindo o formato americano MM/DD/YYYY HH:MM.</p> <p>Caminho do arquivo</p> <p>Note que agora usamos apenas <code>'trip.csv'</code> (sem caminho) porque o arquivo est\u00e1 mapeado diretamente no diret\u00f3rio <code>user_files</code>.</p> Mark as done <p>Exercise</p> <p>Verifique se os dados foram importados corretamente:</p> <pre><code>SELECT COUNT(*) FROM trip;\nSELECT * FROM trip LIMIT 5;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#consultas-basicas","title":"Consultas b\u00e1sicas","text":""},{"location":"classes/03-data-warehouse/clickhouse/#analise-exploratoria","title":"An\u00e1lise explorat\u00f3ria","text":"<p>Exercise</p> <p>Execute consultas b\u00e1sicas para entender os dados:</p> <pre><code>-- Contagem total de viagens\nSELECT COUNT(*) as total_trips FROM trip;\n\n-- Dura\u00e7\u00e3o m\u00e9dia das viagens\nSELECT AVG(duration) as avg_duration_seconds FROM trip;\n\n-- Top 5 esta\u00e7\u00f5es mais utilizadas como ponto de partida\nSELECT \n    start_station_name,\n    COUNT(*) as trip_count\nFROM trip \nGROUP BY start_station_name \nORDER BY trip_count DESC \nLIMIT 5;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#funcoes-de-data-e-tempo","title":"Fun\u00e7\u00f5es de data e tempo","text":"<p>O ClickHouse possui fun\u00e7\u00f5es avan\u00e7adas para manipula\u00e7\u00e3o de datas e tempo.</p> <p>Exercise</p> <p>Analise padr\u00f5es temporais:</p> <pre><code>-- Viagens por dia da semana\nSELECT \n    toDayOfWeek(start_date) as day_of_week,\n    COUNT(*) as trip_count\nFROM trip \nGROUP BY day_of_week \nORDER BY day_of_week;\n\n-- Viagens por hora do dia\nSELECT \n    toHour(start_date) as hour_of_day,\n    COUNT(*) as trip_count\nFROM trip \nGROUP BY hour_of_day \nORDER BY hour_of_day;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#consultas-analiticas-avancadas","title":"Consultas anal\u00edticas avan\u00e7adas","text":""},{"location":"classes/03-data-warehouse/clickhouse/#window-functions","title":"Window Functions","text":"<p>O ClickHouse suporta window functions para an\u00e1lises sofisticadas.</p> <p>Exercise</p> <p>Utilize window functions para an\u00e1lises avan\u00e7adas:</p> <pre><code>-- Ranking das esta\u00e7\u00f5es mais populares por m\u00eas\nSELECT \n    toYYYYMM(start_date) as month,\n    start_station_name,\n    COUNT(*) as trip_count,\n    ROW_NUMBER() OVER (PARTITION BY toYYYYMM(start_date) ORDER BY COUNT(*) DESC) as rank\nFROM trip \nGROUP BY month, start_station_name \nORDER BY month, rank\nLIMIT 20;\n</code></pre> Mark as done <p>Exercise</p> <p>Calcule m\u00e9dias m\u00f3veis:</p> <pre><code>-- M\u00e9dia m\u00f3vel de viagens por dia (janela de 7 dias)\nSELECT \n    date,\n    daily_trips,\n    AVG(daily_trips) OVER (ORDER BY date ROWS 6 PRECEDING) as moving_avg_7days\nFROM (\n    SELECT \n        toDate(start_date) as date,\n        COUNT(*) as daily_trips\n    FROM trip \n    GROUP BY date\n    ORDER BY date\n)\nORDER BY date;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#agregacoes-complexas","title":"Agrega\u00e7\u00f5es complexas","text":"<p>Exercise</p> <p>Execute an\u00e1lises com m\u00faltiplas agrega\u00e7\u00f5es:</p> <pre><code>-- Estat\u00edsticas por tipo de assinatura\nSELECT \n    subscription_type,\n    COUNT(*) as trip_count,\n    AVG(duration) as avg_duration,\n    MIN(duration) as min_duration,\n    MAX(duration) as max_duration,\n    quantile(0.5)(duration) as median_duration,\n    quantile(0.95)(duration) as p95_duration\nFROM trip \nGROUP BY subscription_type;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#trabalhando-com-formatos-avancados","title":"Trabalhando com formatos avan\u00e7ados","text":""},{"location":"classes/03-data-warehouse/clickhouse/#importando-dados-parquet","title":"Importando dados Parquet","text":"<p>O ClickHouse tem suporte nativo ao formato Parquet.</p> <p>Exercise</p> <p>Crie uma tabela lendo diretamente do Parquet (com convers\u00e3o de tipos):</p> <pre><code>CREATE TABLE trip_parquet\n(\n    id String,\n    duration Int32,\n    start_date DateTime,\n    start_station_name String,\n    start_station_id Int32,\n    end_date DateTime,\n    end_station_name String,\n    end_station_id Int32,\n    bike_id String,\n    subscription_type String,\n    zip_code String\n)\nENGINE = MergeTree\nPARTITION BY toYYYYMM(start_date)\nORDER BY (start_date, start_station_id);\n\n-- Then insert the transformed data\nINSERT INTO trip_parquet\nSELECT\n    id,\n    duration,\n    parseDateTimeBestEffort(start_date) AS start_date,\n    start_station_name,\n    start_station_id,\n    parseDateTimeBestEffort(end_date) AS end_date,\n    end_station_name,\n    end_station_id,\n    bike_id,\n    subscription_type,\n    zip_code\nFROM file('parquets/trip.parquet', 'Parquet');\n</code></pre> <p>Convers\u00e3o de tipos</p> <p>Como o ClickHouse pode interpretar colunas de data como String ao ler Parquet, usamos <code>parseDateTimeBestEffort()</code> para converter adequadamente os tipos de data tanto na parti\u00e7\u00e3o quanto na ordena\u00e7\u00e3o.</p> Mark as done <p>Exercise</p> <p>Teste a leitura da tabela rec\u00e9m criada com:</p> <pre><code>SELECT *\nFROM trip_parquet\nLIMIT 10;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#formatos-de-saida","title":"Formatos de sa\u00edda","text":"<p>Exercise</p> <p>O ClickHouse suporta m\u00faltiplos formatos de sa\u00edda:</p> <pre><code>-- JSON\nSELECT * FROM station LIMIT 3 FORMAT JSON;\n\n-- CSV\nSELECT * FROM station LIMIT 3 FORMAT CSV;\n\n-- Pretty (formatado para humanos!)\nSELECT * FROM station LIMIT 3 FORMAT Pretty;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#analises-geoespaciais","title":"An\u00e1lises geoespaciais","text":"<p>O ClickHouse possui fun\u00e7\u00f5es geoespaciais avan\u00e7adas.</p> <p>Exercise</p> <p>Fa\u00e7a an\u00e1lises baseadas em localiza\u00e7\u00e3o:</p> <pre><code>-- Jun\u00e7\u00e3o para obter coordenadas das esta\u00e7\u00f5es\nCREATE TABLE trip_with_coords\nENGINE = MergeTree\nORDER BY (start_station_id, end_station_id)\nAS SELECT \n    t.*,\n    s1.lat as start_lat,\n    s1.long as start_long,\n    s2.lat as end_lat, \n    s2.long as end_long\nFROM trip t\nLEFT JOIN station s1 ON t.start_station_id = s1.id\nLEFT JOIN station s2 ON t.end_station_id = s2.id;\n</code></pre> Mark as done <p>Exercise</p> <p>Calcule dist\u00e2ncias entre esta\u00e7\u00f5es:</p> <pre><code>-- Dist\u00e2ncia aproximada entre esta\u00e7\u00f5es (em metros)\nSELECT \n    start_station_name,\n    end_station_name,\n    COUNT(*) as trip_count,\n    geoDistance(start_long, start_lat, end_long, end_lat) as distance_meters\nFROM trip_with_coords \nWHERE start_lat IS NOT NULL AND end_lat IS NOT NULL\nGROUP BY start_station_name, end_station_name, start_lat, start_long, end_lat, end_long\nHAVING trip_count &gt; 10\nORDER BY distance_meters DESC\nLIMIT 10;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#monitoramento-e-metricas","title":"Monitoramento e m\u00e9tricas","text":""},{"location":"classes/03-data-warehouse/clickhouse/#system-tables","title":"System tables","text":"<p>O ClickHouse oferece tabelas de sistema para monitoramento.</p> <p>Exercise</p> <p>Explore as tabelas de sistema:</p> <pre><code>-- Ver queries em execu\u00e7\u00e3o\nSELECT * FROM system.processes;\n\n-- Estat\u00edsticas das tabelas\nSELECT \n    name,\n    rows,\n    bytes_on_disk,\n    formatReadableSize(bytes_on_disk) as size\nFROM system.parts \nWHERE database = 'bike_share';\n\n-- Performance das queries\nSELECT \n    query,\n    read_rows,\n    read_bytes,\n    result_rows,\n    memory_usage,\n    query_duration_ms\nFROM system.query_log \nWHERE type = 'QueryFinish'\nORDER BY query_duration_ms DESC\nLIMIT 5;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#integracao-com-python","title":"Integra\u00e7\u00e3o com Python","text":"<p>Exercise</p> <p>Crie ou ative um ambiente virtual adequado com Python 3.12!</p> Mark as done <p>Exercise</p> <p>Instale o cliente Python para ClickHouse:</p> Arquivo <code>requirements.txt</code> <pre><code>clickhouse-connect==0.8.18\npandas==2.3.1\npython-dotenv==1.0.1\n</code></pre> <pre><code>$ uv pip install -r requirements.txt\n</code></pre> Mark as done <p>Exercise</p> <p>Crie um script <code>src/clickhouse_python.py</code> para interagir com ClickHouse:</p> Arquivo <code>.env</code> <pre><code># ClickHouse Configuration\nCLICKHOUSE_HOST=localhost\nCLICKHOUSE_PORT=8123\nCLICKHOUSE_USERNAME=admin\nCLICKHOUSE_PASSWORD=password123\nCLICKHOUSE_DATABASE=bike_share\n</code></pre> <pre><code>import clickhouse_connect\nimport pandas as pd\nimport os\nfrom dotenv import load_dotenv\n\n# Carregar vari\u00e1veis de ambiente do arquivo .env\nload_dotenv()\n\n# Conectar ao ClickHouse usando vari\u00e1veis de ambiente\nclient = clickhouse_connect.get_client(\n    host=os.getenv('CLICKHOUSE_HOST', 'localhost'),\n    port=int(os.getenv('CLICKHOUSE_PORT', 8123)),\n    username=os.getenv('CLICKHOUSE_USERNAME', 'admin'), \n    password=os.getenv('CLICKHOUSE_PASSWORD'),\n    database=os.getenv('CLICKHOUSE_DATABASE', 'bike_share')\n)\n\n# Executar consulta e obter DataFrame\nresult = client.query_df(\"\"\"\n    SELECT \n        start_station_name,\n        COUNT(*) as trip_count,\n        AVG(duration) as avg_duration\n    FROM trip \n    GROUP BY start_station_name \n    ORDER BY trip_count DESC \n    LIMIT 10\n\"\"\")\n\nprint(\"Top 10 esta\u00e7\u00f5es mais utilizadas:\")\nprint(result)\n\n# Inserir dados via DataFrame\nsample_data = pd.DataFrame({\n    'id': [999, 1000],\n    'name': ['Test Station 1', 'Test Station 2'], \n    'lat': [37.7849, 37.7849],\n    'long': [-122.4194, -122.4094],\n    'dockcount': [15, 20],\n    'landmark': ['San Francisco', 'San Francisco'],\n    'installation': ['2023-01-01', '2023-01-02']\n})\n\nclient.insert_df('station', sample_data)\nprint(\"Dados inseridos com sucesso!\")\n</code></pre> Mark as done <p>Exercise</p> <p>Rode o script com:</p> <pre><code>$ python src/clickhouse_python.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#comparacao-de-performance","title":"Compara\u00e7\u00e3o de Performance","text":"<p>Exercise</p> <p>Execute um benchmark simples comparando diferentes tipos de consulta:</p> <pre><code>-- Consulta simples de agrega\u00e7\u00e3o\nSELECT \n    subscription_type, \n    COUNT(*) \nFROM trip \nGROUP BY subscription_type;\n\n-- Consulta com filtro temporal\nSELECT COUNT(*) \nFROM trip \nWHERE start_date &gt;= '2013-09-01' AND start_date &lt; '2013-10-01';\n\n-- Consulta complexa com m\u00faltiplas agrega\u00e7\u00f5es\nSELECT \n    toYYYYMM(start_date) as month,\n    start_station_name,\n    COUNT(*) as trips,\n    AVG(duration) as avg_duration,\n    quantile(0.95)(duration) as p95_duration\nFROM trip \nGROUP BY month, start_station_name\nHAVING trips &gt; 50\nORDER BY month, trips DESC;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#exercicios-finais","title":"Exerc\u00edcios finais","text":"<p>Exercise</p> <p>Compare o ClickHouse com o DuckDB em rela\u00e7\u00e3o aos casos de uso. Quando voc\u00ea usaria cada um?</p> Submit <p>Answer</p> <p>O ClickHouse \u00e9 otimizado para consultas anal\u00edticas em grandes volumes de dados, enquanto o DuckDB \u00e9 mais adequado para an\u00e1lises em ambientes locais e em menor escala.</p> <p>Usaria o ClickHouse para an\u00e1lises em tempo real e grandes conjuntos de dados, enquanto o DuckDB seria ideal para prot\u00f3tipos r\u00e1pidos e an\u00e1lises ad-hoc.</p> <p>Exercise</p> <p>Crie uma an\u00e1lise que:</p> <ol> <li>Identifique os fluxos de viagens mais populares (origem \u2192 destino)</li> <li>Calcule estat\u00edsticas temporais (padr\u00f5es por hora, dia da semana)</li> <li>Analise a utiliza\u00e7\u00e3o das bicicletas individuais</li> <li>Exporte os resultados em diferentes formatos</li> </ol> Mark as done <p>Exercise</p> <p>Quais s\u00e3o as principais vantagens do armazenamento columnar do ClickHouse para consultas anal\u00edticas?</p> Submit <p>Documenta\u00e7\u00e3o</p> <p>Consulte a documenta\u00e7\u00e3o oficial do ClickHouse para mais recursos avan\u00e7ados!</p>"},{"location":"classes/03-data-warehouse/clickhouse/#substituindo-postgresql-por-clickhouse-no-data-warehouse","title":"Substituindo PostgreSQL por ClickHouse no Data Warehouse","text":"<p>Agora que voc\u00ea domina os conceitos b\u00e1sicos do ClickHouse, vamos aplic\u00e1-lo em um cen\u00e1rio pr\u00e1tico: substituir o PostgreSQL do Data Warehouse da aula anterior (DW Parte I) pelo ClickHouse.</p>"},{"location":"classes/03-data-warehouse/clickhouse/#preparando-o-ambiente_1","title":"Preparando o ambiente","text":"<p>Exercise</p> <p>Navegue at\u00e9 o diret\u00f3rio da aula anterior onde voc\u00ea implementou o pipeline ETL.</p> <p>Crie uma c\u00f3pia da pasta <code>02-etl</code>.</p> <pre><code>$ cp -r /caminho/para/dataeng-03-repo-base/02-etl /caminho/para/dataeng-03-repo-base/02-etl-clickhouse\n</code></pre> <p>Aten\u00e7\u00e3o!</p> <p>Certifique-se de que todos os servi\u00e7os estejam parados antes de iniciar a migra\u00e7\u00e3o.</p> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#modificando-a-arquitetura","title":"Modificando a arquitetura","text":"<p>Exercise</p> <p>Edite o arquivo <code>02-etl-clickhouse/docker-compose.yml</code> para substituir o PostgreSQL pelo ClickHouse como Data Warehouse.</p> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#criando-o-schema-clickhouse","title":"Criando o schema ClickHouse","text":"<p>Exercise</p> <p>Crie um novo arquivo <code>02-etl/sql/0001-ddl-clickhouse.sql</code> com o schema otimizado para ClickHouse.</p> Mark as done <p>Exercise</p> <p>Quais otimiza\u00e7\u00f5es foram aplicadas por voc\u00ea no schema do ClickHouse em compara\u00e7\u00e3o com o PostgreSQL?</p> Submit <p>Answer</p> <p>Otimiza\u00e7\u00f5es que poderiam ser aplicadas (pesquise como fazer):</p> <ol> <li>LowCardinality: Para campos com poucos valores \u00fanicos (nome, categoria, uf, status)</li> <li>Particionamento: Tabelas particionadas por m\u00eas usando <code>toYYYYMM()</code></li> <li>Tipos espec\u00edficos: UInt32/UInt64 para IDs, Decimal para valores monet\u00e1rios</li> <li>Ordena\u00e7\u00e3o otimizada: ORDER BY considera padr\u00f5es de consulta (data, categoria, etc.)</li> <li>Remo\u00e7\u00e3o de constraints: Sem chaves estrangeiras ou checks para melhor performance de inser\u00e7\u00e3o</li> </ol>"},{"location":"classes/03-data-warehouse/clickhouse/#adaptando-o-script-de-inicializacao","title":"Adaptando o script de inicializa\u00e7\u00e3o","text":"<p>Exercise</p> <p>Crie um novo arquivo <code>02-etl/src/init_clickhouse.py</code> para inicializar o ClickHouse.</p> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#modificando-o-pipeline-etl","title":"Modificando o pipeline ETL","text":"<p>Exercise</p> <p>Crie um novo arquivo <code>02-etl/src/etl_vendas_clickhouse.py</code> e fa\u00e7a o processo de ETL.</p> Mark as done <p>Exercise</p> <p>Atualize o arquivo <code>02-etl/requirements.txt</code> para incluir o cliente ClickHouse.</p> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#testando-a-nova-arquitetura","title":"Testando a nova arquitetura","text":"<p>Exercise</p> <p>Pare os servi\u00e7os anteriores (se estiverem rodando):</p> <p>Aten\u00e7\u00e3o!</p> <p>O <code>-v</code> no comando abaixo ir\u00e1 garantir que os volumes criados sejam apagados.</p> <p>Utilize isto quando quiser apagar dados persistidos em volumes do docker.</p> <pre><code>$ docker compose down -v\n</code></pre> Mark as done <p>Exercise</p> <p>Construa e inicie os novos servi\u00e7os:</p> <pre><code>$ docker compose build\n$ docker compose up\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique os logs para confirmar que:</p> <ol> <li>O simulador de vendas est\u00e1 gerando dados no PostgreSQL</li> <li>O ETL est\u00e1 extraindo dados do PostgreSQL e carregando no ClickHouse</li> <li>N\u00e3o h\u00e1 erros de conex\u00e3o ou inser\u00e7\u00e3o</li> </ol> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#validacao-e-analises","title":"Valida\u00e7\u00e3o e an\u00e1lises","text":"<p>Exercise</p> <p>Conecte-se ao ClickHouse via cliente CLI para validar os dados:</p> <pre><code>$ docker exec -it clickhouse-dw clickhouse-client --user dw_user --password dw_pass --database vendas_dw\n</code></pre> Mark as done <p>Exercise</p> <p>Execute consultas de valida\u00e7\u00e3o no ClickHouse:</p> <pre><code>-- Verificar contagem de registros por tabela\nSELECT 'cidade' as tabela, count() as total FROM cidade\nUNION ALL\nSELECT 'cliente', count() FROM cliente\nUNION ALL  \nSELECT 'produto', count() FROM produto\nUNION ALL\nSELECT 'venda', count() FROM venda\nUNION ALL\nSELECT 'item_venda', count() FROM item_venda;\n\n-- An\u00e1lise temporal das vendas\nSELECT \n    toDate(data) as data,\n    count(*) as total_vendas,\n    sum(valor_total) as receita_total\nFROM venda\nGROUP BY data \nORDER BY data DESC\nLIMIT 10;\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#exercicios-de-comparacao","title":"Exerc\u00edcios de compara\u00e7\u00e3o","text":"<p>Exercise</p> <p>Compare o desempenho do ClickHouse com o PostgreSQL para consultas anal\u00edticas. Quais diferen\u00e7as voc\u00ea observa?</p> Submit <p>Exercise</p> <p>Quais s\u00e3o as principais vantagens de usar ClickHouse como Data Warehouse em compara\u00e7\u00e3o com PostgreSQL?</p> Submit <p>Exercise</p> <p>Em que cen\u00e1rios voc\u00ea ainda preferiria usar PostgreSQL em vez de ClickHouse?</p> Submit <p>Exercise</p> <p>Modifique o dashboard (Jupyter Notebook) para conectar-se ao ClickHouse em vez do PostgreSQL e compare os tempos de resposta das consultas.</p> Mark as done <p>Exercise</p> <p>Implemente um mecanismo de monitoramento que compare m\u00e9tricas entre os dois sistemas: 1. Tempo de resposta das consultas 2. Uso de CPU e mem\u00f3ria 3. Espa\u00e7o em disco utilizado 4. Throughput de inser\u00e7\u00e3o de dados</p> Mark as done"},{"location":"classes/03-data-warehouse/clickhouse/#limpeza","title":"Limpeza","text":"<p>Exercise</p> <p>Para parar e remover os containers e volumes:</p> <p>Warning</p> <p>O <code>-v</code> no comando abaixo ir\u00e1 garantir que os volumes criados sejam apagados.</p> <p>Utilize isto quando quiser apagar dados persistidos em volumes do docker.</p> <p>Limpeza!</p> <p>Lembre-se que voc\u00ea criou/alterou dois conjuntos de servi\u00e7os nesta aula!</p> <pre><code>$ docker compose down -v\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/data-warehousing/","title":"Data Warehousing","text":""},{"location":"classes/03-data-warehouse/data-warehousing/#data-warehousing","title":"Data Warehousing","text":"<p>O Data Warehousing \u00e9 uma abordagem para a coleta, armazenamento e an\u00e1lise de grandes volumes de dados provenientes de diversas fontes. O objetivo principal de um Data Warehouse \u00e9 fornecer uma vis\u00e3o unificada e consistente dos dados, facilitando a tomada de decis\u00f5es estrat\u00e9gicas nas organiza\u00e7\u00f5es.</p>"},{"location":"classes/03-data-warehouse/data-warehousing/#caracteristicas-de-um-data-warehouse","title":"Caracter\u00edsticas de um Data Warehouse","text":"<p>Um Data Warehouse possui caracter\u00edsticas espec\u00edficas que o diferenciam de sistemas transacionais:</p> <ul> <li> <p>Subject-Oriented (Orientado por Assunto): Os dados s\u00e3o organizados por temas ou assuntos de neg\u00f3cio (vendas, marketing, finan\u00e7as) ao inv\u00e9s de por aplica\u00e7\u00f5es.</p> </li> <li> <p>Integrated (Integrado): Dados de diferentes fontes s\u00e3o padronizados e consolidados em um formato consistente.</p> </li> <li> <p>Time-Variant (Vari\u00e1vel no Tempo): Mant\u00e9m hist\u00f3rico de dados, permitindo an\u00e1lises temporais e identifica\u00e7\u00e3o de tend\u00eancias.</p> </li> <li> <p>Non-Volatile (N\u00e3o Vol\u00e1til): Uma vez carregados, os dados n\u00e3o s\u00e3o alterados, garantindo consist\u00eancia nas an\u00e1lises.</p> </li> </ul>"},{"location":"classes/03-data-warehouse/data-warehousing/#arquitetura-tipica-de-data-warehouse","title":"Arquitetura T\u00edpica de Data Warehouse","text":"<p>A arquitetura de um Data Warehouse segue uma estrutura bem definida que permite a transforma\u00e7\u00e3o eficiente de dados brutos em informa\u00e7\u00f5es anal\u00edticas.</p> <p>Esta arquitetura reflete diretamente o ciclo de vida da engenharia de dados que vimos anteriormente: ingest\u00e3o, transforma\u00e7\u00e3o e disponibiliza\u00e7\u00e3o dos dados.</p> <pre><code>flowchart LR\n    DS1[Data source]\n    DS2[Data source]\n    DM1[Data mart]\n    DM2[Data mart]\n    BI[Reports/Dashboards]\n\n    subgraph ETL[ETL/ELT Process]\n        SA[Staging Area]\n        DW[Data warehouse]\n        SA --&gt; DW\n    end\n\n    DS1 --&gt; SA\n    DS2 --&gt; SA\n    DW --&gt; DM1\n    DW --&gt; DM2\n    DM1 --&gt; BI\n    DM2 --&gt; BI</code></pre>"},{"location":"classes/03-data-warehouse/data-warehousing/#componentes-da-arquitetura","title":"Componentes da Arquitetura","text":"<ol> <li>Fontes de Dados: Sistemas OLTP, APIs, arquivos CSV, logs de aplica\u00e7\u00f5es, sensores IoT</li> <li>ETL/ELT: Processos de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga dos dados</li> <li>Staging Area: \u00c1rea tempor\u00e1ria onde dados s\u00e3o processados e validados</li> <li>Data Warehouse: Reposit\u00f3rio central consolidado e otimizado para an\u00e1lises</li> <li>Data Marts: Subconjuntos especializados por departamento ou \u00e1rea de neg\u00f3cio</li> <li>Camada de Apresenta\u00e7\u00e3o: Dashboards, relat\u00f3rios e ferramentas de Business Intelligence</li> </ol>"},{"location":"classes/03-data-warehouse/data-warehousing/#etl-vs-elt","title":"ETL vs ELT","text":"<p>No contexto do ciclo de vida da engenharia de dados, tanto ETL quanto ELT s\u00e3o abordagens para implementar as etapas de ingest\u00e3o e transforma\u00e7\u00e3o. A escolha entre elas depende dos recursos dispon\u00edveis e das necessidades espec\u00edficas do projeto.</p>"},{"location":"classes/03-data-warehouse/data-warehousing/#etl-extract-transform-load","title":"ETL (Extract, Transform, Load)","text":"<p>O ETL \u00e9 a abordagem tradicional onde os dados s\u00e3o transformados antes de serem carregados no destino.</p> <ul> <li>Processo: Dados s\u00e3o extra\u00eddos da fonte, transformados em um sistema intermedi\u00e1rio, e depois carregados</li> <li>Vantagem: Dados chegam ao destino j\u00e1 limpos e estruturados</li> <li>Limita\u00e7\u00e3o: Requer mais recursos computacionais no sistema de processamento intermedi\u00e1rio</li> <li>Quando usar: Ideal para dados que precisam de limpeza pesada ou quando o sistema de destino tem recursos limitados</li> </ul>"},{"location":"classes/03-data-warehouse/data-warehousing/#elt-extract-load-transform","title":"ELT (Extract, Load, Transform)","text":"<p>O ELT \u00e9 uma abordagem mais moderna onde os dados s\u00e3o carregados antes de serem transformados.</p> <ul> <li>Processo: Dados s\u00e3o extra\u00eddos da fonte, carregados diretamente no destino, e transformados l\u00e1</li> <li>Vantagem: Aproveita o poder computacional do sistema de destino (ex: cloud data warehouses)</li> <li>Limita\u00e7\u00e3o: Requer um sistema de destino com capacidade de processamento robusta</li> <li>Quando usar: Ideal com sistemas cloud modernos que t\u00eam grande capacidade de processamento</li> </ul> <p>Benef\u00edcios do Data Warehouse</p> <ul> <li>Centraliza\u00e7\u00e3o: Uma \u00fanica fonte da verdade para toda a organiza\u00e7\u00e3o</li> <li>Performance: Otimizado para consultas anal\u00edticas complexas</li> <li>Hist\u00f3rico: Capacidade de an\u00e1lise temporal e identifica\u00e7\u00e3o de tend\u00eancias</li> <li>Qualidade: Dados limpos, padronizados e validados</li> <li>Seguran\u00e7a: Controle centralizado de acesso e auditoria</li> </ul> <p>Resumo</p> <p>O Data Warehouse \u00e9 um reposit\u00f3rio centralizado que coleta e integra dados de diversas fontes de dados menores, que podem ser bancos de dados operacionais (como sistemas de vendas, RH ou finan\u00e7as), arquivos de texto, planilhas, etc.</p> <p>Enquanto os bancos de dados operacionais s\u00e3o otimizados para opera\u00e7\u00f5es di\u00e1rias e em tempo real (como registrar uma venda, por exemplo), o Data Warehouse \u00e9 projetado para an\u00e1lise e tomada de decis\u00e3o.</p>"},{"location":"classes/03-data-warehouse/data-warehousing/#voltando-ao-problema-inicial","title":"Voltando ao Problema Inicial","text":"<p>Agora conseguimos entender por que a proposta inicial do analista era problem\u00e1tica. A solu\u00e7\u00e3o adequada seria:</p> <ol> <li>Manter o PostgreSQL para opera\u00e7\u00f5es OLTP (vendas)</li> <li>Criar um pipeline ETL para extrair dados do PostgreSQL</li> <li>Implementar um Data Warehouse otimizado para an\u00e1lises (OLAP)</li> <li>Conectar o Dashboard ao Data Warehouse, n\u00e3o ao sistema transacional</li> </ol> <p>Arquitetura de Data Warehouse</p> <p>Uma empresa possui dados de vendas no PostgreSQL, dados de marketing no MySQL e dados de atendimento em arquivos CSV. Para criar um Dashboard unificado, qual seria a abordagem mais adequada?</p> Conectar o Dashboard diretamente em cada fonte de dados Migrar todos os dados para um \u00fanico PostgreSQL (todos passam a utilizar apenas esse banco) Criar um Data Warehouse que integre dados de todas as fontes via ETL Usar apenas os dados do PostgreSQL por ser o mais completo Submit <p>Answer</p> <p>A abordagem mais adequada \u00e9 criar um Data Warehouse que integre dados de todas as fontes via ETL. Isso permite ter uma vis\u00e3o unificada e consistente, otimizada para an\u00e1lises, sem impactar os sistemas operacionais.</p> <p>As outras op\u00e7\u00f5es apresentam limita\u00e7\u00f5es de performance, integra\u00e7\u00e3o ou completude dos dados.</p> <p>Exercise</p> <p>Descreva as principais diferen\u00e7as entre usar um sistema OLTP diretamente para an\u00e1lises versus implementar um Data Warehouse dedicado.</p> Submit <p>Answer</p> <p>Sistema OLTP diretamente:</p> <ul> <li>Performance: Consultas anal\u00edticas podem impactar opera\u00e7\u00f5es transacionais</li> <li>Estrutura: Dados normalizados dificultam consultas complexas com muitos JOINs</li> <li>Integra\u00e7\u00e3o: Dificuldade para combinar dados de m\u00faltiplas fontes</li> <li>Hist\u00f3rico: Limitado, pois dados antigos s\u00e3o frequentemente arquivados ou deletados</li> <li>Seguran\u00e7a: Risco de expor dados operacionais sens\u00edveis</li> </ul> <p>Data Warehouse dedicado:</p> <ul> <li>Performance: Otimizado para consultas anal\u00edticas sem impactar opera\u00e7\u00f5es</li> <li>Estrutura: Dados desnormalizados facilitam an\u00e1lises complexas</li> <li>Integra\u00e7\u00e3o: Consolida dados de m\u00faltiplas fontes em formato consistente</li> <li>Hist\u00f3rico: Mant\u00e9m dados hist\u00f3ricos por longos per\u00edodos para an\u00e1lise temporal</li> <li>Seguran\u00e7a: Ambiente controlado e auditado, separado dos sistemas operacionais</li> <li>Qualidade: Dados limpos, validados e padronizados atrav\u00e9s do processo ETL</li> </ul> <p>Controle da fonte de dados</p> <p>O Engenheiro de dados geralmente n\u00e3o tem controle sobre a fonte de dados.</p> <p>Suponha que um sistema da empresa utiliza MySQL ou Oracle como SGBD-R (Sistema de Gerenciamento de Banco de Dados Relacional). Se este sistema \u00e9 fonte importante de dados para um Dashboard que ser\u00e1 disponibilizado para o time de vendas, \u00e9 sua tarefa, como engenheiro, trabalhar com estas tecnologias e criar rotinas adequadas para ingest\u00e3o, transforma\u00e7\u00e3o e disponibiliza\u00e7\u00e3o dos dados.</p>"},{"location":"classes/03-data-warehouse/data-warehousing/#ponto-importante","title":"Ponto importante","text":"<p>O papel do engenheiro de dados vai muito al\u00e9m das habilidades t\u00e9cnicas. Voc\u00ea precisar\u00e1 interagir com diferentes equipes, entender as necessidades de neg\u00f3cios e garantir que os dados estejam alinhados com os objetivos da organiza\u00e7\u00e3o.</p> <p>Dica</p> <p>A comunica\u00e7\u00e3o eficaz e a colabora\u00e7\u00e3o s\u00e3o essenciais para o sucesso na implementa\u00e7\u00e3o de solu\u00e7\u00f5es de dados.</p>"},{"location":"classes/03-data-warehouse/data-warehousing/#referencias","title":"Refer\u00eancias","text":"<ul> <li>FDE. Reis, J., Housley, M. (2022). Fundamentals of Data Engineering: Plan and Build Robust Data Systems. Estados Unidos: O'Reilly Media.</li> <li>DDIA, Cap 3. Kleppmann, M. (2017). Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. Estados Unidos: O'Reilly Media.</li> </ul>"},{"location":"classes/03-data-warehouse/duckdb/","title":"DuckDB","text":""},{"location":"classes/03-data-warehouse/duckdb/#duckdb","title":"DuckDB","text":"<p>O DuckDB \u00e9 um sistema de gerenciamento de banco de dados (SGBD) anal\u00edtico (OLAP) projetado para an\u00e1lise de dados. \u00c9 conhecido por sua velocidade, facilidade de uso e capacidade de processar diretamente arquivos em formato como Parquet, CSV e JSON.</p> <p>Info</p> <p>O DuckDB \u00e9 otimizado para consultas anal\u00edticas.</p> <p>Oferece execu\u00e7\u00e3o columnar e processamento vetorizado para alta performance.</p>"},{"location":"classes/03-data-warehouse/duckdb/#caracteristicas-principais","title":"Caracter\u00edsticas principais","text":"<ul> <li>In-process: Roda como uma biblioteca embarcada, sem necessidade de servidor separado</li> <li>ACID compliant: Garante consist\u00eancia transacional</li> <li>SQL padr\u00e3o: Suporte completo ao SQL com extens\u00f5es para an\u00e1lise</li> <li>Formatos diversos: L\u00ea diretamente Parquet, CSV, JSON e outros formatos</li> <li>Zero-copy: Acessa dados sem necessidade de importa\u00e7\u00e3o pr\u00e9via</li> <li>Parallel processing: Execu\u00e7\u00e3o paralela autom\u00e1tica de consultas</li> </ul>"},{"location":"classes/03-data-warehouse/duckdb/#preparando-o-ambiente","title":"Preparando o ambiente","text":"<p>Vamos configurar um ambiente Python para trabalhar com DuckDB e os dados do warm up da primeira aula.</p> <p>Exercise</p> <p>Crie um ambiente virtual Python para o projeto:</p> <p>Dica</p> <p>Utilize <code>venv</code>, <code>conda</code> ou <code>uv</code> conforme sua prefer\u00eancia!</p> <pre><code>$ uv venv --python 3.12 venv-duckdb\n</code></pre> Mark as done <p>Exercise</p> <p>Ative o ambiente virtual:</p> Linux/macOSWindows <pre><code>$ source venv-duckdb/bin/activate\n</code></pre> <pre><code>$ venv-duckdb\\Scripts\\activate\n</code></pre> Mark as done <p>Exercise</p> <p>Instale as depend\u00eancias necess\u00e1rias:</p> <code>requirements.txt</code>: <pre><code>duckdb==1.3.2\npytz==2025.2\npyarrow==21.0.0\npandas==2.3.1\n</code></pre> <pre><code>$ uv pip install -r requirements.txt\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#obtendo-os-dados","title":"Obtendo os dados","text":"<p>Vamos utilizar os dados do S3 da aula de warm up para praticar com o DuckDB.</p> <p>Exercise</p> <p>Caso ainda n\u00e3o tenha feito, configure as credenciais AWS conforme instru\u00e7\u00f5es da aula de warm up:</p> <pre><code>$ aws configure --profile dataeng-warmup\n</code></pre> Mark as done <p>Exercise</p> <p>Defina o perfil AWS:</p> Linux/macOSWindows CMDWindows PowerShell <pre><code>$ export AWS_PROFILE=dataeng-warmup\n</code></pre> <pre><code>$ set AWS_PROFILE=dataeng-warmup\n</code></pre> <pre><code>$ $env:AWS_PROFILE=\"dataeng-warmup\"\n</code></pre> Mark as done <p>Exercise</p> <p>Crie um diret\u00f3rio para os dados e baixe os arquivos do S3:</p> <p>Dica</p> <p>O comando abaixo ir\u00e1 ignorar o download do arquivo <code>status.csv</code> pois ele tem quase 2GB!</p> <pre><code>$ mkdir dados\n$ aws s3 cp s3://dataeng-warmup/data_raw/ dados/ --recursive --exclude=\"status.csv\" --profile dataeng-warmup\n</code></pre> <p>Arquivo grande!</p> <p>Caso tamb\u00e9m queira baixar o arquivo <code>status.csv</code>, voc\u00ea pode remover a op\u00e7\u00e3o <code>--exclude</code>:</p> <pre><code>$ mkdir dados\n$ aws s3 cp s3://dataeng-warmup/data_raw/ dados/ --recursive --profile dataeng-warmup\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique os arquivos baixados:</p> <pre><code>$ ls -la dados/\n</code></pre> <p>Voc\u00ea deve ver arquivos como <code>station.csv</code>, <code>status.csv</code>, <code>trip.csv</code> e <code>weather.csv</code>.</p> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#primeiros-passos-com-duckdb","title":"Primeiros passos com DuckDB","text":""},{"location":"classes/03-data-warehouse/duckdb/#interface-python","title":"Interface Python","text":"<p>Apesar do DuckDB ter API compat\u00edvel com diversas linguagens (C, Go, Rust, R, etc.), vamos trabalhar principalmente atrav\u00e9s da interface Python do DuckDB.</p> <p>Exercise</p> <p>Crie uma pasta <code>src</code> para manter seus scripts organizados:</p> <pre><code>$ mkdir src\n</code></pre> Mark as done <p>Aten\u00e7\u00e3o!</p> <p>Para os c\u00f3digos experimentais na sequ\u00eancia, voc\u00ea pode tanto criar um jupyter notebook ou utilizar arquivos <code>.py</code>, conforme sua prefer\u00eancia.</p> <p>Exercise</p> <p>Crie um arquivo <code>duckdb_inicio.py</code> com o seguinte conte\u00fado:</p> <pre><code>import duckdb\nimport pandas as pd\n\n# Conecta ao DuckDB (em mem\u00f3ria)\nconn = duckdb.connect()\n\n# Executa uma query simples\nresult = conn.execute(\"SELECT 'Hello DuckDB!' as mensagem\").fetchall()\nprint(result)\n\n# Executa query que retorna a data e hora atual\nresult = conn.execute(\"SELECT NOW() as agora\").fetchall()\nprint(result)\n\nconn.close()\n</code></pre> Mark as done <p>Dica</p> <p>A estrutura do diret\u00f3rio ser\u00e1:</p> <pre><code>./\n\u251c\u2500\u2500 dados\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 station.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 status.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 trip.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 weather.csv\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 codigos-duckdb.ipynb # Caso tenha criado\n    \u2514\u2500\u2500 duck_inicio.py\n</code></pre> <p>Exercise</p> <p>Execute o script:</p> <pre><code>$ python duckdb_inicio.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#lendo-dados-diretamente","title":"Lendo dados diretamente","text":"<p>Uma das grandes vantagens do DuckDB \u00e9 a capacidade de ler dados diretamente de arquivos, sem necessidade de importa\u00e7\u00e3o pr\u00e9via.</p>"},{"location":"classes/03-data-warehouse/duckdb/#lendo-arquivos-csv","title":"Lendo arquivos CSV","text":"<p>Exercise</p> <p>Crie um script <code>src/ler_csv.py</code> para explorar os dados das esta\u00e7\u00f5es:</p> <pre><code>import duckdb\n\nconn = duckdb.connect()\n\n# L\u00ea diretamente o arquivo CSV\nquery = \"\"\"\nSELECT * \nFROM '../dados/station.csv' \nLIMIT 5\n\"\"\"\n\nresult = conn.execute(query).fetchdf()\nprint(\"Primeiras 5 linhas das esta\u00e7\u00f5es:\")\nprint(result)\nprint(f\"\\nTotal de linhas: {len(result)}\")\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute e analise o resultado:</p> <pre><code>$ python ler_csv.py\n</code></pre> Mark as done <p>Exercise</p> <p>Utilizando o script anterior como refer\u00eancia, crie um arquivo <code>src/total_viagens.py</code> que retorna a quantidade total de viagens realizadas.</p> Mark as done <p>Answer</p> <pre><code>import duckdb\n\nconn = duckdb.connect()\n\nquery = \"\"\"\nSELECT COUNT(*) AS qtde_viagens\nFROM '../dados/trip.csv'\n\"\"\"\n\nresult = conn.execute(query).fetchdf()\nprint(\"Quantidade total de viagens:\")\nprint(result)\n\nconn.close()\n</code></pre>"},{"location":"classes/03-data-warehouse/duckdb/#consultas-analiticas","title":"Consultas anal\u00edticas","text":"<p>Exercise</p> <p>Crie um script <code>src/analise_viagens.py</code> para realizar consultas anal\u00edticas:</p> <pre><code>import duckdb\n\nconn = duckdb.connect()\n\n# Top 10 esta\u00e7\u00f5es de origem mais utilizadas\nquery_top_estacoes = \"\"\"\nSELECT \n    start_station_name,\n    COUNT(*) as total_viagens\nFROM '../dados/trip.csv'\nGROUP BY start_station_name\nORDER BY total_viagens DESC\nLIMIT 10\n\"\"\"\n\nprint(\"Top 10 esta\u00e7\u00f5es de origem:\")\nresult = conn.execute(query_top_estacoes).fetchdf()\nprint(result)\n\n# Dura\u00e7\u00e3o m\u00e9dia das viagens por dia da semana\nquery_duracao = \"\"\"\nSELECT \n    DAYNAME(strptime(start_date, '%m/%d/%Y %H:%M')) as dia_semana,\n    AVG(duration) as duracao_media_segundos,\n    AVG(duration)/60 as duracao_media_minutos\nFROM '../dados/trip.csv'\nGROUP BY DAYNAME(strptime(start_date, '%m/%d/%Y %H:%M'))\nORDER BY duracao_media_segundos DESC\n\"\"\"\n\nprint(\"\\n\\nDura\u00e7\u00e3o m\u00e9dia por dia da semana:\")\nresult = conn.execute(query_duracao).fetchdf()\nprint(result) # Exiba DF sem print se estiver utilizando jupyter notebooks\n\nconn.close()\n\nresult\n</code></pre> Mark as done <p>Exercise</p> <p>Execute a an\u00e1lise:</p> <pre><code>$ python analise_viagens.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#criando-tabelas-e-databases","title":"Criando tabelas e databases","text":""},{"location":"classes/03-data-warehouse/duckdb/#database-persistente","title":"Database persistente","text":"<p>Exercise</p> <p>Crie um script <code>src/criar_database.py</code> para trabalhar com um database persistente:</p> <pre><code>import duckdb\nimport os\n\n# Remove database existente se houver\nif os.path.exists('bike_share.db'):\n    os.remove('bike_share.db')\n\n# Conecta a um database persistente\nconn = duckdb.connect('bike_share.db')\n\nprint(\"Database criado: bike_share.db\")\nprint(\"Tabelas dispon\u00edveis:\", conn.execute(\"SHOW TABLES\").fetchall())\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute o script:</p> <pre><code>$ python criar_database.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#criando-tabelas-a-partir-de-arquivos","title":"Criando tabelas a partir de arquivos","text":"<p>Exercise</p> <p>Crie um script <code>src/importar_dados.py</code> para criar tabelas no database:</p> <pre><code>import duckdb \n\n# Conecta ao database persistente\nconn = duckdb.connect('bike_share.db')\n\n# Cria tabela de esta\u00e7\u00f5es\nconn.execute(\"\"\"\n    CREATE TABLE station AS \n    SELECT * FROM '../dados/station.csv'\n\"\"\")\n\n# Cria tabela de viagens (pode ser grande, ent\u00e3o vamos limitar)\nconn.execute(\"\"\"\n    CREATE TABLE trip AS \n    SELECT * FROM '../dados/trip.csv'\n    LIMIT 100000\n\"\"\")\n\n# Cria tabela de clima\nconn.execute(\"\"\"\n    CREATE TABLE weather AS \n    SELECT * FROM '../dados/weather.csv'\n\"\"\")\n\n# Verifica as tabelas criadas\nprint(\"Tabelas no database:\")\ntabelas = conn.execute(\"SHOW TABLES\").fetchall()\nfor tabela in tabelas:\n    print(f\"- {tabela[0]}\")\n\n    # Conta registros\n    count = conn.execute(f\"SELECT COUNT(*) FROM {tabela[0]}\").fetchone()[0]\n    print(f\"  Registros: {count:,}\")\n\n# Exemplo de consulta usando as tabelas\nprint(\"\\nTop 5 esta\u00e7\u00f5es por n\u00famero de viagens:\")\nresult = conn.execute(\"\"\"\n    SELECT \n        e.name as estacao,\n        COUNT(t.id) as total_viagens\n    FROM trip t\n    JOIN station e ON t.start_station_name = e.name\n    GROUP BY e.name\n    ORDER BY total_viagens DESC\n    LIMIT 5\n\"\"\").fetchdf()\nprint(result)\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute o script:</p> <pre><code>$ python importar_dados.py\n</code></pre> Mark as done <p>Info</p> <p>Vantagem de criar tabelas no DuckDB em vez de ler arquivos diretamente a cada consulta:</p> <p>Vantagens de criar tabelas:</p> <ul> <li>Performance: Dados ficam otimizados internamente</li> <li>\u00cdndices: Podem ser criados para acelerar consultas</li> <li>Joins: Mais eficientes entre tabelas do mesmo database</li> <li>Persist\u00eancia: Dados ficam dispon\u00edveis entre sess\u00f5es</li> </ul> <p>Vantagens de ler diretamente:</p> <ul> <li>Flexibilidade: Dados sempre atualizados no arquivo fonte</li> <li>Espa\u00e7o: N\u00e3o duplica armazenamento</li> <li>Simplicidade: \u00datil para prot\u00f3tipos e an\u00e1lises r\u00e1pidas</li> </ul>"},{"location":"classes/03-data-warehouse/duckdb/#trabalhando-com-parquet","title":"Trabalhando com Parquet","text":"<p>Conforme conversamos na primeira aula, o Parquet \u00e9 um formato columnar altamente eficiente, especialmente adequado para an\u00e1lises com DuckDB.</p>"},{"location":"classes/03-data-warehouse/duckdb/#exportando-para-parquet","title":"Exportando para Parquet","text":"<p>Exercise</p> <p>Crie um script <code>src/exportar_parquet.py</code>:</p> <pre><code>import duckdb\nimport os\n\nconn = duckdb.connect('bike_share.db')\n\n# Cria diret\u00f3rio para parquets\nos.makedirs('../dados/parquets', exist_ok=True)\n\n# Exporta tabelas para Parquet\ntabelas = ['station', 'trip', 'weather']\n\nfor tabela in tabelas:\n    print(f\"Exportando {tabela}...\")\n    conn.execute(f\"\"\"\n        COPY {tabela} TO '../dados/parquets/{tabela}.parquet' (FORMAT PARQUET)\n    \"\"\")\n    print(f\"- {tabela}.parquet criado\")\n\n# Verifica tamanhos dos arquivos\nprint(\"\\nTamanhos dos arquivos:\")\nfor tabela in tabelas:\n    size = os.path.getsize(f'../dados/parquets/{tabela}.parquet')\n    print(f\"- {tabela}.parquet: {size:,} bytes ({size/1024/1024:.2f} MB)\")\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute o export:</p> <pre><code>$ python exportar_parquet.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#lendo-arquivos-parquet","title":"Lendo arquivos Parquet","text":"<p>Exercise</p> <p>Crie um script <code>src/ler_parquet.py</code>:</p> <pre><code>import duckdb\n\nconn = duckdb.connect()\n\n# L\u00ea diretamente do Parquet\nprint(\"Informa\u00e7\u00f5es das esta\u00e7\u00f5es (de Parquet):\")\nresult = conn.execute(\"\"\"\n    SELECT \n        COUNT(*) as total_estacoes,\n        COUNT(DISTINCT city) as cidades\n    FROM '../dados/parquets/station.parquet'\n\"\"\").fetchone()\n\nprint(f\"Total de esta\u00e7\u00f5es: {result[0]}\")\nprint(f\"N\u00famero de cidades: {result[1]}\")\n\n# An\u00e1lise complexa combinando m\u00faltiplos Parquets\nprint(\"\\nAn\u00e1lise de viagens por cidade:\")\nresult = conn.execute(\"\"\"\n    SELECT \n        s.city,\n        COUNT(t.id) as viagens,\n        AVG(t.duration/60.0) as duracao_media_min\n    FROM '../dados/parquets/trip.parquet' t\n    JOIN '../dados/parquets/station.parquet' s\n        ON t.start_station_name = s.name\n    GROUP BY s.city\n    ORDER BY viagens DESC\n\"\"\").fetchdf()\n\nprint(result)\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute a an\u00e1lise:</p> <pre><code>$ python ler_parquet.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#performance-csv-vs-parquet","title":"Performance: CSV vs Parquet","text":"<p>Exercise</p> <p>Crie um script <code>src/benchmark.py</code> para comparar performance:</p> <pre><code>import duckdb\nimport time\n\nconn = duckdb.connect()\n\n# Consulta complexa para benchmark\nquery_csv = \"\"\"\n    SELECT \n        start_station_name,\n        COUNT(*) as viagens,\n        AVG(duration) as duracao_media\n    FROM '../dados/trip.csv'\n    GROUP BY start_station_name\n    HAVING COUNT(*) &gt; 100\n    ORDER BY viagens DESC\n\"\"\"\n\nquery_parquet = \"\"\"\n    SELECT \n        start_station_name,\n        COUNT(*) as viagens,\n        AVG(duration) as duracao_media\n    FROM '../dados/parquets/trip.parquet'\n    GROUP BY start_station_name\n    HAVING COUNT(*) &gt; 100\n    ORDER BY viagens DESC\n\"\"\"\n\n# Teste CSV\nprint(\"Executando consulta em CSV...\")\nstart = time.time()\nresult_csv = conn.execute(query_csv).fetchall()\ntime_csv = time.time() - start\nprint(f\"CSV: {time_csv:.2f} segundos\")\n\n# Teste Parquet\nprint(\"Executando consulta em Parquet...\")\nstart = time.time()\nresult_parquet = conn.execute(query_parquet).fetchall()\ntime_parquet = time.time() - start\nprint(f\"Parquet: {time_parquet:.2f} segundos\")\n\nspeedup = time_csv / time_parquet\nprint(f\"\\nParquet \u00e9 {speedup:.1f}x mais r\u00e1pido que CSV\")\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute o benchmark:</p> <pre><code>$ python benchmark.py\n</code></pre> <p>Info</p> <p>Repita a execu\u00e7\u00e3o do script m\u00faltiplas vezes para verificar se os resultados s\u00e3o consistentes!</p> Mark as done <p>Exercise</p> <p>Por que o Parquet \u00e9 geralmente mais r\u00e1pido que CSV para consultas anal\u00edticas?</p> Submit <p>Answer</p> <p>O Parquet \u00e9 mais eficiente porque:</p> <ul> <li>Formato columnar: L\u00ea apenas as colunas necess\u00e1rias para a consulta</li> <li>Compress\u00e3o: Dados similares ficam juntos, comprimindo melhor</li> <li>Metadados: Estat\u00edsticas permitem pular blocos irrelevantes</li> <li>Encoding: Otimiza\u00e7\u00f5es espec\u00edficas por tipo de dado</li> </ul>"},{"location":"classes/03-data-warehouse/duckdb/#analises-avancadas","title":"An\u00e1lises avan\u00e7adas","text":""},{"location":"classes/03-data-warehouse/duckdb/#window-functions","title":"Window Functions","text":"<p>Exercise</p> <p>Crie um script <code>src/analise_avancada.py</code>. Neste exemplo, utilizaremos window functions:</p> <p>Fun\u00e7\u00f5es de janela</p> <p>As fun\u00e7\u00f5es de janela permitem realizar c\u00e1lculos em um conjunto de linhas relacionadas, sem agrupar os resultados.</p> <p>Isso \u00e9 \u00fatil para an\u00e1lises que requerem informa\u00e7\u00f5es sobre a \"janela\" de dados ao redor da linha atual.</p> <pre><code>import duckdb\n\nconn = duckdb.connect()\n\n# Ranking de esta\u00e7\u00f5es por viagens\nprint(\"Top esta\u00e7\u00f5es com ranking:\")\nresult = conn.execute(\"\"\"\n    SELECT \n        start_station_name,\n        COUNT(*) as viagens,\n        RANK() OVER (ORDER BY COUNT(*) DESC) as ranking,\n        PERCENT_RANK() OVER (ORDER BY COUNT(*)) as percentil\n    FROM '../dados/parquets/trip.parquet'\n    GROUP BY start_station_name\n    ORDER BY viagens DESC\n    LIMIT 10\n\"\"\").fetchdf()\nprint(result)\n\n# An\u00e1lise temporal com moving average\nprint(\"\\n\\nViagens por hora com m\u00e9dia m\u00f3vel:\")\nresult = conn.execute(\"\"\"\n    SELECT \n        HOUR(strptime(start_date, '%m/%d/%Y %H:%M')) as hora,\n        COUNT(*) as viagens,\n        AVG(COUNT(*)) OVER (\n            ORDER BY HOUR(strptime(start_date, '%m/%d/%Y %H:%M'))\n            ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING\n        ) as media_movel_5h\n    FROM '../dados/parquets/trip.parquet'\n    WHERE start_date IS NOT NULL\n    GROUP BY HOUR(strptime(start_date, '%m/%d/%Y %H:%M'))\n    ORDER BY hora\n\"\"\").fetchdf()\nprint(result)\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute as an\u00e1lises avan\u00e7adas:</p> <pre><code>$ python analise_avancada.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#integracao-com-pandas","title":"Integra\u00e7\u00e3o com Pandas","text":"<p>O DuckDB integra perfeitamente com Pandas, permitindo usar DataFrames como tabelas.</p> <p>Exercise</p> <p>Crie um script <code>integracao_pandas.py</code>:</p> <pre><code>import duckdb\nimport pandas as pd\n\nconn = duckdb.connect()\n\n# Carrega dados em DataFrame\ndf_viagens = pd.read_parquet('../dados/parquets/trip.parquet')\nprint(f\"DataFrame carregado: {len(df_viagens):,} registros\")\n\n# Usa DataFrame diretamente em query SQL\nresult = conn.execute(\"\"\"\n    SELECT \n        subscription_type,\n        COUNT(*) as total,\n        AVG(duration) as duracao_media\n    FROM df_viagens\n    GROUP BY subscription_type\n\"\"\").fetchdf()\n\nprint(\"\\nAn\u00e1lise por tipo de assinatura:\")\nprint(result)\n\n# Cria novo DataFrame com resultado de SQL\ndf_resultado = conn.execute(\"\"\"\n    SELECT \n        start_station_name,\n        COUNT(*) as viagens,\n        AVG(duration) as duracao_media\n    FROM df_viagens\n    GROUP BY start_station_name\n    HAVING COUNT(*) &gt; 1000\n    ORDER BY viagens DESC\n\"\"\").df()\n\n# Usa m\u00e9todos do Pandas no resultado\nprint(f\"\\nEstat\u00edsticas das esta\u00e7\u00f5es principais:\")\nprint(f\"M\u00e9dia de viagens: {df_resultado['viagens'].mean():.0f}\")\nprint(f\"Mediana de viagens: {df_resultado['viagens'].median():.0f}\")\nprint(f\"Desvio padr\u00e3o: {df_resultado['viagens'].std():.0f}\")\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute a integra\u00e7\u00e3o:</p> <pre><code>$ python integracao_pandas.py\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#casos-de-uso-praticos","title":"Casos de uso pr\u00e1ticos","text":""},{"location":"classes/03-data-warehouse/duckdb/#etl-com-duckdb","title":"ETL com DuckDB","text":"<p>O DuckDB \u00e9 excelente para processos ETL (Extract, Transform, Load).</p> <p>Exercise</p> <p>Crie um script <code>etl_exemplo.py</code>:</p> <pre><code>import duckdb\nimport os\n\nconn = duckdb.connect()\n\n# EXTRACT: L\u00ea dados de m\u00faltiplas fontes\nprint(\"=== EXTRACT ===\")\nprint(\"Lendo dados de CSV e Parquet...\")\n\n# TRANSFORM: Limpa e processa dados\nprint(\"\\n=== TRANSFORM ===\")\nquery_transform = \"\"\"\n    CREATE TEMP TABLE viagens_limpas AS\n    SELECT \n        id,\n        strptime(start_date, '%m/%d/%Y %H:%M') as start_date,\n        strptime(end_date, '%m/%d/%Y %H:%M') as end_date,\n        duration,\n        start_station_name,\n        end_station_name,\n        bike_id,\n        subscription_type,\n        -- Calcula dura\u00e7\u00e3o em horas\n        duration / 3600.0 as duration_hours,\n        -- Categoriza dura\u00e7\u00e3o\n        CASE \n            WHEN duration &lt;= 600 THEN 'Curta'\n            WHEN duration &lt;= 1800 THEN 'M\u00e9dia'\n            ELSE 'Longa'\n        END as categoria_duracao,\n        -- Extrai informa\u00e7\u00f5es temporais\n        DAYNAME(strptime(start_date, '%m/%d/%Y %H:%M')) as dia_semana,\n        HOUR(strptime(start_date, '%m/%d/%Y %H:%M')) as hora_inicio\n    FROM '../dados/trip.csv'\n    WHERE \n        duration &gt; 60  -- Remove viagens muito curtas\n        AND duration &lt; 86400  -- Remove viagens muito longas (&gt;24h)\n        AND start_station_name IS NOT NULL\n        AND end_station_name IS NOT NULL\n\"\"\"\n\nconn.execute(query_transform)\nprint(\"\u2713 Dados transformados e limpos\")\n\n# Verifica qualidade dos dados\nstats = conn.execute(\"\"\"\n    SELECT \n        COUNT(*) as total_registros,\n        COUNT(DISTINCT start_station_name) as estacoes_origem,\n        MIN(duration_hours) as min_duracao_h,\n        MAX(duration_hours) as max_duracao_h,\n        AVG(duration_hours) as avg_duracao_h\n    FROM viagens_limpas\n\"\"\").fetchone()\n\nprint(f\"Total de registros limpos: {stats[0]:,}\")\nprint(f\"Esta\u00e7\u00f5es de origem: {stats[1]}\")\nprint(f\"Dura\u00e7\u00e3o: {stats[2]:.2f}h - {stats[3]:.2f}h (m\u00e9dia: {stats[4]:.2f}h)\")\n\n# LOAD: Salva resultado processado\nprint(\"\\n=== LOAD ===\")\nos.makedirs('../dados/output', exist_ok=True)\n\n# Salva dados limpos em Parquet\nconn.execute(\"\"\"\n    COPY viagens_limpas \n    TO '../dados/output/viagens_processadas.parquet' \n    (FORMAT PARQUET)\n\"\"\")\n\n# Cria resumo para an\u00e1lise\nconn.execute(\"\"\"\n    COPY (\n        SELECT \n            dia_semana,\n            hora_inicio,\n            categoria_duracao,\n            COUNT(*) as viagens,\n            AVG(duration_hours) as duracao_media_h\n        FROM viagens_limpas\n        GROUP BY ALL\n        ORDER BY dia_semana, hora_inicio\n    ) TO '../dados/output/resumo_viagens.csv' (HEADER)\n\"\"\")\n\nprint(\"\u2713 Dados salvos em:\")\nprint(\"  - ../dados/output/viagens_processadas.parquet\")\nprint(\"  - ../dados/output/resumo_viagens.csv\")\n\nconn.close()\n</code></pre> Mark as done <p>Exercise</p> <p>Execute o ETL:</p> <pre><code>$ python etl_exemplo.py\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique os arquivos gerados:</p> <pre><code>$ ls -la output/\n$ head output/resumo_viagens.csv\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/duckdb/#exercicios-finais","title":"Exerc\u00edcios finais","text":"<p>Exercise</p> <p>Compare o DuckDB com o PostgreSQL em rela\u00e7\u00e3o aos casos de uso. Quando voc\u00ea usaria cada um?</p> Submit <p>Answer</p> <p>DuckDB \u00e9 melhor para:</p> <ul> <li>An\u00e1lise explorat\u00f3ria de dados</li> <li>ETL/ELT em arquivos (Parquet, CSV)</li> <li>Prototipagem r\u00e1pida</li> <li>An\u00e1lises cient\u00edficas</li> <li>Integra\u00e7\u00e3o com notebooks/Python</li> </ul> <p>PostgreSQL \u00e9 melhor para:</p> <ul> <li>Aplica\u00e7\u00f5es transacionais (OLTP)</li> <li>Sistemas multi-usu\u00e1rio</li> <li>Requer ACID rigoroso</li> <li>Integra\u00e7\u00e3o com aplica\u00e7\u00f5es web</li> <li>Dados que mudam frequentemente</li> </ul> <p>Exercise</p> <p>Considere os dados que estamos utilizando nesta aual e crie uma an\u00e1lise que:</p> <ol> <li>Identifique padr\u00f5es de uso por hora do dia</li> <li>Calcule a taxa de ocupa\u00e7\u00e3o das esta\u00e7\u00f5es</li> <li>Encontre rotas mais populares</li> <li>Exporte os resultados em m\u00faltiplos formatos (CSV, Parquet, JSON)</li> </ol> Mark as done <p>Exercise</p> <p>Experimente conectar o DuckDB com dados remotos.</p> <p>Fa\u00e7a com que o DuckDB leia dados diretamente do bucket da aula 01 - warm up.</p> Mark as done <p>Documenta\u00e7\u00e3o</p> <ul> <li>Documenta\u00e7\u00e3o oficial do DuckDB!</li> </ul>"},{"location":"classes/03-data-warehouse/intro/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/03-data-warehouse/intro/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Nas aulas anteriores, exploramos o ciclo de vida de engenharia de dados (ingest\u00e3o, transforma\u00e7\u00e3o e disponibiliza\u00e7\u00e3o) e a cria\u00e7\u00e3o de softwares desacoplados que interagem entre si. Na aula de hoje, vamos explorar:</p> <ul> <li>Data warehouse</li> <li>Contexto hist\u00f3rico de cria\u00e7\u00e3o das solu\u00e7\u00e3o.</li> </ul>"},{"location":"classes/03-data-warehouse/intro/#o-problema","title":"O problema","text":"<p>Como engenheiro de dados, voc\u00ea ter\u00e1 que trabalhar lado a lado, interagir e levantar requisitos de profissionais como cientistas de dados e analistas de dados.</p> <p>Suponha que voc\u00ea foi designado para um projeto que envolve a cria\u00e7\u00e3o de um Dashboard para analisar os dados de vendas.</p> <p>Para levantar os requisitos, voc\u00ea marca uma reuni\u00e3o com os stakeholders do projeto. Ao ser perguntado sobre os objetivos do projeto, o analista de dados menciona:</p> <p>Quote</p> <p>A diretoria do departamento de vendas, que atua como cliente neste projeto, reclamou da falta de informa\u00e7\u00f5es consolidadas sobre as vendas.</p> <p>Os gestores desejam ter uma vis\u00e3o clara e integrada das vendas, permitindo identificar tend\u00eancias, oportunidades e \u00e1reas que precisam de aten\u00e7\u00e3o.</p> <p>O time de engenharia logo comenta:</p> <p>Quote</p> <p>J\u00e1 levantamos que os dados de vendas est\u00e3o armazenados em um RDBMS (ou SGBD-R, Sistema de Gerenciamento de Banco de Dados Relacional) PostgreSQL, que pode atuar como fonte dos dados.</p> <p>O analista de dados, por sua vez, complementa:</p> <p>Quote</p> <p>Nossa conversa inicial indicou que a principal necessidade, neste momento, seria uma vis\u00e3o integrada e hist\u00f3rica da evolu\u00e7\u00e3o das vendas.</p> <p>Como os gestores desejam visualizar as vendas agregadas por UF e realizar filtros por per\u00edodo, a proposta se encaminha para a cria\u00e7\u00e3o de um Dashboard.</p> <p>Sabendo que os dados est\u00e3o dispon\u00edveis no PostgreSQL, um analista de dados comenta.</p> <p>Quote</p> <p>Nossa solu\u00e7\u00e3o de Dashboard integra com o PostgreSQL.</p> <p>Posso criar uma query SQL e consumir direto deste banco de dados.</p> <p>Exercise</p> <p>O que voc\u00ea acha da proposta do analista?</p> Caso n\u00e3o tenha entendido muito a pergunta... <p>O que voc\u00ea acha da proposta do analista de dados de criar um Dashboard que se conecta e consome os dados diretamente do PostgreSQL?</p> Submit <p>Answer</p> <p>A proposta do analista de dados de criar um Dashboard que se conecta e consome os dados diretamente do PostgreSQL \u00e9 geralmente problem\u00e1tica.</p> <p>Apesar de ser uma abordagem utilizada especialmente por empresas menores, ela pode levar a desafios significativos em termos de escalabilidade, manuten\u00e7\u00e3o e governan\u00e7a dos dados.</p> <p>Poss\u00edveis problemas:</p> <ul> <li>O que vai acontecer quando um processamento pesado for realizado diretamente no PostgreSQL?<ul> <li>As vendas v\u00e3o parar para que o Dashboard possa ser atualizado?</li> </ul> </li> <li>O que vai acontecer quanto \u00e0 seguran\u00e7a e governan\u00e7a dos dados?<ul> <li>Quem ter\u00e1 acesso a esses dados?</li> <li>Como garantir que os dados sens\u00edveis estejam protegidos?</li> </ul> </li> <li>O que vai acontecer quanto \u00e0 evolu\u00e7\u00e3o do software versus a evolu\u00e7\u00e3o do Dashboard?<ul> <li>Como garantir que as mudan\u00e7as no sistema gerencial (novas features) n\u00e3o quebrem o Dashboard?</li> <li>Como garantir que o analista n\u00e3o realize modifica\u00e7\u00f5es que quebrem o sistema gerencial?</li> </ul> </li> </ul> <p>Podemos perceber que o servidor utilizado pelo sistema de vendas da empresa possui um prop\u00f3sito espec\u00edfico, que \u00e9 o processamento das vendas, garantindo que os dados estejam \u00edntegros ao longo de todo o ciclo de vida das transa\u00e7\u00f5es. N\u00e3o \u00e9 objetivo deste servidor propiciar an\u00e1lises complexas ou relat\u00f3rios detalhados (data analytics).</p> <p>Com isto, vemos que precisamos entender melhor os diferentes tipos de sistemas de banco de dados e suas finalidades espec\u00edficas.</p>"},{"location":"classes/03-data-warehouse/intro/#oltp","title":"OLTP","text":"<p>O OLTP (Online Transaction Processing) evoluiu dos primeiros sistemas de processamento de dados comerciais, onde cada escrita no banco correspondia a uma transa\u00e7\u00e3o comercial real: efetuar uma venda, fazer um pedido, pagar um sal\u00e1rio. Hoje, o termo \"transa\u00e7\u00e3o\" se expandiu para qualquer grupo de leituras e escritas que formam uma unidade l\u00f3gica.</p> <p>Sistemas OLTP s\u00e3o projetados para permitir leituras e escritas de baixa lat\u00eancia por milhares de usu\u00e1rios simult\u00e2neos, contrastando com processamento em lote que roda periodicamente.</p>"},{"location":"classes/03-data-warehouse/intro/#caracteristicas-principais-do-oltp","title":"Caracter\u00edsticas principais do OLTP:","text":"<ul> <li>Padr\u00e3o de acesso interativo: A aplica\u00e7\u00e3o busca poucos registros por chave usando \u00edndices</li> <li>Baixa lat\u00eancia e alta concorr\u00eancia: Sele\u00e7\u00e3o ou atualiza\u00e7\u00e3o de registros em menos de um milissegundo</li> <li>Transa\u00e7\u00f5es ACID: Garantem Atomicity, Consistency, Isolation e Durability</li> <li>Dados normalizados: Estrutura otimizada para minimizar redund\u00e2ncia e garantir integridade</li> <li>Opera\u00e7\u00f5es pontuais: Inser\u00e7\u00f5es e atualiza\u00e7\u00f5es baseadas em input do usu\u00e1rio</li> </ul> <p>OLTP e ACID</p> <p>Nem todo sistema OLTP precisa ter propriedades ACID completas.</p> <p>Alguns sistemas relaxam essas restri\u00e7\u00f5es para melhor performance e escala, usando modelos como eventual consistency.</p> <p>Exemplos de sistemas OLTP</p> <ul> <li>Sistema banc\u00e1rio: Transfer\u00eancias entre contas (transa\u00e7\u00e3o at\u00f4mica)</li> <li>E-commerce: Processamento de pedidos e atualiza\u00e7\u00f5es de estoque</li> <li>Reserva de voos: Milhares de usu\u00e1rios consultando e reservando assentos simultaneamente</li> </ul> <p>Caracter\u00edsticas do OLTP</p> <p>Qual das seguintes caracter\u00edsticas N\u00c3O \u00e9 t\u00edpica de um sistema OLTP?</p> Processamento r\u00e1pido de transa\u00e7\u00f5es pequenas Garantia de consist\u00eancia ACID Otimiza\u00e7\u00e3o para consultas anal\u00edticas complexas Suporte a m\u00faltiplos usu\u00e1rios concorrentes Submit <p>Answer</p> <p>Sistemas OLTP s\u00e3o otimizados para transa\u00e7\u00f5es r\u00e1pidas e opera\u00e7\u00f5es CRUD, n\u00e3o para consultas anal\u00edticas complexas.</p> <p>Consultas anal\u00edticas s\u00e3o caracter\u00edsticas de sistemas OLAP (Online Analytical Processing).</p> <p>Exemplos de solu\u00e7\u00f5es OLTP</p> <ul> <li>MySQL</li> <li>PostgreSQL</li> <li>Oracle Database</li> </ul>"},{"location":"classes/03-data-warehouse/intro/#olap","title":"OLAP","text":"<p>O OLAP (Online Analytical Processing) surgiu quando bancos de dados come\u00e7aram a ser usados para an\u00e1lise de dados (data analytics), com padr\u00f5es de acesso muito diferentes do processamento transacional.</p> <p>Enquanto OLTP busca poucos registros, uma consulta anal\u00edtica t\u00edpica precisa escanear milhares ou milh\u00f5es de registros, lendo apenas algumas colunas e calculando estat\u00edsticas agregadas (count, sum, average) ao inv\u00e9s de retornar dados brutos.</p>"},{"location":"classes/03-data-warehouse/intro/#caracteristicas-principais-do-olap","title":"Caracter\u00edsticas principais do OLAP:","text":"<ul> <li>Consultas anal\u00edticas de grande escala: Escaneiam grandes volumes, frequentemente 100MB+ por consulta</li> <li>Foco em agrega\u00e7\u00f5es: C\u00e1lculos como totais, m\u00e9dias, contagens ao inv\u00e9s de registros individuais</li> <li>Otimizado para scan: Bancos colunares reduzem a depend\u00eancia de \u00edndices para melhor performance de varredura</li> <li>Processamento em lote: Atualiza\u00e7\u00f5es peri\u00f3dicas, n\u00e3o em tempo real</li> <li>Ineficiente para lookups: N\u00e3o adequado para busca de registros individuais</li> </ul> <p>Exemplos t\u00edpicos de consultas OLAP</p> <ul> <li>\"Qual foi a receita total de cada loja em janeiro?\"</li> <li>\"Quantos produtos a mais vendemos durante a promo\u00e7\u00e3o?\"</li> <li>\"Qual marca \u00e9 mais comprada junto com fraldas da marca X?\"</li> </ul>"},{"location":"classes/03-data-warehouse/intro/#oltp-vs-olap-padroes-de-uso","title":"OLTP vs OLAP: Padr\u00f5es de Uso","text":"<p>Vamos resumir, de forma geral, os padr\u00f5es de uso de OLTP e OLAP:</p> Aspecto OLTP OLAP Padr\u00e3o de consulta Poucos registros por chave Scan de milh\u00f5es de registros Lat\u00eancia Milissegundos Segundos a minutos Volume por opera\u00e7\u00e3o Registros individuais Grandes agrega\u00e7\u00f5es Usu\u00e1rios t\u00edpicos Aplica\u00e7\u00f5es e usu\u00e1rios finais Analistas de neg\u00f3cio Finalidade Estado da aplica\u00e7\u00e3o Business Intelligence Exemplo Consultar saldo da conta Receita total por regi\u00e3o/m\u00eas <p>OLTP para Analytics: Problema Comum</p> <p>Empresas pequenas frequentemente rodam analytics diretamente no OLTP. Isso funciona no curto prazo, mas n\u00e3o \u00e9 escal\u00e1vel devido a:</p> <ul> <li>Limita\u00e7\u00f5es estruturais do OLTP para consultas anal\u00edticas</li> <li>Conten\u00e7\u00e3o de recursos entre cargas transacionais e anal\u00edticas</li> </ul> <p>Engenheiros devem configurar integra\u00e7\u00f5es adequadas sem degradar a performance dos sistemas produtivos.</p> <p>Exercise</p> <p>Considerando o cen\u00e1rio apresentado no in\u00edcio da aula, explique por que conectar o Dashboard diretamente ao PostgreSQL (sistema OLTP) pode causar problemas de performance.</p> Submit <p>Answer</p> <p>Conectar o Dashboard diretamente ao PostgreSQL (OLTP) pode causar s\u00e9rios problemas de performance porque:</p> <ol> <li> <p>Consultas anal\u00edticas pesadas: O Dashboard precisar\u00e1 executar queries com agrega\u00e7\u00f5es complexas (vendas por m\u00eas, por UF) que s\u00e3o computacionalmente caras</p> </li> <li> <p>Bloqueio de opera\u00e7\u00f5es: Enquanto o PostgreSQL processa essas consultas pesadas, as opera\u00e7\u00f5es de venda podem ficar lentas ou bloqueadas</p> </li> <li> <p>Estrutura n\u00e3o otimizada: Bancos OLTP s\u00e3o normalizados para efici\u00eancia transacional, n\u00e3o para consultas anal\u00edticas</p> </li> <li> <p>Concorr\u00eancia: O sistema precisa atender tanto \u00e0s vendas em tempo real quanto \u00e0s consultas do Dashboard, competindo pelos mesmos recursos</p> </li> </ol> <p>Por isso, a solu\u00e7\u00e3o adequada seria extrair os dados do OLTP, transform\u00e1-los e carreg\u00e1-los em um sistema OLAP otimizado para an\u00e1lises.</p> <p>Agora que voc\u00ea entendeu a import\u00e2ncia de separar as cargas de trabalho transacionais e anal\u00edticas, vamos explorar os diferentes ambientes utilizados por empresas. N\u00e3o \u00e9 algo que iremos aprofundar muito, mas \u00e9 importante ter uma vis\u00e3o geral.</p>"},{"location":"classes/03-data-warehouse/intro/#ambientes","title":"Ambientes","text":"<p>No desenvolvimento de solu\u00e7\u00f5es, \u00e9 fundamental trabalhar com diferentes ambientes para garantir a qualidade, seguran\u00e7a e confiabilidade dos sistemas. Cada ambiente serve a um prop\u00f3sito espec\u00edfico no ciclo de desenvolvimento e opera\u00e7\u00e3o.</p>"},{"location":"classes/03-data-warehouse/intro/#tipos-de-ambientes","title":"Tipos de Ambientes","text":""},{"location":"classes/03-data-warehouse/intro/#desenvolvimento-dev","title":"Desenvolvimento (DEV)","text":"<p>Ambiente onde os engenheiros desenvolvem e testam inicialmente suas solu\u00e7\u00f5es.</p> <ul> <li>Prop\u00f3sito: Desenvolvimento e testes iniciais</li> <li>Dados: Dados sint\u00e9ticos ou subconjuntos pequenos dos dados reais</li> <li>Acesso: Restrito \u00e0 equipe de desenvolvimento</li> <li>Estabilidade: Esperada instabilidade durante desenvolvimento</li> </ul>"},{"location":"classes/03-data-warehouse/intro/#homologacaostaging-stg","title":"Homologa\u00e7\u00e3o/Staging (STG)","text":"<p>Ambiente que replica o ambiente de produ\u00e7\u00e3o para testes finais.</p> <ul> <li>Prop\u00f3sito: Testes de integra\u00e7\u00e3o, valida\u00e7\u00e3o de performance e homologa\u00e7\u00e3o</li> <li>Dados: Dados similares aos de produ\u00e7\u00e3o, mas mascarados ou anonimizados</li> <li>Acesso: Equipe de desenvolvimento, QA e stakeholders para homologa\u00e7\u00e3o</li> <li>Estabilidade: Deve ser est\u00e1vel para testes confi\u00e1veis</li> </ul>"},{"location":"classes/03-data-warehouse/intro/#producao-prod","title":"Produ\u00e7\u00e3o (PROD)","text":"<p>Ambiente onde o sistema opera para usu\u00e1rios finais.</p> <ul> <li>Prop\u00f3sito: Opera\u00e7\u00e3o real do sistema</li> <li>Dados: Dados reais da empresa</li> <li>Acesso: Restrito e auditado</li> <li>Estabilidade: M\u00e1xima estabilidade e disponibilidade</li> </ul> <p>Seguran\u00e7a nos Ambientes</p> <p>\u00c9 fundamental que os dados sens\u00edveis de produ\u00e7\u00e3o nunca sejam copiados diretamente para ambientes de desenvolvimento. Utilize t\u00e9cnicas de:</p> <ul> <li>Mascaramento de dados: Substitui\u00e7\u00e3o de dados sens\u00edveis por valores falsos</li> <li>Anonimiza\u00e7\u00e3o: Remo\u00e7\u00e3o de identificadores pessoais</li> <li>Dados sint\u00e9ticos: Gera\u00e7\u00e3o de dados artificiais que mant\u00eam as caracter\u00edsticas estat\u00edsticas</li> </ul> <p>Ambientes de Desenvolvimento</p> <p>Um engenheiro de dados precisa testar um novo pipeline ETL que processar\u00e1 dados de clientes contendo CPF, nomes e informa\u00e7\u00f5es financeiras. Em qual ambiente ele deve realizar os primeiros testes?</p> Desenvolvimento (DEV) com dados mascarados Produ\u00e7\u00e3o (PROD) com dados reais Diretamente em Homologa\u00e7\u00e3o (STG) Produ\u00e7\u00e3o (PROD) em hor\u00e1rio de baixo uso Submit <p>Answer</p> <p>O engenheiro deve usar o ambiente de Desenvolvimento (DEV) com dados mascarados. Nunca se deve testar diretamente em produ\u00e7\u00e3o, e os primeiros testes devem ocorrer em DEV antes de ir para homologa\u00e7\u00e3o.</p> <p>Ainda, \u00e9 adequado que dados sens\u00edveis sejam mascarados em ambientes n\u00e3o-produtivos.</p> <p>Sabendo que necessitaremos de um servi\u00e7o separado para analytics, \u00e9 importante considerar a arquitetura do sistema e como os dados ser\u00e3o processados e armazenados. Na sequ\u00eancia, exploraremos as solu\u00e7\u00f5es de arquitetura propostas para atender a essas necessidades, de um ponto de vista hist\u00f3rico.</p>"},{"location":"classes/03-data-warehouse/linha-tempo/","title":"Linha do Tempo","text":"<p>A evolu\u00e7\u00e3o dos bancos de dados \u00e9 um reflexo direto da crescente demanda por gerenciar volumes de dados cada vez maiores e mais complexos.</p> <p>Na sequ\u00eancia, apresentamos uma linha do tempo com destaque para os principais marcos.</p> <ol> <li> <p>D\u00e9cadas de 1960 e 1970</p> <ul> <li>Modelos de Dados Hier\u00e1rquicos e de Rede: Antes dos bancos de dados relacionais, os modelos hier\u00e1rquicos e de rede dominavam. O Information Management System (IMS), da IBM, lan\u00e7ado em 1968, \u00e9 um exemplo proeminente de banco de dados hier\u00e1rquico, que organizava dados em uma estrutura de \u00e1rvore. O modelo de rede, com sua estrutura mais flex\u00edvel, permitia que um registro tivesse m\u00faltiplos \"pais\".</li> <li>A Proposta do Modelo Relacional: Em 1970, Edgar F. Codd, um cientista da computa\u00e7\u00e3o da IBM, publicou um artigo que propunha o modelo relacional. Esse modelo, baseado na teoria de conjuntos e na \u00e1lgebra relacional, organizava os dados em tabelas (rela\u00e7\u00f5es), usando chaves prim\u00e1rias e chaves estrangeiras para estabelecer conex\u00f5es entre elas. O conceito revolucionou a forma como os dados eram armazenados e acessados, tornando-os mais f\u00e1ceis de entender e manipular.</li> </ul> </li> <li> <p>D\u00e9cadas de 1980 e 1990</p> <ul> <li>O Surgimento dos Bancos de Dados Relacionais: A d\u00e9cada de 1980 viu a ascens\u00e3o dos Sistemas de Gerenciamento de Banco de Dados Relacionais (RDBMS). Empresas como a Oracle, IBM (com o DB2) e Microsoft (com o SQL Server) come\u00e7aram a dominar o mercado. A linguagem SQL (Structured Query Language), que havia sido desenvolvida na d\u00e9cada anterior, tornou-se o padr\u00e3o da ind\u00fastria para interagir com esses bancos de dados.</li> </ul> <p>Tip</p> <p>O modelo relacional RDBMS s\u00e3o assuntos principais da disciplina de Megadados do Insper!</p> <p>Info</p> <p>O projeto PostgreSQL teve in\u00edcio em 1986, sob a dire\u00e7\u00e3o do Professor Michael Stonebreaker, da Universidade da Calif\u00f3rnia, Berkeley.</p> <p>J\u00e1 o MySQL teve seu lan\u00e7amento em 1995.</p> <ul> <li>O Conceito de Data Warehousing: No final dos anos 1980, Bill Inmon prop\u00f4s o conceito de Data Warehouse, definindo-o como um reposit\u00f3rio centralizado de dados integrados, organizados por assunto, n\u00e3o-vol\u00e1teis e variantes no tempo, especificamente projetado para suporte \u00e0 tomada de decis\u00f5es. Este conceito revolucionou a forma como as empresas armazenavam e analisavam dados para business intelligence.</li> <li>Bancos de Dados Orientados a Objetos: Surge o conceito de bancos de dados orientados a objetos, que tentavam preencher a lacuna entre a programa\u00e7\u00e3o orientada a objetos e o armazenamento de dados. No entanto, sua ado\u00e7\u00e3o foi limitada e eles n\u00e3o conseguiram substituir a popularidade dos RDBMS.</li> </ul> </li> <li> <p>D\u00e9cada de 2000</p> <ul> <li>Bancos de Dados para a Web e a Explos\u00e3o de Dados: Com a populariza\u00e7\u00e3o da internet e o crescimento das plataformas online, a necessidade de lidar com grandes volumes de dados (Big Data) se tornou mais evidente. A estrutura r\u00edgida dos RDBMS nem sempre era a mais adequada para os dados n\u00e3o estruturados gerados pela web.</li> <li>O Surgimento do NoSQL: Em resposta a essas novas necessidades, o movimento NoSQL (Not Only SQL) ganhou for\u00e7a. Essa classe de bancos de dados se desviava do modelo relacional, oferecendo diferentes modelos de dados (como chave-valor, documentos, colunas e grafos) para maior flexibilidade e escalabilidade horizontal, especialmente para aplica\u00e7\u00f5es web.</li> </ul> </li> <li> <p>D\u00e9cada de 2010</p> <ul> <li>O Advento do Big Data e Data Lakes: A prolifera\u00e7\u00e3o de dados de diversas fontes (redes sociais, sensores, IoT) levou \u00e0 cria\u00e7\u00e3o do conceito de Data Lake. Diferente de um Data Warehouse, que armazena dados j\u00e1 estruturados e processados, um Data Lake \u00e9 um reposit\u00f3rio centralizado que armazena grandes volumes de dados brutos e de diversos formatos. O objetivo \u00e9 guardar os dados \"crus\" para futura an\u00e1lise, sem a necessidade de pr\u00e9-defini\u00e7\u00e3o de um esquema.</li> <li>A Evolu\u00e7\u00e3o dos Data Warehouses: Os Data Warehouses, que j\u00e1 existiam desde os anos 80, evolu\u00edram para lidar com Big Data. Solu\u00e7\u00f5es na nuvem, como o Amazon Redshift e o Google BigQuery, tornaram-se populares, oferecendo escalabilidade e performance para an\u00e1lise.</li> </ul> </li> <li> <p>D\u00e9cada de 2020</p> <ul> <li>Lakehouses: O Melhor de Dois Mundos: O conceito de Lakehouse emerge para unir as melhores caracter\u00edsticas dos Data Lakes e dos Data Warehouses. Um Lakehouse \u00e9 constru\u00eddo sobre um Data Lake e oferece a estrutura e as funcionalidades de um Data Warehouse, permitindo an\u00e1lises de BI e machine learning diretamente sobre os dados brutos. Isso elimina a necessidade de mover os dados entre sistemas, garantindo mais consist\u00eancia e efici\u00eancia.</li> </ul> </li> </ol> <p>Importante!</p> <p>Os bancos de dados relacionais continuam sendo a espinha dorsal de muitas aplica\u00e7\u00f5es, mas a diversidade de ferramentas e abordagens, como o NoSQL e o Lakehouse, demonstra a necessidade de solu\u00e7\u00f5es mais flex\u00edveis para os desafios de dados do mundo moderno.</p> <p>Nas pr\u00f3ximas aulas, conceitos como Data Lakes ser\u00e3o explorados com mais detalhes. Nosso foco hoje ser\u00e1 em Data Warehouses.</p>"},{"location":"classes/03-data-warehouse/links/","title":"Links interessantes","text":""},{"location":"classes/03-data-warehouse/links/#links","title":"Links","text":"<ul> <li>Documenta\u00e7\u00e3o oficial do DuckDB</li> <li>Ranking de DB-Engines</li> <li>DuckDB no GitHub</li> <li>DuckDB no DB-Engines</li> <li>Clickhouse no GitHub</li> <li>Clickhouse no DB-Engines</li> </ul>"},{"location":"classes/03-data-warehouse/links/#referencias","title":"Refer\u00eancias","text":"<ul> <li>FDE. Reis, J., Housley, M. (2022). Fundamentals of Data Engineering: Plan and Build Robust Data Systems. Estados Unidos: O'Reilly Media.</li> <li>DDIA, Cap 3. Kleppmann, M. (2017). Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. Estados Unidos: O'Reilly Media.</li> </ul>"},{"location":"classes/03-data-warehouse/olap/","title":"Sistemas OLAP","text":""},{"location":"classes/03-data-warehouse/olap/#sistemas-olap","title":"Sistemas OLAP","text":"<p>Na aula passada, aprendemos sobre por que separar os ambientes de produ\u00e7\u00e3o e analytics.</p> <p>Isto nos levou a pensar em duas categorias de sistemas: OLTP (Online Transaction Processing) e OLAP (Online Analytical Processing)</p> <p>Exercise</p> <p>Qual a principal aplica\u00e7\u00e3o de sistemas OLTP?</p> Submit <p>Answer</p> <p>A principal aplica\u00e7\u00e3o de sistemas OLTP \u00e9 em cen\u00e1rios onde as prioridades s\u00e3o o processamento de transa\u00e7\u00f5es e a manuten\u00e7\u00e3o da integridade dos dados (propriedades ACID), como em sistemas de vendas, reservas e gerenciamento de estoque.</p> <p>Exercise</p> <p>Qual a principal aplica\u00e7\u00e3o de sistemas OLAP?</p> Submit <p>Answer</p> <p>A principal aplica\u00e7\u00e3o de sistemas OLAP ocorre quando a prioridade \u00e9 a an\u00e1lise de grandes volumes de dados, como em sistemas de suporte \u00e0 decis\u00e3o, data warehouses e an\u00e1lise de tend\u00eancias.</p>"},{"location":"classes/03-data-warehouse/olap/#fluxo-dw","title":"Fluxo DW","text":"<p>Para ilustrar melhor o fluxo de dados entre sistemas OLTP e OLAP, considere o diagrama abaixo.</p> <pre><code>graph TD\n    subgraph Usu\u00e1rios\n        Cliente(Cliente)\n        AtendenteSuporte(Atendente de suporte)\n        ContadorFinanceiro(Contador)\n    end\n\n    subgraph \"Sistemas OLTP\"\n        SiteEcommerce(Site de e-commerce)\n        SistemaCRM(Sistema de CRM)\n        SistemaContabil(Sistema cont\u00e1bil)\n\n        BDVendas[(BD de vendas)]\n        BDAtendimento[(BD de atendimento)]\n        BDFinanceiro[(BD financeiro)]\n\n        SiteEcommerce --&gt; BDVendas\n        SistemaCRM --&gt; BDAtendimento\n        SistemaContabil --&gt; BDFinanceiro\n    end\n\n    subgraph \"Sistemas OLAP\"\n        ExtracaoVendas[Extra\u00e7\u00e3o]\n        ExtracaoAtendimento[Extra\u00e7\u00e3o]\n        ExtracaoFinanceiro[Extra\u00e7\u00e3o]\n\n        TransformacaoVendas[Transforma\u00e7\u00e3o]\n        TransformacaoAtendimento[Transforma\u00e7\u00e3o]\n        TransformacaoFinanceiro[Transforma\u00e7\u00e3o]\n\n        CargaVendas[Carga]\n        CargaAtendimento[Carga]\n        CargaFinanceiro[Carga]\n\n        ArmazemDados[(Armaz\u00e9m de dados - Data Warehouse)]\n\n        BDVendas --&gt; ExtracaoVendas\n        BDAtendimento --&gt; ExtracaoAtendimento\n        BDFinanceiro --&gt; ExtracaoFinanceiro\n\n        ExtracaoVendas --&gt; TransformacaoVendas\n        ExtracaoAtendimento --&gt; TransformacaoAtendimento\n        ExtracaoFinanceiro --&gt; TransformacaoFinanceiro\n\n        TransformacaoVendas --&gt; CargaVendas\n        TransformacaoAtendimento --&gt; CargaAtendimento\n        TransformacaoFinanceiro --&gt; CargaFinanceiro\n\n        CargaVendas --&gt; ArmazemDados\n        CargaAtendimento --&gt; ArmazemDados\n        CargaFinanceiro --&gt; ArmazemDados\n    end\n\n    Cliente --&gt; SiteEcommerce\n    AtendenteSuporte --&gt; SistemaCRM\n    ContadorFinanceiro --&gt; SistemaContabil</code></pre> <p>Uma empresa pode conter diversos sistemas OLTP. Eles suportam o dia a dia da opera\u00e7\u00e3o do neg\u00f3cio (sistema de vendas, CRM, gest\u00e3o de contas, etc.), sendo esperado deles resposta com baixa lat\u00eancia e foco na integridade dos dados.</p> <p>Para que os dados desses sistemas possam ser analisados de forma eficiente, processos de ELT ou ETL (Extract-Transform-Load) extraem dados de diversos sistemas transacionais, transformam para um formato adequado \u00e0 an\u00e1lise e os carregam em um Data Warehouse (DW).</p> <p>Quando os analistas de neg\u00f3cio ou cientistas de dados necessitarem de informa\u00e7\u00f5es para an\u00e1lise, eles consultar\u00e3o o Data Warehouse (DW).</p> <pre><code>graph LR\n    subgraph Analistas\n        AnalistaNegocios(Analista de neg\u00f3cios)\n        Consulta[Query de consulta]\n        AnalistaNegocios --&gt; Consulta\n    end\n\n    ArmazemDados[(Armaz\u00e9m de dados - Data Warehouse)]\n\n    Consulta --&gt; ArmazemDados</code></pre> <p>Info!</p> <p>O DW \u00e9 otimizado para consultas complexas e an\u00e1lises de grandes volumes de dados.</p> <p>Nesta aula, vamos explorar algumas solu\u00e7\u00f5es OLAP (Online Analytical Processing) e como eles se diferenciam de solu\u00e7\u00f5es focadas em OLTP.</p>"},{"location":"classes/03-data-warehouse/pratica/","title":"Praticando","text":""},{"location":"classes/03-data-warehouse/pratica/#praticando","title":"Praticando","text":"<p>Como pr\u00e1tica, iremos implementar um pipeline de ETL para um Data Warehouse.</p> <p>Nosso desafio ser\u00e1 construir um processo que extrai dados da aplica\u00e7\u00e3o de vendas e os carrega no Data Warehouse.</p> <p>Este ser\u00e1 o fluxo de dados:</p> <pre><code>flowchart LR\n    subgraph App_vendas[\"App vendas\"]\n        A[(PostgreSQL)]\n    end\n\n    A --&gt; B[ELT]\n\n    subgraph Warehouse[\"Warehouse\"]\n        C[(PostgreSQL)]\n    end\n\n    B --&gt; C\n\n    N[Analytics]\n    C --&gt; N</code></pre> <p>Exercise</p> <p>No diagrama, quem faz o papel de OLTP?</p> Submit <p>Answer</p> <p>O PostgreSQL da aplica\u00e7\u00e3o de vendas.</p> <p>Exercise</p> <p>No diagrama, quem faz o papel de OLAP?</p> Submit <p>Answer</p> <p>O PostgreSQL do Data Warehouse.</p> <p>Exercise</p> <p>O PostgreSQL \u00e9 uma escolha adequada como reposit\u00f3rio centralizado de dados para an\u00e1lise (data warehouse)?</p> Submit <p>Answer</p> <p>O PostgreSQL \u00e9 um OLTP, ou seja, um sistema de gerenciamento de banco de dados otimizado para transa\u00e7\u00f5es r\u00e1pidas e consultas em tempo real (mais adequado para sistemas comerciais onde ACID \u00e9 importante).</p> <p>Entretanto, o PostgreSQL tamb\u00e9m pode ser utilizado como um reposit\u00f3rio de dados para an\u00e1lise (data warehouse), embora n\u00e3o seja sua principal fun\u00e7\u00e3o.</p> <p>Info</p> <p>Por enquanto, manteremos o PostgreSQL para simplificar nossa arquitetura e posteriormente estudaremos solu\u00e7\u00f5es nativamente OLAP.</p>"},{"location":"classes/03-data-warehouse/pratica/#sistema-de-vendas","title":"Sistema de vendas","text":"<p>Iremos trabalhar em um cen\u00e1rio semelhante ao apresentado na introdu\u00e7\u00e3o. Suponha que um sistema de vendas esteja em opera\u00e7\u00e3o, tratando dados de vendas, clientes e produtos.</p> <p>O banco de dados da aplica\u00e7\u00e3o de vendas \u00e9 um PostgreSQL e o seguinte diagrama do modelo relacional representa a estrutura dos dados armazenados:</p> <p></p>"},{"location":"classes/03-data-warehouse/pratica/#simulador-de-vendas","title":"Simulador de vendas","text":"<p>Para termos um ambiente de testes, iremos criar um simulador de vendas que ir\u00e1 gerar dados fict\u00edcios para nossa aplica\u00e7\u00e3o. Esse simulador ir\u00e1 inserir dados aleat\u00f3rios nas tabelas do banco de dados da aplica\u00e7\u00e3o de vendas, permitindo que possamos testar nosso pipeline de ETL.</p> <p>Exercise</p> <p>Clone o reposit\u00f3rio base para a aula e abra no VSCode.</p> <pre><code>$ git clone git@github.com:macielcalebe/dataeng-03-repo-base.git\n</code></pre> Mark as done <p>Aten\u00e7\u00e3o</p> <p>Execute os pr\u00f3ximos exerc\u00edcios a partir da pasta <code>01-vendas</code>.</p> <p>Exercise</p> <p>Confira o conte\u00fado do arquivo <code>sql/0001-ddl.sql</code>. O que ele faz?</p> Submit <p>Answer</p> <p>O arquivo <code>sql/0001-ddl.sql</code> cont\u00e9m as instru\u00e7\u00f5es DDL (Data Definition Language) para criar as tabelas do banco de dados da aplica\u00e7\u00e3o de vendas.</p> <p>Exercise</p> <p>Analise o conte\u00fado do arquivo <code>sql/0002-dml-base.sql</code>. O que ele faz?</p> Submit <p>Answer</p> <p>O arquivo <code>sql/0002-dml-base.sql</code> cont\u00e9m as instru\u00e7\u00f5es DML (Data Manipulation Language) para inserir dados fict\u00edcios nas tabelas do banco de dados da aplica\u00e7\u00e3o de vendas.</p> <p>Assim, inicializaremos o simulador com dados de clientes, produtos e cidades.</p> <p>Exercise</p> <p>Estude o conte\u00fado dos arquivos <code>src/init_database.py</code> e <code>src/db_utils.py</code>. Qual a utilidade deles?</p> Submit <p>Answer</p> <p>O arquivo <code>src/init_database.py</code> cont\u00e9m o c\u00f3digo para inicializar o banco de dados da aplica\u00e7\u00e3o de vendas, executando as instru\u00e7\u00f5es DDL e DML contidas nos arquivos SQL.</p> <p>O arquivo <code>src/db_utils.py</code> cont\u00e9m fun\u00e7\u00f5es utilit\u00e1rias para interagir com o banco de dados, como executar consultas e manipular dados.</p> <p>Exercise</p> <p>Confira com aten\u00e7\u00e3o o arquivo <code>src/sales_simulator.py</code>. Entenda como ele funciona. Quais s\u00e3o suas caracter\u00edsticas principais?</p> Submit <p>Answer</p> <p>O arquivo <code>src/sales_simulator.py</code> cont\u00e9m o c\u00f3digo para simular vendas na aplica\u00e7\u00e3o. Ele gera dados fict\u00edcios e os insere nas tabelas do banco de dados da aplica\u00e7\u00e3o de vendas.</p> <p>As principais caracter\u00edsticas do simulador de vendas s\u00e3o: - Gera\u00e7\u00e3o de dados aleat\u00f3rios para simular vendas e itens de vendas. - Inser\u00e7\u00e3o dos dados gerados nas tabelas do banco de dados.</p> <p>Exercise</p> <p>Agora analise o arquivo <code>docker-compose.yml</code>. Quantos servi\u00e7os s\u00e3o iniciados e quais s\u00e3o eles?</p> Submit <p>Answer</p> <p>S\u00e3o iniciados dois servi\u00e7os:</p> <ul> <li><code>postgres-app</code>: O banco de dados PostgreSQL OLTP (aplica\u00e7\u00e3o de vendas).</li> <li> <p><code>python-app</code>: O simulador de vendas, que ir\u00e1 fazer o papel de aplica\u00e7\u00e3o (servi\u00e7o que produz dados). A sequ\u00eancia de comandos \u00e9:</p> <ol> <li>Instala depend\u00eancias (<code>pip install -r /app/requirements.txt</code>).</li> <li>Inicializa o banco (<code>python src/init_database.py</code>).</li> <li>Inicia o simulador de vendas (<code>python src/sales_simulator.py</code>).</li> </ol> </li> </ul> <p>Exercise</p> <p>Ainda no arquivo <code>docker-compose.yml</code>, o que quer dizer o trecho <code>${POSTGRES_PORT_APP:-5432}:5432</code>?</p> Submit <p>Answer</p> <p>O trecho permite que, ao definir a vari\u00e1vel de ambiente <code>POSTGRES_PORT_APP</code>, voc\u00ea escolha dinamicamente em qual porta o Postgres vai aparecer no host, mapeando ela para a porta <code>5432</code> no container.</p> <p>Se nada for configurado (a vari\u00e1vel <code>POSTGRES_PORT_APP</code> n\u00e3o est\u00e1 definida), ele abre <code>5432:5432</code>.</p> <p>Exercise</p> <p>Crie um arquivo <code>.env</code> a partir do <code>.env.example</code> e ajuste as vari\u00e1veis de ambiente conforme necess\u00e1rio.</p> Mark as done <p>Exercise</p> <p>Inicie os servi\u00e7os utilizando o Docker Compose.</p> <p>Para iniciar os servi\u00e7os, execute o seguinte comando no terminal:</p> <pre><code>$ docker-compose up\n</code></pre> <p>Isso ir\u00e1 iniciar os servi\u00e7os em primeiro plano.</p> Mark as done <p>Exercise</p> <p>Garanta que voc\u00ea consegue ver, no terminal, o log de inicializa\u00e7\u00e3o dos servi\u00e7os e as vendas sendo geradas pelo simulador.</p> Mark as done"},{"location":"classes/03-data-warehouse/pratica/#dbeaver","title":"DBeaver","text":"<p>Vamos instalar um cliente de banco de dados chamado DBeaver, que \u00e9 uma ferramenta gr\u00e1fica para gerenciar bancos de dados.</p> <p></p> <p>Info</p> <p>Caso voc\u00ea j\u00e1 tenha instalado um cliente que suporte PostgreSQL, voc\u00ea pode utiliz\u00e1-lo para se conectar ao banco de dados da aplica\u00e7\u00e3o de vendas e ignorar a instala\u00e7\u00e3o do DBeaver.</p> <p>Para instalar o DBeaver, siga as instru\u00e7\u00f5es para o seu sistema operacional:</p> WindowsmacOSLinux <ol> <li>Baixe o instalador do DBeaver aqui.</li> <li>Execute o instalador e siga as instru\u00e7\u00f5es na tela.</li> </ol> <ol> <li>Baixe o arquivo DMG do DBeaver aqui.</li> <li>Abra o arquivo DMG e arraste o DBeaver para a pasta Aplicativos.</li> </ol> <p>Para distribui\u00e7\u00f5es baseadas em Debian/Ubuntu, voc\u00ea pode usar o seguinte comando:</p> SnapFlatpak <p><pre><code>$ sudo snap install dbeaver-ce\n</code></pre> </p> <p><pre><code>$ flatpak install flathub io.dbeaver.DBeaverCommunity\n</code></pre> </p> <p>Para mais detalhes, consulte a p\u00e1gina oficial.</p> <p>Ap\u00f3s a instala\u00e7\u00e3o, abra o DBeaver e crie uma nova conex\u00e3o com o banco de dados PostgreSQL da aplica\u00e7\u00e3o de vendas utilizando as informa\u00e7\u00f5es configuradas no <code>.env</code>.</p> <p>Aten\u00e7\u00e3o</p> <p>Perceba que, no DBeaver, a conex\u00e3o ser\u00e1 pela porta exposta pelo <code>docker-compose.yml</code> (<code>5435</code>).</p> <p>J\u00e1 o container do simulador de vendas enxergar\u00e1 o PostgreSQL na porta <code>5432</code>.</p> <p>Para entender melhor, veja <code>.env.example</code> e como no servi\u00e7o <code>python-app</code> do <code>docker-compose.yml</code> as vari\u00e1veis <code>POSTGRES_HOST_APP</code> e <code>POSTGRES_PORT_APP</code> s\u00e3o redefinidas na se\u00e7\u00e3o <code>environment</code>:</p> <pre><code>    # RECORTE DE PARTE DO docker-compose.yml\n    postgres-app:\n        environment: # Redefine ou define vari\u00e1veis de ambiente\n        - POSTGRES_DB=${POSTGRES_DB_APP}\n        - POSTGRES_USER=${POSTGRES_USER_APP}\n        - POSTGRES_PASSWORD=${POSTGRES_PASSWORD_APP}\n        ports: # A porta exposta no host ser\u00e1 `POSTGRES_PORT_APP` (5435)\n        - \"${POSTGRES_PORT_APP:-5432}:5432\"\n\n    python-app:\n        environment: # Redefine ou define vari\u00e1veis de ambiente\n        - POSTGRES_HOST_APP=postgres-app # Container name do DB PostgreSQL\n        - POSTGRES_PORT_APP=5432 # O python-app acessa o DB pela porta interna do container\n</code></pre> <p>Exercise</p> <p>Garanta que voc\u00ea consegue observar, no DBeaver, as tabelas e os dados inseridos pelo simulador de vendas.</p> <p></p> <p>Dica</p> <p>D\u00ea dois cliques nas tabelas e acesse a aba data</p> Mark as done <p>Question</p> <p>Os dados do sistema do banco ser\u00e3o persistidos quando o container for parado ou removido?</p> Sim N\u00e3o Submit <p>Answer</p> <p>N\u00e3o. N\u00e3o foi criado nenhum volume para isto. Mas isto n\u00e3o \u00e9 problema nesta aula pois estamos apenas simulando um ambiente.</p>"},{"location":"classes/03-data-warehouse/pratica/#etl","title":"ETL","text":"<p>Agora que a aplica\u00e7\u00e3o de vendas est\u00e1 em funcionamento e os dados est\u00e3o sendo gerados, vamos implementar um pipeline de ETL para extrair esses dados e carreg\u00e1-los no Data Warehouse.</p> <p>Aten\u00e7\u00e3o</p> <p>Execute os pr\u00f3ximos exerc\u00edcios a partir da pasta <code>02-etl</code> do reposit\u00f3rio base.</p> <p>Exercise</p> <p>No <code>02-etl/docker-compose.yml</code>, defina um servi\u00e7o PostgreSQL que servir\u00e1 como Data Warehouse.</p> <p>Aten\u00e7\u00e3o</p> <p>Garanta que est\u00e1 trabalhando na pasta <code>02-etl</code> do reposit\u00f3rio base.</p> <p>Leia o <code>.env.example</code> para entender as vari\u00e1veis de ambiente que voc\u00ea pode usar.</p> Mark as done <p>Exercise</p> <p>Descubra o IPv4 local da sua m\u00e1quina e atualize o arquivo <code>02-etl/.env</code> com o valor encontrado.</p> Mark as done <p>Exercise</p> <p>Utilizando o arquivo <code>01-vendas/sql/0001-ddl.sql</code> como refer\u00eancia, crie uma vers\u00e3o de DDL no arquivo <code>02-etl/sql/0001-ddl-warehouse.sql</code> para definir o schema inicial do Data Warehouse.</p> <p>Ser\u00e1 apenas uma c\u00f3pia do arquivo?</p> Mark as done <p>Answer</p> <p>N\u00e3o. O schema do Data Warehouse deve ser otimizado para an\u00e1lises, ent\u00e3o voc\u00ea pode querer ajustar os tipos de dados, remover constraints de chave estrangeira, obrigatoriedade de preenchimento das colunas, remover colunas desnecess\u00e1rias, remover triggers de atualiza\u00e7\u00e3o autom\u00e1tica, retirar auto-incremento, ou criar \u00edndices para melhorar a performance das consultas.</p> <p>Exercise</p> <p>Crie um arquivo python <code>02-etl/src/init_database.py</code> que leia o arquivo <code>02-etl/sql/0001-ddl-warehouse.sql</code> e execute o comando para criar o banco de dados.</p> Mark as done <p>Exercise</p> <p>Execute o arquivo Python para definir o schema do Data Warehouse.</p> <p>Crie uma nova conex\u00e3o no DBeaver e garanta que voc\u00ea consegue visualizar o schema da base.</p> Mark as done <p>Exercise</p> <p>Utilizando o DBeaver, crie uma nova aba de query (new SQL script) para trabalhar em modo explorador!</p> <p>Agora, crie uma query que retorne todos os dados criados nos \u00faltimos dois minutos.</p> <p>Pode escolher qualquer tabela de sua prefer\u00eancia.</p> Mark as done <p>Answer</p> <p>Exemplo para a tabela de <code>venda</code>:</p> <pre><code>SELECT * FROM public.venda\nWHERE created_at &gt;= date_trunc('minute', now()) - INTERVAL '2 minutes'\nORDER BY created_at\n</code></pre> <p>Ficaria melhor ainda se n\u00e3o utiliz\u00e1ssemos <code>SELECT *</code> e sim selecion\u00e1ssemos apenas as colunas necess\u00e1rias!</p> <p>Exercise</p> <p>Crie um script Python <code>02-etl/src/etl_vendas.py</code> que ir\u00e1:</p> <ol> <li>Ler os dados criados recentemente na tabela de origem</li> <li>Inserir os dados no Data Warehouse</li> </ol> <p>Dica</p> <p>Fa\u00e7a primeiro com uma tabela. Depois, fa\u00e7a o mesmo para as outras tabelas. Voc\u00ea pode utilizar bibliotecas como <code>pandas</code> para facilitar a manipula\u00e7\u00e3o dos dados (ou qualquer outra de sua prefer\u00eancia).</p> Mark as done <p>Exercise</p> <p>Edite o arquivo <code>02-etl/docker-compose.yml</code> para incluir o servi\u00e7o de ETL, que deve executar, por exemplo, a cada dois minutos.</p> <p>Dica</p> <p>Por enquanto, basta executar um loop infinito com espera ao seu final. Melhoraremos nas pr\u00f3ximas aulas!</p> <p>Aten\u00e7\u00e3o</p> <p>N\u00e3o se esque\u00e7a de atualizar, na se\u00e7\u00e3o <code>environment</code> do servi\u00e7o de ETL, as vari\u00e1veis de ambiente para conectar ao banco de dados do Data Warehouse:</p> <pre><code># RECORTE DE PARTE DO ARQUIVO docker-compose.yml DO ETL\npython-etl:\n    environment: # Supondo que o servi\u00e7o DW se chama postgres-warehouse\n    - POSTGRES_HOST_WAREHOUSE=postgres-warehouse\n    - POSTGRES_PORT_WAREHOUSE=5432\n</code></pre> Mark as done <p>Exercise</p> <p>Garanta que o servi\u00e7o de ETL est\u00e1 funcionando corretamente.</p> <p>Confira se:</p> <ul> <li>Novos dados est\u00e3o sendo gerados pelo simulador na origem.</li> <li>O processo de ETL est\u00e1 salvando os dados no Data Warehouse.</li> </ul> Mark as done <p>Exercise</p> <p>Inicie todos servi\u00e7os e deixe-os rodando.</p> Mark as done"},{"location":"classes/03-data-warehouse/pratica/#dashboard","title":"Dashboard","text":"<p>Agora que voc\u00ea j\u00e1 possui os dados no Data Warehouse, \u00e9 hora de consumir!</p> <p>Um dashboard \u00e9 uma ferramenta de visualiza\u00e7\u00e3o de dados que permite acompanhar m\u00e9tricas e indicadores de desempenho. Para simular esta funcionalidade, iremos utilizar um Jupyter Notebook.</p> <p>Exercise</p> <p>Estude e execute o Jupyter Notebook dispon\u00edvel no diret\u00f3rio <code>03-dashboard</code>.</p> <p>Garanta que voc\u00ea consegue visualizar as vendas, produtos e clientes no dashboard.</p> Mark as done"},{"location":"classes/03-data-warehouse/pratica/#para-finalizar","title":"Para finalizar","text":"<p>Exercise</p> <p>O que aconteceria se os dados fossem atualizados?</p> <p>Por exemplo, se o nome de algum cliente for alterado, ou o status de entrega de alguma venda?</p> Submit <p>Answer</p> <p>O ETL deve ser projetado para lidar com atualiza\u00e7\u00f5es, entretanto, nosso ETL atual n\u00e3o considera essas mudan\u00e7as e sup\u00f5e que os dados s\u00e3o sempre novos (incrementais).</p> <p>Para lidar com isso, precisar\u00edamos implementar uma l\u00f3gica de atualiza\u00e7\u00e3o que identificasse registros existentes no Data Warehouse e os atualizasse conforme necess\u00e1rio.</p>"},{"location":"classes/03-data-warehouse/pratica/#limpeza","title":"Limpeza","text":"<p>Exercise</p> <p>Para parar e remover os containers e volumes:</p> <p>Warning</p> <p>O <code>-v</code> no comando abaixo ir\u00e1 garantir que os volumes criados sejam apagados.</p> <p>Utilize isto quando quiser apagar dados persistidos em volumes do docker.</p> <p>Nesta aula, os volumes mapeiam para pastas do pr\u00f3prio sistema (bind mounts), mas fica o aprendizado para as pr\u00f3ximas aulas.</p> <p>Limpeza!</p> <p>Lembre-se que voc\u00ea criou/alterou dois conjuntos de servi\u00e7os nesta aula!</p> <pre><code>$ docker compose down -v\n</code></pre> Mark as done"},{"location":"classes/03-data-warehouse/uv/","title":"Conhecendo o <code>uv</code>","text":"<p>o uv \u00e9 um gerenciador de pacotes, ambientes e projetos para Python. Ele foi escrito em Rust e serve como uma alternativa a como utilizamos <code>conda</code> e <code>venv</code> nos cursos do Insper.</p> <p>Para esta aula e as demais, indicamos utilizar o uv para gerenciar ambientes virtuais.</p> <p>Aten\u00e7\u00e3o!</p> <p>Utilizar o uv \u00e9 opcional.</p> <p>Caso prefira, pode ignorar os passos de instala\u00e7\u00e3o e seguir com seu gerenciador de pacotes preferido!</p>"},{"location":"classes/03-data-warehouse/uv/#instalacao","title":"Instala\u00e7\u00e3o","text":"Linux e MacOSWindows <p> <pre><code>$ curl -LsSf https://astral.sh/uv/install.sh | sh\n$ uv --version\n</code></pre> </p> <p>No Powershell, fa\u00e7a:</p> <p> <pre><code>$ powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> </p> <p>Caso tenha problemas, veja mais informa\u00e7\u00f5es no site oficial do UV.</p>"},{"location":"classes/03-data-warehouse/uv/#criar-ambiente-virtual","title":"Criar ambiente virtual","text":"uvvenvconda Linux e MacOSWindows <p> <pre><code>$ uv venv --python 3.12 .olap-env\n$ source .olap-env/bin/activate\n</code></pre> </p> <p> <pre><code>$ uv venv --python 3.12 .olap-env\n$ .olap-env\\Scripts\\Activate.ps1\n</code></pre> </p> <p>Tip</p> <p>Caso precise, voc\u00ea pode baixar a vers\u00e3o espec\u00edfica de python com:</p> <p> <pre><code>$ uv python install 3.12\n</code></pre> </p> <p>Para ver as vers\u00f5es do Python dispon\u00edveis e instaladas, utilize:</p> <p> <pre><code>$ uv python list\n</code></pre> </p> <p>No Powershell, fa\u00e7a:</p> <p> <pre><code>$ python -m venv .olap-env\n$ .olap-env\\Scripts\\activate\n</code></pre> </p> <p> <pre><code>$ conda create --name olap-env python=3.12\n$ conda activate olap-env\n</code></pre> </p>"},{"location":"classes/04-cloud/config-aws-cli/","title":"Configura\u00e7\u00e3o AWS CLI","text":""},{"location":"classes/04-cloud/config-aws-cli/#configuracao-de-ambiente","title":"Configura\u00e7\u00e3o de Ambiente","text":"<p>Para seguir com os exemplos pr\u00e1ticos desta aula, voc\u00ea precisar\u00e1 configurar seu ambiente de desenvolvimento. Aqui est\u00e3o as etapas recomendadas:</p> <ol> <li> <p>Ativar Conta na AWS: Se ainda n\u00e3o ativou sua conta, confira o aviso enviado pelos professores para maior detalhes.</p> </li> <li> <p>Instalar o AWS CLI: O AWS Command Line Interface (CLI) \u00e9 uma das ferramentas que utilizaremos para interagir com os servi\u00e7os da AWS. Siga as instru\u00e7\u00f5es da primeira aula para instalar e configurar o AWS CLI.</p> </li> <li> <p>Configurar Credenciais: Vamos configurar o acesso \u00e0 conta AWS pelo SSO (esquema diferente da primeira aula).</p> </li> </ol> <p>Aten\u00e7\u00e3o</p> <p>Execute o seguinte comando:</p> <p>Importante</p> <p>Defina a regi\u00e3o como <code>us-east-1</code>. Vamos centralizar recursos em uma regi\u00e3o! Assim mantemos uma melhor organiza\u00e7\u00e3o e controle sobre o que criamos durante o curso.</p> <p>Login pelo navegador!</p> <p>Durante esta etapa, voc\u00ea ser\u00e1 solicitado a fazer login em sua conta AWS por meio do navegador.</p> <p>Siga as instru\u00e7\u00f5es na tela para concluir o processo de autentica\u00e7\u00e3o.</p> <p>Caso solicitado, escolha a conta da disciplina!</p> <p>Configura\u00e7\u00f5es</p> <ul> <li><code>SSO session name (Recommended):</code> pode utilizar <code>dataeng</code></li> <li><code>SSO start URL:</code> utilize a URL de login que est\u00e1 no e-mail de convite</li> <li><code>SSO region [None]</code>: utilize <code>us-east-1</code></li> <li><code>SSO registration scopes [sso:account:access]:</code> apenas aperte ENTER</li> </ul> <pre><code>$ aws configure sso --profile dataeng\n$ aws sts get-caller-identity --profile dataeng\n$ aws sso login --profile dataeng\n</code></pre> <p>Cuidado para n\u00e3o fazer confus\u00e3o!</p> <p>Sempre confira se o perfil <code>dataeng</code> est\u00e1 ativo ao executar comandos da AWS CLI nesta disciplina.</p> <p>Voc\u00ea pode listar todos os perfis dispon\u00edveis com:</p> <pre><code>$ aws configure list-profiles\n</code></pre> <p>Agora que voc\u00ea configurou seu ambiente, est\u00e1 pronto para come\u00e7ar a trabalhar com a AWS CLI e explorar os servi\u00e7os da AWS.</p>"},{"location":"classes/04-cloud/intro/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/04-cloud/intro/#introducao","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/04-cloud/intro/#cloud-computing-para-engenharia-de-dados","title":"Cloud Computing para Engenharia de Dados","text":"<p>Nas aulas anteriores, exploramos o ciclo de vida da engenharia de dados (ingest\u00e3o, transforma\u00e7\u00e3o e disponibiliza\u00e7\u00e3o), a import\u00e2ncia de sistemas desacoplados usando filas de mensagens, e as diferen\u00e7as fundamentais entre OLTP e OLAP. </p> <p>Hoje vamos dar um passo importante: entender onde e como implementar nossa infraestrutura de dados.</p> <p>Info</p> <p>Esta aula \u00e9 introdut\u00f3ria e focada na forma\u00e7\u00e3o de conceitos fundamentais sobre computa\u00e7\u00e3o em nuvem para engenharia de dados.</p>"},{"location":"classes/04-cloud/intro/#o-contexto-onde-executar-nossa-infraestrutura","title":"O Contexto: Onde Executar Nossa Infraestrutura?","text":"<p>Imagine que voc\u00ea foi contratado como engenheiro de dados em uma empresa que est\u00e1 crescendo rapidamente.</p> <p>Os dados est\u00e3o aumentando exponencialmente, e a infraestrutura atual j\u00e1 mostra sinais de sobrecarga. A diretoria questiona:</p> <p>Diretoria</p> <p>Devemos investir em mais servidores pr\u00f3prios ou migrar para a nuvem?</p> <p>Esta \u00e9 uma decis\u00e3o que pode ter significado existencial para organiza\u00e7\u00f5es modernas:</p> <ul> <li> <p>Mover-se muito devagar pode significar ficar para tr\u00e1s da concorr\u00eancia mais \u00e1gil.</p> </li> <li> <p>Por outro lado, uma migra\u00e7\u00e3o mal planejada pode levar a falhas tecnol\u00f3gicas e custos catastr\u00f3ficos.</p> </li> </ul> <p>Exercise</p> <p>Antes de prosseguir, reflita: onde voc\u00ea acha que as empresas executam seus sistemas de dados atualmente? Quais fatores voc\u00ea considera importantes nessa decis\u00e3o?</p> Submit <p>Answer</p> <p>As empresas hoje t\u00eam m\u00faltiplas op\u00e7\u00f5es: on-premises (servidores pr\u00f3prios), cloud (nuvem), h\u00edbrido (combina\u00e7\u00e3o de ambos) ou multicloud (m\u00faltiplos provedores de nuvem).</p> <p>Dentre os fatores importantes, podemos citar custo, escalabilidade, seguran\u00e7a, controle, compliance e agilidade para inova\u00e7\u00e3o.</p> <p>Vamos explorar as principais op\u00e7\u00f5es de localiza\u00e7\u00e3o (location) para executar nossa infraestrutura de dados.</p>"},{"location":"classes/04-cloud/intro/#on-premises-o-modelo-tradicional","title":"On-Premises: O Modelo Tradicional","text":"<p>Embora novas startups nas\u00e7am cada vez mais na nuvem, os sistemas on-premises ainda s\u00e3o o padr\u00e3o para empresas estabelecidas.</p> <p>Defini\u00e7\u00e3o: On-Premises</p> <p>No modelo on-premises, as empresas possuem seu pr\u00f3prio hardware, que pode estar localizado em data centers pr\u00f3prios ou em espa\u00e7os de colocation alugados.</p> <p>As empresas s\u00e3o operacionalmente respons\u00e1veis por seu hardware e pelo software que roda nele.</p> Colocation <p>Colocation (infraestrutura) \u00e9 um modelo de hospedagem em que uma empresa aluga espa\u00e7o f\u00edsico em um data center de terceiros para instalar seus pr\u00f3prios servidores, equipamentos de rede e armazenamento.</p> <p>Principais recursos e servi\u00e7os compartilhados:</p> <ul> <li> <p>Espa\u00e7o f\u00edsico: pode ser apenas um rack unit (U), racks inteiros ou at\u00e9 \u00e1reas dedicadas no data center.</p> </li> <li> <p>Energia el\u00e9trica: fornecida com redund\u00e2ncia (geradores e no-breaks).</p> </li> <li> <p>Climatiza\u00e7\u00e3o: controle de temperatura e umidade para manter os equipamentos est\u00e1veis.</p> </li> <li> <p>Conectividade: acesso a diversos provedores de internet (carrier-neutral), possibilitando links redundantes e de alta velocidade.</p> </li> <li> <p>Seguran\u00e7a f\u00edsica: controle de acesso, monitoramento 24/7, c\u00e2meras, vigil\u00e2ncia.</p> </li> </ul> <p>Exemplos de provedores de colocation em S\u00e3o Paulo: ODATA e Ascenty</p> <p>Exercise</p> <p>Por que uma empresa iria optar por colocation em vez de data centers pr\u00f3prios?</p> Submit <p>Answer</p> <p>Construir e gerenciar a infraestrutura de TI internamente pode ser caro e demorado:</p> <ul> <li>Ser\u00e1 que a distribuidora de energia el\u00e9trica ir\u00e1 nos fornecer a energia necess\u00e1ria para suportar a demanda?</li> <li>Quais ser\u00e3o as altera\u00e7\u00f5es necess\u00e1rias na infraestrutura do pr\u00e9dio (climatiza\u00e7\u00e3o, el\u00e9trica, rede, etc.)?</li> <li>Temos espa\u00e7o f\u00edsico suficiente para instalar novos servidores?</li> <li>Temos equipe interna capacitada para gerenciar a infraestrutura?</li> </ul> <p>O colocation permite que as empresas se beneficiem de infraestrutura de data center de alta qualidade sem os custos e responsabilidades de gerenciar fisicamente o hardware.</p>"},{"location":"classes/04-cloud/intro/#caracteristicas-do-on-premises","title":"Caracter\u00edsticas do On-Premises","text":""},{"location":"classes/04-cloud/intro/#responsabilidades-operacionais","title":"Responsabilidades Operacionais","text":"<ul> <li>Manuten\u00e7\u00e3o de hardware: Se o hardware falha, a empresa precisa reparar ou substituir</li> <li>Ciclos de atualiza\u00e7\u00e3o: Gest\u00e3o de upgrades a cada poucos anos</li> <li>Capacidade de pico: Necessidade de hospedar capacidade suficiente para picos de demanda</li> </ul> <p>Exemplo Pr\u00e1tico</p> <p>Para um varejista online, isso significa ter capacidade suficiente para lidar com os picos de carga da Black Friday.</p> <p>Para engenheiros de dados, significa comprar sistemas grandes o suficiente para permitir boa performance em cargas de pico sem gastar excessivamente.</p>"},{"location":"classes/04-cloud/intro/#vantagens-do-on-premises","title":"Vantagens do On-Premises","text":"<ul> <li>Pr\u00e1ticas estabelecidas: Empresas estabelecidas t\u00eam pr\u00e1ticas operacionais que funcionam bem</li> <li>Controle total: Controle completo sobre hardware, software e dados</li> <li>Compliance: Pode ser necess\u00e1rio para certas regulamenta\u00e7\u00f5es</li> <li>Custos previs\u00edveis: Investimento inicial alto, mas custos operacionais mais previs\u00edveis</li> </ul>"},{"location":"classes/04-cloud/intro/#desafios-do-on-premises","title":"Desafios do On-Premises","text":"<ul> <li>Falta de agilidade: Tempo longo para provisionar novos recursos</li> <li>Subutiliza\u00e7\u00e3o: Hardware parado durante per\u00edodos de baixa demanda</li> <li>Expertise t\u00e9cnica: Necessidade de equipe especializada em infraestrutura</li> <li>Escalabilidade limitada: Dif\u00edcil escalar rapidamente para demandas imprevistas</li> </ul> <p>Question</p> <p>Uma empresa de e-commerce precisa dobrar sua capacidade de processamento de dados para a Black Friday, que acontece em 2 meses.</p> <p>Qual seria o principal desafio usando infraestrutura on-premises?</p> Tempo longo para comprar, instalar e configurar novos servidores Custo alto dos servidores Falta de expertise t\u00e9cnica Problemas de seguran\u00e7a Submit <p>Answer</p> <p>O principal desafio seria o tempo.</p> <p>Comprar, instalar e configurar novos servidores pode levar semanas ou meses, tornando imposs\u00edvel atender \u00e0 demanda da Black Friday a tempo.</p>"},{"location":"classes/04-cloud/intro/#cloud-a-revolucao","title":"Cloud: A Revolu\u00e7\u00e3o!","text":"<p>A computa\u00e7\u00e3o em nuvem inverte completamente o modelo on-premises.</p> <p>Defini\u00e7\u00e3o: Cloud Computing</p> <p>Na computa\u00e7\u00e3o em nuvem, ao inv\u00e9s de comprar hardware, voc\u00ea simplesmente aluga hardware e servi\u00e7os gerenciados de um provedor de nuvem (como AWS, Azure ou Google Cloud).</p> <p>Esses recursos podem ser reservados em bases extremamente curtas: VMs s\u00e3o criadas em menos de um minuto, e o uso subsequente \u00e9 cobrado em incrementos (por tempo de uso).</p>"},{"location":"classes/04-cloud/intro/#por-que-a-nuvem-e-revolucionaria","title":"Por Que a Nuvem \u00e9 Revolucion\u00e1ria?","text":""},{"location":"classes/04-cloud/intro/#escalabilidade-dinamica","title":"Escalabilidade Din\u00e2mica","text":"<p>A nuvem permite aos usu\u00e1rios escalar dinamicamente recursos que eram inconceb\u00edveis com servidores on-premises.</p> <p>Exemplo de Escalabilidade</p> <p>Um pipeline de dados que processa 100GB por dia durante a semana pode precisar processar 1TB no final do m\u00eas. Na nuvem, voc\u00ea pode:</p> <ol> <li>Executar com 2 m\u00e1quinas durante a semana</li> <li>Automaticamente escalar para 20 m\u00e1quinas no final do m\u00eas</li> <li>Retornar para 2 m\u00e1quinas depois do processamento</li> <li>Pagar apenas pelos recursos utilizados</li> </ol>"},{"location":"classes/04-cloud/intro/#agilidade-para-experimentacao","title":"Agilidade para Experimenta\u00e7\u00e3o","text":"<p>Engenheiros podem rapidamente lan\u00e7ar projetos e experimentar sem se preocupar com planejamento de hardware de longo prazo. Podem come\u00e7ar a rodar servidores assim que seu c\u00f3digo est\u00e1 pronto para deploy.</p> <p>Exercise</p> <p>Pense em um cen\u00e1rio onde voc\u00ea precisa testar uma nova arquitetura de pipeline de dados. Como a agilidade da nuvem poderia acelerar esse processo comparado ao on-premises?</p> Submit <p>Answer</p> <p>Na nuvem, voc\u00ea pode instantaneamente criar o ambiente de teste completo (bancos de dados, processamento, storage), testar sua arquitetura, e depois deletar tudo pagando apenas pelas horas utilizadas.</p> <p>No modelo on-premises isso poderia levar semanas apenas para conseguir hardware dispon\u00edvel para teste.</p>"},{"location":"classes/04-cloud/intro/#evolucao-dos-servicos-em-nuvem","title":"Evolu\u00e7\u00e3o dos Servi\u00e7os em Nuvem","text":"<p>A era inicial da nuvem era dominada por ofertas de Infrastructure as a Service (IaaS). No modelo IaaS, a provedora fornece produtos como VMs e discos virtuais, que s\u00e3o essencialmente fatias alugadas de hardware.</p>"},{"location":"classes/04-cloud/intro/#tres-modelos-de-servico","title":"Tr\u00eas Modelos de Servi\u00e7o","text":""},{"location":"classes/04-cloud/intro/#infrastructure-as-a-service-iaas","title":"Infrastructure as a Service (IaaS)","text":"<ul> <li>O que \u00e9: Recursos b\u00e1sicos de computa\u00e7\u00e3o (VMs, storage, rede)</li> <li>Responsabilidade: Voc\u00ea gerencia o sistema operacional e aplica\u00e7\u00f5es</li> <li>Exemplos: Amazon EC2, Google Compute Engine, Azure Virtual Machines</li> </ul>"},{"location":"classes/04-cloud/intro/#platform-as-a-service-paas","title":"Platform as a Service (PaaS)","text":"<ul> <li>O que \u00e9: Inclui IaaS + servi\u00e7os gerenciados mais sofisticados</li> <li>Responsabilidade: Provedor gerencia infraestrutura e plataforma</li> <li>Exemplos para Dados: </li> <li>Bancos gerenciados: Amazon RDS, Google Cloud SQL</li> <li>Streaming: Amazon Kinesis, Azure Event Hubs</li> <li>Containers: Google Kubernetes Engine (GKE), Azure Kubernetes Service (AKS)</li> </ul>"},{"location":"classes/04-cloud/intro/#software-as-a-service-saas","title":"Software as a Service (SaaS)","text":"<ul> <li>O que \u00e9: Plataforma de software empresarial totalmente funcional</li> <li>Responsabilidade: Provedor gerencia tudo, voc\u00ea apenas usa</li> <li>Exemplos: Salesforce, Google Workspace, Fivetran (para engenharia de dados)</li> </ul> <p>Serverless: O Futuro?</p> <p>Serverless est\u00e1 se tornando cada vez mais importante em ofertas PaaS e SaaS.</p> <p>Produtos serverless geralmente oferecem escalonamento automatizado de zero a taxas de uso extremamente altas, s\u00e3o cobrados por uso, e permitem que engenheiros operem sem conhecimento operacional dos servidores subjacentes.</p> <p>Question</p> <p>Voc\u00ea precisa de um banco de dados PostgreSQL para seu pipeline de dados, mas n\u00e3o quer se preocupar com backups, atualiza\u00e7\u00f5es de seguran\u00e7a ou monitoramento.</p> <p>Qual modelo de servi\u00e7o seria mais adequado?</p> IaaS - Criar uma VM e instalar PostgreSQL PaaS - Usar um servi\u00e7o gerenciado como Amazon RDS SaaS - Usar uma ferramenta de BI Serverless - Usar fun\u00e7\u00f5es Lambda Submit <p>Answer</p> <p>PaaS seria o modelo ideal.</p> <p>Servi\u00e7os como Amazon RDS, Google Cloud SQL ou Azure Database fornecem PostgreSQL totalmente gerenciado, onde o provedor cuida de backups, patches de seguran\u00e7a, monitoramento e manuten\u00e7\u00e3o.</p>"},{"location":"classes/04-cloud/intro/#por-que-cloud-e-importante-para-engenharia-de-dados","title":"Por Que Cloud \u00e9 Importante para Engenharia de Dados?","text":"<p>A nuvem oferece uma s\u00e9rie de vantagens que a tornam importante para engenharia de dados:</p>"},{"location":"classes/04-cloud/intro/#1-natureza-dos-workloads-de-dados","title":"1. Natureza dos Workloads de Dados","text":"<p>Os workloads de engenharia de dados t\u00eam caracter\u00edsticas \u00fanicas:</p> <ul> <li>Variabilidade: Processamento batch pode precisar de 10x mais recursos durante execu\u00e7\u00e3o</li> <li>Experimenta\u00e7\u00e3o: Necessidade constante de testar novas arquiteturas e ferramentas  </li> <li>Sazonalidade: E-commerce tem picos na Black Friday, varejo tem sazonalidade</li> <li>Crescimento imprevis\u00edvel: Volume de dados pode crescer exponencialmente</li> </ul> <p>COVID-19</p> <p>A ocorr\u00eancia do COVID-19 em 2020 foi um grande driver da ado\u00e7\u00e3o de nuvem.</p> <p>Empresas reconheceram o valor de rapidamente escalar processos de dados para obter insights em um clima de neg\u00f3cios altamente incerto.</p> <p>Tamb\u00e9m tiveram que lidar com carga substancialmente aumentada devido ao pico de compras online e trabalho remoto.</p>"},{"location":"classes/04-cloud/intro/#2-diversidade-de-ferramentas","title":"2. Diversidade de Ferramentas","text":"<p>Engenharia de dados moderna requer uma ampla gama de ferramentas especializadas:</p> <ul> <li>Armazenamento: Data lakes, data warehouses, object storage</li> <li>Processamento: Batch processing, stream processing, ETL/ELT</li> <li>Orquestra\u00e7\u00e3o: Workflow management, job scheduling</li> <li>Monitoramento: Observability, data quality, lineage</li> </ul> <p>Managed Services</p> <p>Provedores de nuvem oferecem vers\u00f5es gerenciadas dessas ferramentas, permitindo que engenheiros ignorem detalhes operacionais de gerenciar m\u00e1quinas individuais e implantar frameworks em sistemas distribu\u00eddos.</p>"},{"location":"classes/04-cloud/intro/#3-modelo-de-pricing-flexivel","title":"3. Modelo de Pricing Flex\u00edvel","text":""},{"location":"classes/04-cloud/intro/#problemas-do-modelo-traditional","title":"Problemas do Modelo Traditional","text":"<ul> <li>Overprovisioning: Comprar para pico de capacidade</li> <li>Underutilization: Hardware parado maior parte do tempo</li> <li>Capex vs Opex: Grande investimento inicial vs custos operacionais flex\u00edveis</li> </ul>"},{"location":"classes/04-cloud/intro/#vantagens-do-modelo-cloud","title":"Vantagens do Modelo Cloud","text":"<ul> <li>Pay-as-you-go: Pague apenas pelos recursos utilizados</li> <li>Spot instances: Inst\u00e2ncias com desconto para workloads tolerantes a falhas</li> <li>Reserved instances: Descontos para comprometimentos de longo prazo</li> </ul> <p>Exercise</p> <p>Uma empresa processa dados em batch uma vez por m\u00eas, usando 100 servidores por 8 horas. Como os custos se comparariam entre on-premises e cloud?</p> Submit <p>Answer</p> <p>On-premises: Precisa manter 100 servidores 24/7, pagando por 720 horas por m\u00eas mesmo usando apenas 8 horas (mesmo que sejam desligados, foi necess\u00e1rio compr\u00e1-los).</p> <p>Cloud: Paga apenas pelas 8 horas \u00d7 100 servidores = 800 server-hours por m\u00eas. Potencial economia de custos de infraestrutura, energia, refrigera\u00e7\u00e3o, espa\u00e7o f\u00edsico e pessoal operacional.</p> <p>N\u00e3o se engane!</p> <p>A nuvem n\u00e3o \u00e9 uma solu\u00e7\u00e3o m\u00e1gica e a praticidade tem seu custo!</p> <p>Decis\u00f5es erradas podem levar a gastos inesperados.</p>"},{"location":"classes/04-cloud/intro/#4-escalabilidade-e-performance","title":"4. Escalabilidade e Performance","text":""},{"location":"classes/04-cloud/intro/#limitacoes-fisicas","title":"Limita\u00e7\u00f5es F\u00edsicas","text":"<p>Estamos chegando aos limites f\u00edsicos de qu\u00e3o r\u00e1pido uma \u00fanica CPU pode ser. Para escalar para mais dados, voc\u00ea deve ser capaz de paralelizar sua computa\u00e7\u00e3o.</p> <p>Horizontal vs Vertical Scaling</p> <ul> <li>Vertical scaling: Comprar uma m\u00e1quina melhor (limitado fisicamente)</li> <li>Horizontal scaling: Adicionar mais m\u00e1quinas (virtualmente ilimitado)</li> </ul>"},{"location":"classes/04-cloud/intro/#algoritmos-paralelos","title":"Algoritmos Paralelos","text":"<p>Isso levou ao surgimento de algoritmos paralelos shared-nothing e seus sistemas correspondentes, como MapReduce. A nuvem facilita enormemente esse tipo de processamento distribu\u00eddo.</p> <p>Question</p> <p>Seu pipeline de dados precisa processar 10TB de dados em 20 minutos para um relat\u00f3rio urgente. Qual abordagem seria mais eficaz?</p> Usar uma \u00fanica m\u00e1quina muito potente (vertical scaling) Distribuir o processamento entre 100 m\u00e1quinas menores (horizontal scaling) Processar sequencialmente durante a madrugada Reduzir o volume de dados processados Submit <p>Answer</p> <p>Horizontal scaling seria mais eficaz.</p> <p>Distribuir entre 100 m\u00e1quinas permite paraleliza\u00e7\u00e3o massiva, completando o job muito mais r\u00e1pido que uma \u00fanica m\u00e1quina, independentemente de qu\u00e3o poderosa seja.</p>"},{"location":"classes/04-cloud/intro/#impacto-na-engenharia-de-dados-moderna","title":"Impacto na Engenharia de Dados Moderna","text":""},{"location":"classes/04-cloud/intro/#transformacao-dos-data-warehouses-e-data-lakes","title":"Transforma\u00e7\u00e3o dos Data Warehouses e Data Lakes","text":"<p>Tr\u00eas fatores transformaram completamente a paisagem de analytics e data warehousing nos \u00faltimos 10 anos:</p> <ol> <li>Facilidade de constru\u00e7\u00e3o: Deploy de pipelines, data lakes, warehouses e processamento anal\u00edtico na nuvem sem larga depend\u00eancia de departamentos de TI</li> <li>Queda cont\u00ednua nos custos: Armazenamento em nuvem ficou extremamente barato (pelo menos inicialmente!)</li> <li>Databases colunares escal\u00e1veis: Amazon Redshift, Snowflake, Google BigQuery</li> </ol> <p>Relembrando defini\u00e7\u00f5es Importantes</p> <p>Data Warehouse: Database onde dados de diferentes sistemas s\u00e3o armazenados e modelados para suportar an\u00e1lise. Dados estruturados e otimizados para queries de relat\u00f3rio.</p> <p>Data Lake: Onde dados s\u00e3o armazenados sem a estrutura ou otimiza\u00e7\u00e3o de query de um data warehouse. Alto volume e variedade de tipos de dados.</p> Novas Arquiteturas <p>A nuvem permite arquiteturas que eram impratic\u00e1veis on-premises:</p> <ul> <li>Serverless data processing: Processamento em escala, sob demanda, sem necessidade de gerenciar servidores ou infraestrutura</li> <li>Multi-region deployments: Replica\u00e7\u00e3o global com baixa lat\u00eancia</li> <li>Event-driven architectures: Processamento reativo baseado em eventos</li> <li>Elastic clusters: Clusters que crescem e encolhem baseado na demanda</li> </ul> <p>Exercise</p> <p>Como voc\u00ea descreveria, com suas palavras, a principal diferen\u00e7a entre um data warehouse e um data lake?</p> <p>Em que cen\u00e1rios cada um seria mais adequado?</p> Submit <p>Answer</p> <p>Data Warehouse \u00e9 como uma biblioteca organizada: dados limpos, catalogados e otimizados para consultas espec\u00edficas. Ideal para relat\u00f3rios regulares e an\u00e1lises de BI.</p> <p>Data Lake \u00e9 como um dep\u00f3sito: armazena todos os tipos de dados (estruturados e n\u00e3o estruturados) sem organiza\u00e7\u00e3o pr\u00e9via. Ideal para explora\u00e7\u00e3o de dados, machine learning e quando voc\u00ea n\u00e3o sabe ainda que an\u00e1lises precisar\u00e1 fazer.</p>"},{"location":"classes/04-cloud/intro/#consideracoes-e-desafios","title":"Considera\u00e7\u00f5es e Desafios","text":""},{"location":"classes/04-cloud/intro/#mudanca-de-mentalidade","title":"Mudan\u00e7a de Mentalidade","text":"<p>Migrar para a nuvem requer uma mudan\u00e7a dram\u00e1tica na forma de pensar, especialmente sobre pricing. Empresas frequentemente cometem erros graves de deployment por n\u00e3o adaptar adequadamente suas pr\u00e1ticas ao modelo de pricing da nuvem.</p>"},{"location":"classes/04-cloud/intro/#finops-financial-operations","title":"FinOps: Financial Operations","text":"<p>FinOps</p> <p>FinOps \u00e9 uma pr\u00e1tica cultural e disciplina operacional que une equipes de tecnologia, neg\u00f3cios e finan\u00e7as para acelerar a cria\u00e7\u00e3o de valor de neg\u00f3cio atrav\u00e9s de tomada de decis\u00e3o informada sobre dados financeiros na nuvem.</p> <p>Principais pr\u00e1ticas de FinOps para engenharia de dados:</p> <ul> <li>Tagging e categoriza\u00e7\u00e3o de recursos</li> <li>Monitoring de custos em tempo real</li> <li>Right-sizing de inst\u00e2ncias</li> <li>Scheduling de recursos n\u00e3o-cr\u00edticos</li> <li>Reserved instances para workloads previs\u00edveis</li> </ul>"},{"location":"classes/04-cloud/intro/#desafios-comuns","title":"Desafios Comuns","text":"<ol> <li>Vendor lock-in: Depend\u00eancia de servi\u00e7os espec\u00edficos de um provedor</li> <li>Complexidade de pricing: Centenas de servi\u00e7os com modelos diferentes</li> <li>Seguran\u00e7a e compliance: Novos desafios em ambiente compartilhado</li> <li>Skills gap: Necessidade de novas compet\u00eancias t\u00e9cnicas</li> </ol> <p>Question</p> <p>Uma empresa est\u00e1 preocupada com os custos crescentes de sua infraestrutura de dados na nuvem. Qual seria a primeira a\u00e7\u00e3o recomendada?</p> Migrar de volta para on-premises Trocar de provedor de nuvem Implementar pr\u00e1ticas de FinOps e monitoramento de custos Reduzir a quantidade de dados processados Submit <p>Answer</p> <p>A primeira a\u00e7\u00e3o deve ser implementar pr\u00e1ticas de FinOps.</p> <p>Muitas vezes os custos altos s\u00e3o resultado de recursos mal dimensionados, n\u00e3o tagueados ou executando desnecessariamente.</p> <p>Monitoramento e otimiza\u00e7\u00e3o s\u00e3o fundamentais antes de considerar mudan\u00e7as dr\u00e1sticas.</p>"},{"location":"classes/04-cloud/intro/#conclusao","title":"Conclus\u00e3o","text":"<p>A computa\u00e7\u00e3o em nuvem n\u00e3o \u00e9 apenas uma quest\u00e3o de onde executar sua infraestrutura, \u00e9 uma mudan\u00e7a fundamental em como pensamos sobre recursos computacionais, escalabilidade e opera\u00e7\u00f5es.</p> <p>Para engenheiros de dados, a nuvem oferece:</p> <ul> <li>Agilidade para experimentar e inovar rapidamente</li> <li>Escalabilidade para lidar com volumes crescentes de dados  </li> <li>Diversidade de ferramentas especializadas e gerenciadas</li> <li>Flexibilidade financeira com modelos pay-as-you-go</li> </ul> <p>Pr\u00f3ximos Passos</p> <p>Nas pr\u00f3ximas aulas, exploraremos na pr\u00e1tica como utilizar servi\u00e7os de nuvem da AWS para implementar pipelines de dados.</p> <p>Exercise</p> <p>Reflita sobre sua futura carreira: em que contextos voc\u00ea escolheria on-premises vs cloud?</p> <p>Quais fatores seriam decisivos na sua recomenda\u00e7\u00e3o para uma empresa?</p> Submit <p>Answer</p> <p>On-premises pode ser adequado para:</p> <ul> <li>Empresas com regulamenta\u00e7\u00f5es r\u00edgidas de dados</li> <li>Workloads extremamente previs\u00edveis e est\u00e1veis  </li> <li>Organiza\u00e7\u00f5es com expertise t\u00e9cnica profunda</li> <li>Casos onde a economia de escala justifica o investimento</li> </ul> <p>Cloud \u00e9 geralmente prefer\u00edvel para:</p> <ul> <li>Startups e empresas em crescimento</li> <li>Workloads vari\u00e1veis ou experimentais</li> <li>Necessidade de agilidade e inova\u00e7\u00e3o</li> <li>Falta de expertise em infraestrutura</li> <li>Requisitos de escalabilidade global</li> </ul> <p>A decis\u00e3o deve considerar: custo total de propriedade, requisitos de compliance, expertise dispon\u00edvel, velocidade de inova\u00e7\u00e3o necess\u00e1ria e caracter\u00edsticas dos workloads.</p>"},{"location":"classes/04-cloud/intro/#leituras-interessantes","title":"Leituras interessantes","text":"<p>Leitura opcional sobre empresas deixando a nuvem:</p> <ul> <li>Dropbox 1 e Dropbox 2</li> <li>Why Companies Are Ditching the Cloud</li> </ul>"},{"location":"classes/04-cloud/pratica/","title":"Pr\u00e1tica","text":""},{"location":"classes/04-cloud/pratica/#pratica","title":"Pr\u00e1tica","text":""},{"location":"classes/04-cloud/pratica/#api-de-filmes","title":"API de filmes","text":"<p>Para a pr\u00e1tica, iremos utilizar uma API de filmes.</p> <p>Ela, constru\u00edda em fastapi, ser\u00e1 respons\u00e1vel por fornecer informa\u00e7\u00f5es sobre filmes, como t\u00edtulo e ano de lan\u00e7amento. As informa\u00e7\u00f5es ser\u00e3o lidas a partir de um JSON.</p> <p>Perigo!</p> <p>Se fizer esta aula pela metade, n\u00e3o deixe VMs em execu\u00e7\u00e3o.</p> <p>Confira o comando para parar a VM ao final da p\u00e1gina. Voc\u00ea tamb\u00e9m pode utilizar o console (menu superior Instance state / Stop instance).</p> <p>Qualquer d\u00favida, entre em contato com o professor.</p> <p>Antes de realizarmos o deploy na AWS, vamos testar localmente e garantir que funciona. Para isso:</p> <p>Exerc\u00edcio</p> <p>Em sua m\u00e1quina, clone o reposit\u00f3rio:</p> <pre><code>$ git clone https://github.com/macielcalebe/movies-api-example-01.git movies-api\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Crie um ambiente virtual utilizando a ferramenta de sua prefer\u00eancia e instale as depend\u00eancias do projeto:</p> <pre><code>$ cd movies-api\n$ uv venv --python 3.12 .venv\n$ source .venv/bin/activate\n$ uv pip install -r requirements.txt\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Teste a API localmente utilizando o comando:</p> <pre><code>$ fastapi dev src/main.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Teste em seu navegador:</p> <p>Aten\u00e7\u00e3o</p> <p>Confira se a porta \u00e9 realmente <code>8000</code>, caso contr\u00e1rio, ajuste a URL.</p> <ul> <li>http://localhost:8000</li> <li>http://localhost:8000/docs</li> <li>http://localhost:8000/filmes/avatar e tente com outros filmes!</li> <li>http://localhost:8000/filmes</li> </ul> Mark as done <p>Answer</p> <p>No exemplo do <code>avatar</code>, voc\u00ea deve ver algo como: </p> <p>Agora que conferimos a API localmente, vamos explorar o console da AWS.</p> <p>Exercise</p> <p>Antes de prosseguir, acesse a URL de login da conta AWS da disciplina e fa\u00e7a login.</p> Mark as done"},{"location":"classes/04-cloud/pratica/#explorando-o-console","title":"Explorando o Console","text":"<p>O console da AWS \u00e9 uma interface gr\u00e1fica baseada na web que permite gerenciar e interagir com os servi\u00e7os da AWS.</p> <p>Vamos explorar o console para entender melhor como ele funciona. Com login realizado na conta da AWS, encontre o link AdministratorAccess para acessar o console.</p>"},{"location":"classes/04-cloud/pratica/#ec2","title":"EC2","text":"<p>No console da AWS, navegue at\u00e9 o servi\u00e7o EC2 (pela barra de busca). Voc\u00ea deve ver um painel como:</p> <p></p> <p>O EC2 (Elastic Compute Cloud) \u00e9 um servi\u00e7o que fornece capacidade de computa\u00e7\u00e3o escal\u00e1vel na nuvem.</p> <p>Ele permite que os usu\u00e1rios criem e gerenciem inst\u00e2ncias de servidores virtuais (VMs, inst\u00e2ncias EC2).</p> <p>As inst\u00e2ncias EC2 podem ser configuradas com diferentes tipos de hardware, sistemas operacionais e softwares, permitindo que os usu\u00e1rios personalizem suas aplica\u00e7\u00f5es de acordo com suas necessidades.</p>"},{"location":"classes/04-cloud/pratica/#criar-instancia","title":"Criar inst\u00e2ncia","text":"<p>Vamos criar uma inst\u00e2ncia para fazer deploy da nossa API de filmes. Inicialmente, vamos criar recursos utilizando o console da AWS.</p>"},{"location":"classes/04-cloud/pratica/#regiao","title":"Regi\u00e3o","text":"<p>Ap\u00f3s acessar o EC2, garanta que est\u00e1 na regi\u00e3o <code>N. Virginia</code> (<code>us-east-1</code>). Esta informa\u00e7\u00e3o estar\u00e1 dispon\u00edvel no menu superior direito.</p> <p></p> <p>Exercise</p> <p>Selecione a regi\u00e3o correta (<code>us-east-1</code>) antes de prosseguir.</p> Mark as done"},{"location":"classes/04-cloud/pratica/#instancias","title":"Inst\u00e2ncias","text":"<p>No menu esquerdo, clique em Instances (Inst\u00e2ncias). Voc\u00ea deve ver um painel como:</p> <p></p>"},{"location":"classes/04-cloud/pratica/#criacao","title":"Cria\u00e7\u00e3o","text":"<p>Clique em Launch Instances (Iniciar Inst\u00e2ncias) para criar uma nova inst\u00e2ncia.</p> <p>A primeira configura\u00e7\u00e3o necess\u00e1ria ser\u00e1 a escolha do sistema operacional. Escolha conforme a imagem (Ubuntu 24.04 LTS):</p> <p></p> <p>Em seguida, escolha o tipo de inst\u00e2ncia, que delimita o hardware da sua inst\u00e2ncia. Escolha uma <code>t3a.micro</code>.</p> <p></p> <p>Exercise</p> <p>Pesquise sobre as inst\u00e2ncias. Qual a arquitetura da <code>t3a.micro</code>? Qual a arquitetura das inst\u00e2ncias <code>t4g.*</code>?</p> Submit <p>Answer</p> <p>A arquitetura da <code>t3a.micro</code> \u00e9 <code>AMD64</code> (<code>x86_64</code>), aquela que estudaram em SisHard!</p> <p>A arquitetura das inst\u00e2ncias <code>t4g.*</code> \u00e9 <code>ARM64</code> (<code>aarch64</code>).</p> <p>Ent\u00e3o, vamos escolher a chave mestre para acessar a inst\u00e2ncia. Isto ser\u00e1 feito pela cria\u00e7\u00e3o de uma key pair.</p> <p></p> <p>Clique em Create new key pair (Criar novo par de chaves).</p> <p></p> <p>Exercise</p> <p>Garanta que voc\u00ea fez o download da chave privada (<code>*.pem</code>).</p> Mark as done <p>Aten\u00e7\u00e3o</p> <p>Salve a chave privada (<code>*.pem</code>) em um local seguro.</p> <p>Voc\u00ea precisar\u00e1 dela para acessar sua inst\u00e2ncia.</p> <p>N\u00e3o compartilhar</p> <p>Nunca compartilhe sua chave privada com ningu\u00e9m!</p> <p>Nas configura\u00e7\u00f5es de rede, configure conforme a imagem:</p> <p>Aten\u00e7\u00e3o</p> <p>Geralmente, utilizar regras de acesso SSH de qualquer IP (<code>0.0.0.0/0</code>) n\u00e3o \u00e9 recomendado em ambientes de produ\u00e7\u00e3o.</p> <p></p> <p>Ap\u00f3s isso, clique em Launch Instances (Iniciar Inst\u00e2ncias) para criar a inst\u00e2ncia!</p> <p>Exercise</p> <p>Volte para o Dashboard do EC2 e verifique se sua inst\u00e2ncia est\u00e1 em execu\u00e7\u00e3o.</p> Mark as done"},{"location":"classes/04-cloud/pratica/#acesso-ssh","title":"Acesso SSH","text":"<p>Vamos realizar acesso via SSH. Para isso, garanta que voc\u00ea tem a chave privada (<code>.pem</code>) que foi gerada durante a cria\u00e7\u00e3o da inst\u00e2ncia, com as permiss\u00f5es corretas.</p> <p>Aten\u00e7\u00e3o</p> <p>Os comandos abaixo entendem que o arquivo da chave privada est\u00e1 localizado na pasta de trabalho atual.</p> <p>Caso tenha movido para, por exemplo, <code>~/.ssh/</code>, atualize os comandos.</p> <p>Exercise</p> <p>Para acessar a VM utilizando SSH, utilize:</p> <p>Endere\u00e7o IP</p> <p>No painel EC2, localize sua inst\u00e2ncia e copie o Public DNS ou Public IP e sustitua no comando abaixo (onde est\u00e1 <code>17.204.229.1</code>):</p> <pre><code>$ chmod 700 dataeng.pem\n$ ssh ubuntu@17.204.229.1 -i dataeng.pem\n</code></pre> <p>Responda <code>y</code> para a pergunta de seguran\u00e7a!</p> Mark as done <p>Aten\u00e7\u00e3o!</p> <p>Os pr\u00f3ximos comandos devem ser executados dentro da sess\u00e3o SSH.</p> <p>Ou seja, na VM!</p> <p>Exercise</p> <p>Para manter a conex\u00e3o ativa e n\u00e3o perdermos a sess\u00e3o SSH, podemos usar o comando <code>tmux</code> ou <code>screen</code>.  <pre><code>$ tmux\n</code></pre> <p>Para testar, ap\u00f3s dar ENTER, feche a janela (sem dar <code>exit</code> ou <code>logout</code>) e abra novamente a conex\u00e3o SSH. </p> <p>Reestabele\u00e7a a sess\u00e3o com:</p> <pre><code>$ tmux a\n</code></pre> <p>Sim, um <code>\"a\"</code> no final!</p> Mark as done"},{"location":"classes/04-cloud/pratica/#configurando-a-api","title":"Configurando a API","text":"<p>Agora que voc\u00ea tem acesso \u00e0 VM, vamos clonar o reposit\u00f3rio da API e testar se ele funciona corretamente.</p> <p>Exercise</p> <p>Clone o reposit\u00f3rio (pode ser na pasta atual, <code>/home/ubuntu</code>):</p> <pre><code>$ git clone https://github.com/macielcalebe/movies-api-example-01.git\n</code></pre> Mark as done <p>Exercise</p> <p>Crie um ambiente virtual e instale as depend\u00eancias.</p> <p>Voc\u00ea pode instalar <code>uv</code> na VM!</p> Mark as done <p>Exercise</p> <p>Com o ambiente virtual ativado, inicialize a API com:  <pre><code>$ fastapi run src/main.py --host 0.0.0.0 --port 8000\n</code></pre> Mark as done <p>Exercise</p> <p>Tente acessar a API em seu navegador. Utilize o endere\u00e7o <code>http://&lt;Public_IP&gt;:8000</code>.</p> <p>Aten\u00e7\u00e3o</p> <p>Lembre-se de substituir <code>&lt;Public_IP&gt;</code> pelo endere\u00e7o IP p\u00fablico da sua inst\u00e2ncia EC2.</p> <p>N\u00e3o ir\u00e1 funcionar, mas tente mesmo assim e siga o handout!</p> Mark as done <p>Provavelmente n\u00e3o ir\u00e1 funcionar. Por quest\u00e3o de seguran\u00e7a, o EC2 bloqueia o acesso \u00e0 porta 8000 por padr\u00e3o. Para resolver isso, voc\u00ea precisar\u00e1 ajustar as regras de seguran\u00e7a do grupo de seguran\u00e7a associado \u00e0 sua inst\u00e2ncia.</p> <p>Exercise</p> <p>Acesse Instances no painel do EC2 e localize sua inst\u00e2ncia. Em seguida, na aba Security, clique no ID do grupo de seguran\u00e7a associado \u00e0 sua inst\u00e2ncia (Security groups).</p> <p>Clique em Edit inbound rules e adicione uma nova regra:</p> <ul> <li>Tipo: Custom TCP</li> <li>Protocolo: TCP</li> <li>Porta: 8000</li> <li>Origem: 0.0.0.0/0 (ou o IP espec\u00edfico que voc\u00ea deseja permitir)</li> </ul> <p>Ap\u00f3s adicionar a regra, clique em Save rules.</p> <p></p> Mark as done <p>Exercise</p> <p>Tente acessar novamente a API em seu navegador. Utilize o endere\u00e7o <code>http://&lt;Public_IP&gt;:8000</code>.</p> <p>Agora deve funcionar! Teste o acesso \u00e0 documenta\u00e7\u00e3o e as rotas de consultas a filmes!</p> Mark as done <p>Exercise</p> <p>Derrube a API antes de continuar!</p> Mark as done"},{"location":"classes/04-cloud/pratica/#configuracao-de-servico","title":"Configura\u00e7\u00e3o de servi\u00e7o","text":"<p>Exercise</p> <p>Qual o usu\u00e1rio da m\u00e1quina que roda o processo toda vez que a API \u00e9 acessada?</p> <p>Tem algo errado com esta pr\u00e1tica?</p> Submit <p>Answer</p> <p>O usu\u00e1rio que roda o processo da API \u00e9 o usu\u00e1rio <code>ubuntu</code>.</p> <p>Como o usu\u00e1rio <code>ubuntu</code> tem permiss\u00f5es de administrador, isso pode representar um risco de seguran\u00e7a.</p> <p>\u00c9 recomend\u00e1vel criar um usu\u00e1rio espec\u00edfico para rodar a API com permiss\u00f5es limitadas.</p> <p>Vamos fazer isto e al\u00e9m, criaremos um servi\u00e7o para nossa API.</p> <p>Exercise</p> <p>Mas o que \u00e9 um servi\u00e7o?!</p> Submit <p>Answer</p> <p>Um servi\u00e7o \u00e9 um programa que roda em segundo plano e \u00e9 gerenciado pelo sistema operacional.</p> <p>Ele \u00e9 projetado para fornecer funcionalidades espec\u00edficas e pode ser iniciado automaticamente na inicializa\u00e7\u00e3o do sistema.</p>"},{"location":"classes/04-cloud/pratica/#criar-usuario","title":"Criar usu\u00e1rio","text":"<p>O usu\u00e1rio para rodar o servi\u00e7o da API ser\u00e1 chamado <code>uapi</code>. Seus arquivos estar\u00e3o localizados na pasta <code>/srv/movies-api</code>.</p> <p>Exercise</p> <p>Pesquise sobre a hierarquia de pastas raiz do Linux. Quais s\u00e3o as pastas principais?</p> Submit <p>Exercise</p> <p>Crie o usu\u00e1rio e a pasta de destino do servi\u00e7o com:</p> <pre><code>$ sudo useradd --system --create-home --home-dir /srv/movies-api --shell /usr/sbin/nologin uapi\n$ sudo mkdir -p /srv/movies-api/app\n$ sudo chown -R uapi:uapi /srv/movies-api/app\n</code></pre> Mark as done <p>Exercise</p> <p>Perceba que criamos a pasta <code>app</code> para receber os arquivos da aplica\u00e7\u00e3o.</p> <p>Clone com:</p> <pre><code>$ sudo -u uapi git clone https://github.com/macielcalebe/movies-api-example-01.git /srv/movies-api/app\n</code></pre> Mark as done"},{"location":"classes/04-cloud/pratica/#ambiente-virtual-api","title":"Ambiente virtual API","text":"<p>Vamos criar um ambiente virtual para o servi\u00e7o</p> uvvenv <pre><code>$ sudo -u uapi bash -lc 'curl -LsSf https://astral.sh/uv/install.sh | sh'\n$ sudo -u uapi bash -lc 'export PATH=\"$HOME/.cargo/bin:$PATH\"; uv venv /srv/movies-api/app/.venv'\n$ sudo -u uapi bash -lc 'export PATH=\"$HOME/.cargo/bin:$PATH\"; uv pip install -r /srv/movies-api/app/requirements.txt --python /srv/movies-api/app/.venv/bin/python'\n</code></pre> <pre><code>$ sudo apt-get update &amp;&amp; sudo apt-get install -y python3-venv\n$ sudo -u uapi python3 -m venv /srv/movies-api/app/.venv\n$ sudo -u uapi bash -lc '/srv/movies-api/app/.venv/bin/pip install --upgrade pip'\n$ sudo -u uapi bash -lc '/srv/movies-api/app/.venv/bin/pip install -r /srv/movies-api/app/requirements.txt'\n</code></pre>"},{"location":"classes/04-cloud/pratica/#criar-servico","title":"Criar servi\u00e7o","text":"<p>Para criar um servi\u00e7o Linux, precisamos criar um arquivo de configura\u00e7\u00e3o na pasta <code>/etc/systemd/system/</code>.</p> <p>Exercise</p> <p>Execute:  <pre><code>$ sudo vim /etc/systemd/system/movies-api.service\n</code></pre> <p>No vim, aperte a tecla INSERT para entrar em modo de edi\u00e7\u00e3o.</p> <p>Cole o seguinte conte\u00fado:</p> <pre><code>[Unit]\nDescription=Movies API service\nAfter=network.target\n\n[Service]\nUser=uapi\nGroup=uapi\nWorkingDirectory=/srv/movies-api/app\nEnvironment=\"PATH=/srv/movies-api/app/.venv/bin\"\nExecStart=/srv/movies-api/app/.venv/bin/fastapi run src/main.py --workers 2 --host 0.0.0.0 --port 8000\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Em seguida, aperte a tecla ESC e digite <code>:wq</code> para salvar e sair (sim, tamb\u00e9m precisa digitar os dois pontos!).</p> Mark as done <p>Exercise</p> <p>Analise o arquivo de servi\u00e7o. Qual o usu\u00e1rio ir\u00e1 rodar a API?</p> Submit <p>Answer</p> <p>O usu\u00e1rio que roda a API \u00e9 o <code>uapi</code>.</p> <p>Agora que o servi\u00e7o foi criado, precisamos recarregar o systemd para reconhecer o novo servi\u00e7o.</p> <p>Exercise</p> <p>Atualize o systemd com:</p> <p>Info</p> <p>Realize este passo sempre que editar o arquivo de servi\u00e7o!</p> <pre><code>$ sudo systemctl daemon-reload\n</code></pre> Mark as done <p>Exercise</p> <p>Inicie ou reinicie o servi\u00e7o com:  <pre><code>$ sudo systemctl restart movies-api\n</code></pre> Mark as done <p>Exercise</p> <p>Confira o status do servi\u00e7o com:  <pre><code>$ sudo systemctl status movies-api\n</code></pre> Mark as done <p>Exercise</p> <p>Confira os logs do servi\u00e7o com:  <pre><code>$ sudo journalctl -u movies-api -f\n</code></pre> Mark as done <p>Exercise</p> <p>Acesse a API no IP p\u00fablico do navegador e confira se tudo est\u00e1 funcionando corretamente.</p> <p></p> Mark as done <p>Exercise</p> <p>Com tudo funcionando, vamos habilitar o servi\u00e7o para iniciar automaticamente com o sistema.  <pre><code>$ sudo systemctl enable movies-api\n</code></pre> Mark as done <p>Exercise</p> <p>Reinicie a VM e confira se o servi\u00e7o volta a funcionar.</p> <p>Aten\u00e7\u00e3o</p> <p>Espere alguns minutos (um ou dois) para a VM reiniciar completamente.</p> <pre><code>$ sudo shutdown -r 0\n</code></pre> Mark as done"},{"location":"classes/04-cloud/pratica/#fazendo-a-limpa","title":"Fazendo a limpa","text":"<p>EC2 \u00e9 caro!</p> <p>Servi\u00e7os de computa\u00e7\u00e3o em nuvem, como o EC2 da AWS, podem gerar custos significativos.</p> <p>Certifique-se de parar ou encerrar inst\u00e2ncias que n\u00e3o est\u00e3o em uso para evitar cobran\u00e7as desnecess\u00e1rias.</p> <p>Vamos fazer isto utilizando o AWS CLI.</p> <p>Exercise</p> <p>Liste as VMs criadas em sua conta:</p> <pre><code>$ aws ec2 describe-instances --query 'Reservations[*].Instances[*].{ID:InstanceId,State:State.Name}' --output table --profile dataeng\n</code></pre> Mark as done <p>Exercise</p> <p>Pare a inst\u00e2ncia retornada:</p> <p>Aten\u00e7\u00e3o</p> <p>No comando abaixo, substitua <code>i-00c68fbac********</code> pelo ID da inst\u00e2ncia que voc\u00ea deseja parar.</p> <pre><code>$ aws ec2 stop-instances --instance-ids i-00c68fbac******** --profile dataeng\n</code></pre> Mark as done <p>Info!</p> <p>Por enquanto, n\u00e3o \u00e9 necess\u00e1rio remover a inst\u00e2ncia. Existem gastos associados \u00e0 inst\u00e2ncias paradas, mas eles s\u00e3o muito menores do que inst\u00e2ncias em execu\u00e7\u00e3o.</p> <p>Apesar de n\u00e3o ser necess\u00e1rio, voc\u00ea pode fazer isso a qualquer momento utilizando o comando:</p> <p>Aten\u00e7\u00e3o</p> <p>No comando abaixo, substitua <code>i-00c68fbac********</code> pelo ID da inst\u00e2ncia que voc\u00ea deseja remover.</p> <p>Este comando \u00e9 irrevers\u00edvel e resultar\u00e1 na perda de todos os dados armazenados na inst\u00e2ncia (EBS) se a inst\u00e2ncia estiver com DeleteOnTermination habilitado.</p> <p>Confira utilizando o comando: <pre><code>aws ec2 describe-instances \\\n    --instance-ids i-00c68fbac******** \\\n    --query \"Reservations[].Instances[].BlockDeviceMappings[].Ebs\" \\\n    --profile dataeng\n</code></pre></p> <pre><code>$ aws ec2 terminate-instances --instance-ids i-00c68fbac******** --profile dataeng\n</code></pre>"},{"location":"classes/04-cloud/pratica/#conclusao","title":"Conclus\u00e3o","text":"<p>Esta foi nossa primeira experi\u00eancia com o EC2. Por enquanto, a configura\u00e7\u00e3o envolveu etapas manuais, n\u00e3o automatizadas. Trabalharemos nesse t\u00f3pico nas pr\u00f3ximas aulas!</p> <p>Outras ideias que poderiam ter sido exploradas (voc\u00ea pode tentar, s\u00f3 n\u00e3o deixe a VM ligada continuamente!):</p> <ul> <li>Configurar um servidor web, como Nginx, Apache ou Caddy, para servir a API.</li> <li>Configurar um dom\u00ednio personalizado para a API, com reverse proxy. Voc\u00ea pode conseguir um provis\u00f3rio com o DuckDNS.</li> <li>Configurar https para seu dom\u00ednio personalizado. Uma dica \u00e9 utilizar o certbot Let's Encrypt.</li> </ul>"},{"location":"classes/05-iac/intro/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/05-iac/intro/#introducao","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/05-iac/intro/#introducao-a-infrastructure-as-code-iac","title":"Introdu\u00e7\u00e3o \u00e0 Infrastructure as Code (IaC)","text":"<p>Na era moderna do desenvolvimento de software e engenharia de dados, a infraestrutura que suporta nossas aplica\u00e7\u00f5es se tornou t\u00e3o cr\u00edtica quanto o pr\u00f3prio c\u00f3digo. Tradicionalmente, o provisionamento e gerenciamento de infraestrutura era um processo manual, propenso a erros e inconsist\u00eancias entre diferentes ambientes.</p> <p>Imagine o seguinte cen\u00e1rio: voc\u00ea desenvolveu um pipeline de dados que funciona perfeitamente no seu ambiente local. Ao tentar replic\u00e1-lo no ambiente de produ\u00e7\u00e3o, descobrem que a vers\u00e3o do banco de dados \u00e9 diferente, as configura\u00e7\u00f5es de rede n\u00e3o s\u00e3o as mesmas, e v\u00e1rias depend\u00eancias est\u00e3o em vers\u00f5es incompat\u00edveis.</p> Problemas comuns <p>Situa\u00e7\u00f5es comuns incluem: diferen\u00e7as nas vers\u00f5es de bibliotecas, configura\u00e7\u00f5es de banco de dados distintas, vari\u00e1veis de ambiente n\u00e3o sincronizadas, permiss\u00f5es de acesso diferentes, ou at\u00e9 mesmo sistemas operacionais com configura\u00e7\u00f5es espec\u00edficas que funcionam em um ambiente mas falham em outro.</p> <p>Este tipo de problema motivou o surgimento do conceito de Infrastructure as Code, uma abordagem que revolucionou a forma como gerenciamos e provisionamos infraestrutura.</p>"},{"location":"classes/05-iac/intro/#o-que-e-infrastructure-as-code","title":"O que \u00e9 Infrastructure as Code?","text":"<p>Infrastructure as Code (IaC, Infraestrutura como C\u00f3digo) \u00e9 uma pr\u00e1tica de gerenciamento e provisionamento de infraestrutura atrav\u00e9s de c\u00f3digo, ao inv\u00e9s de processos manuais. Com IaC, voc\u00ea define sua infraestrutura usando arquivos de configura\u00e7\u00e3o que podem ser versionados, testados e reutilizados. </p> <p>Defini\u00e7\u00e3o</p> <p>Infrastructure as Code \u00e9 a pr\u00e1tica de gerenciar e provisionar recursos de infraestrutura (servidores, redes, bancos de dados, etc.) atrav\u00e9s de c\u00f3digo declarativo ou imperativo, permitindo automa\u00e7\u00e3o, versionamento e reprodutibilidade da infraestrutura.</p> <p>Pense na IaC como uma receita detalhada para construir sua infraestrutura. Ela especifica exatamente quais recursos criar e como configur\u00e1-los.</p>"},{"location":"classes/05-iac/intro/#caracteristicas-fundamentais","title":"Caracter\u00edsticas Fundamentais","text":"<p>A Infrastructure as Code possui algumas caracter\u00edsticas fundamentais que a diferenciam do gerenciamento tradicional de infraestrutura:</p> <p>Declarativa vs Imperativa: A maioria das ferramentas de IaC utiliza uma abordagem declarativa, onde voc\u00ea descreve o estado desejado da infraestrutura, e a ferramenta determina como alcan\u00e7ar esse estado.</p> <p>Versionamento: Como a infraestrutura \u00e9 definida em c\u00f3digo, ela pode ser versionada utilizando git (por exemplo), permitindo rastreamento de mudan\u00e7as e rollbacks quando necess\u00e1rio.</p> <p>Idempot\u00eancia: Executar o mesmo c\u00f3digo de infraestrutura m\u00faltiplas vezes deve produzir o mesmo resultado, sem efeitos colaterais indesejados.</p> <p>Exercise</p> <p>Explique com suas palavras o que significa \"idempot\u00eancia\" no contexto de Infrastructure as Code e por que essa caracter\u00edstica \u00e9 importante.</p> Submit <p>Answer</p> <p>Idempot\u00eancia significa que executar a mesma opera\u00e7\u00e3o v\u00e1rias vezes produz o mesmo resultado. No contexto de IaC, isso significa que aplicar o mesmo arquivo de configura\u00e7\u00e3o de infraestrutura m\u00faltiplas vezes n\u00e3o criar\u00e1 recursos duplicados ou causar\u00e1 conflitos.</p> <p>\u00c9 importante porque garante consist\u00eancia e previsibilidade, permitindo que voc\u00ea execute o c\u00f3digo de infraestrutura com seguran\u00e7a sempre que necess\u00e1rio.</p>"},{"location":"classes/05-iac/intro/#por-que-usar-iac","title":"Por que usar IaC?","text":"<p>A ado\u00e7\u00e3o de Infrastructure as Code traz benef\u00edcios significativos para equipes de desenvolvimento e opera\u00e7\u00f5es, especialmente em projetos de engenharia de dados onde a infraestrutura tende a ser complexa e distribu\u00edda.</p>"},{"location":"classes/05-iac/intro/#consistencia-entre-ambientes","title":"Consist\u00eancia entre Ambientes","text":"<p>Um dos maiores desafios em projetos de dados \u00e9 garantir que os ambientes de desenvolvimento, homologa\u00e7\u00e3o e produ\u00e7\u00e3o sejam id\u00eanticos.</p> <p>Com IaC, voc\u00ea utiliza o mesmo c\u00f3digo para provisionar todos os ambientes. Isso deve diminuir o uso da frase \"funciona na minha m\u00e1quina\"!</p>"},{"location":"classes/05-iac/intro/#escalabilidade-e-automacao","title":"Escalabilidade e Automa\u00e7\u00e3o","text":"<p>\u00c0 medida que suas necessidades de infraestrutura crescem, modificar manualmente dezenas ou centenas de recursos se torna invi\u00e1vel. A IaC permite escalar sua infraestrutura modificando algumas linhas de c\u00f3digo.</p>"},{"location":"classes/05-iac/intro/#recuperacao-de-desastres","title":"Recupera\u00e7\u00e3o de Desastres","text":"<p>Quando sua infraestrutura est\u00e1 definida em c\u00f3digo, recriar todo o ambiente ap\u00f3s um desastre se torna mais f\u00e1cil, por envolver menos etapas manuais, que podem demorar dias.</p> <p>Recupera\u00e7\u00e3o de desastres!</p> <p>Recupera\u00e7\u00e3o de desastres n\u00e3o \u00e9 tema principal de estudo deste curso. IaC pode facilitar este processo, mas n\u00e3o \u00e9 uma solu\u00e7\u00e3o m\u00e1gica, muito menos completa para este problema.</p> <p>Realidade dos Ambientes</p> <p>Na pr\u00e1tica, \u00e9 comum encontrar empresas onde os ambientes de desenvolvimento e produ\u00e7\u00e3o s\u00e3o completamente diferentes, criados manualmente ao longo do tempo.</p> <p>Isso gera inconsist\u00eancias que s\u00f3 s\u00e3o descobertas quando algo falha em produ\u00e7\u00e3o.</p>"},{"location":"classes/05-iac/intro/#documentacao-viva","title":"Documenta\u00e7\u00e3o Viva","text":"<p>O c\u00f3digo da infraestrutura serve como documenta\u00e7\u00e3o sempre atualizada de como o ambiente est\u00e1 configurado. Isso \u00e9 especialmente valioso para novos membros da equipe ou para auditoria de compliance.</p> <p>Benef\u00edcios da IaC</p> <p>Qual dos seguintes benef\u00edcios da Infrastructure as Code \u00e9 mais relevante para equipes que trabalham com m\u00faltiplos ambientes (desenvolvimento, teste, produ\u00e7\u00e3o)?</p> Redu\u00e7\u00e3o de custos de hardware Consist\u00eancia entre ambientes Aumento da velocidade de processamento Redu\u00e7\u00e3o do tamanho do c\u00f3digo Submit <p>Answer</p> <p>A consist\u00eancia entre ambientes \u00e9 o benef\u00edcio mais relevante, pois permite que a mesma defini\u00e7\u00e3o de infraestrutura seja aplicada em todos os ambientes, eliminando as diferen\u00e7as que causam problemas como \"funciona no desenvolvimento mas n\u00e3o na produ\u00e7\u00e3o\".</p>"},{"location":"classes/05-iac/intro/#tipos-de-ferramentas-de-iac","title":"Tipos de Ferramentas de IaC","text":"<p>As ferramentas de Infrastructure as Code podem ser categorizadas de diferentes formas. Uma classifica\u00e7\u00e3o \u00fatil \u00e9 entre ferramentas de configura\u00e7\u00e3o e ferramentas de orquestra\u00e7\u00e3o.</p>"},{"location":"classes/05-iac/intro/#ferramentas-de-configuracao","title":"Ferramentas de Configura\u00e7\u00e3o","text":"<p>Focam na configura\u00e7\u00e3o de servidores individuais, instalando software, configurando servi\u00e7os e gerenciando arquivos de configura\u00e7\u00e3o.</p> <p>Exemplos: Ansible, Puppet, Chef, SaltStack</p> <p>Essas ferramentas s\u00e3o ideais para configurar servidores depois que eles s\u00e3o provisionados, garantindo que o software correto esteja instalado e configurado adequadamente.</p>"},{"location":"classes/05-iac/intro/#ferramentas-de-orquestracao","title":"Ferramentas de Orquestra\u00e7\u00e3o","text":"<p>Focam no provisionamento de recursos de infraestrutura como servidores, redes, balanceadores de carga, bancos de dados e outros servi\u00e7os de nuvem.</p> <p>Exemplos: Terraform, CloudFormation (AWS), Azure Resource Manager</p> <p>Essas ferramentas definem quais recursos criar, como eles se conectam e suas configura\u00e7\u00f5es b\u00e1sicas.</p> <p>Complementaridade</p> <p>Na pr\u00e1tica, \u00e9 comum usar ferramentas de ambas as categorias em conjunto. Por exemplo, usar Terraform para provisionar a infraestrutura base e Ansible para configurar os aplicativos nos servidores criados.</p>"},{"location":"classes/05-iac/intro/#abordagens-declarativa-vs-imperativa","title":"Abordagens: Declarativa vs Imperativa","text":"<p>Declarativa: Voc\u00ea especifica o estado final desejado, e a ferramenta determina como chegar l\u00e1.</p> <pre><code># Exemplo declarativo (gen\u00e9rico)\nresources:\n  - name: web-server\n    type: compute.instance\n    count: 3\n    size: medium\n</code></pre> <p>Imperativa: Voc\u00ea especifica os passos exatos para criar a infraestrutura.</p> <pre><code># Exemplo imperativo (gen\u00e9rico)\ncreate_instance(\"web-server-1\", \"medium\")\ncreate_instance(\"web-server-2\", \"medium\") \ncreate_instance(\"web-server-3\", \"medium\")\n</code></pre> <p>Declarativa vs Imperativa</p> <p>Qual \u00e9 a principal vantagem da abordagem declarativa em Infrastructure as Code?</p> \u00c9 mais r\u00e1pida para executar Requer menos conhecimento t\u00e9cnico Permite idempot\u00eancia mais facilmente Usa menos recursos de sistema Submit <p>Answer</p> <p>A abordagem declarativa permite idempot\u00eancia mais facilmente porque voc\u00ea define o estado desejado e a ferramenta determina automaticamente se alguma altera\u00e7\u00e3o \u00e9 necess\u00e1ria. Se executar novamente com a mesma configura\u00e7\u00e3o, nenhuma mudan\u00e7a ser\u00e1 feita se o estado j\u00e1 estiver correto.</p>"},{"location":"classes/05-iac/intro/#iac-no-contexto-de-engenharia-de-dados","title":"IaC no Contexto de Engenharia de Dados","text":"<p>No contexto espec\u00edfico de engenharia de dados, a IaC se torna ainda mais cr\u00edtica devido \u00e0 complexidade e diversidade dos componentes envolvidos em um pipeline de dados moderno.</p> Um pipeline de dados t\u00edpico pode envolver: <ul> <li>Clusters de processamento distribu\u00eddo (como Spark ou Dask)</li> <li>Bancos de dados relacionais e NoSQL</li> <li>Data lakes e data warehouses</li> <li>Sistemas de mensageria (como RabbitMQ ou Kafka)</li> <li>Ferramentas de orquestra\u00e7\u00e3o (como Airflow)</li> <li>Servi\u00e7os de monitoramento e alertas</li> <li>Redes e configura\u00e7\u00f5es de seguran\u00e7a espec\u00edficas</li> </ul> <p>Info</p> <p>Parte destes compomentes j\u00e1 foram explorados no curso e parte ser\u00e1 explorada nas pr\u00f3ximas aulas!</p> <p>Gerenciar essa infraestrutura manualmente seria n\u00e3o apenas trabalhoso, mas tamb\u00e9m propenso a erros que podem comprometer a integridade dos dados.</p>"},{"location":"classes/05-iac/intro/#ambientes-de-dados","title":"Ambientes de Dados","text":"<p>Diferentemente de aplica\u00e7\u00f5es web tradicionais, pipelines de dados frequentemente precisam de ambientes com caracter\u00edsticas espec\u00edficas:</p> <ul> <li>Desenvolvimento: Processamento de volumes pequenos com dados sint\u00e9ticos ou mascarados</li> <li>Teste: Valida\u00e7\u00e3o com volumes m\u00e9dios e dados representativos</li> <li>Produ\u00e7\u00e3o: Processamento de volumes completos com alta disponibilidade</li> </ul> <p>Cada ambiente pode ter configura\u00e7\u00f5es diferentes de recursos (CPU, mem\u00f3ria, armazenamento), mas a estrutura b\u00e1sica deve ser consistente.</p> <p>Exercise</p> <p>Considerando o pipeline de sensores discutido nas aulas anteriores (sensores \u2192 RabbitMQ \u2192 processamento \u2192 armazenamento), liste pelo menos 5 componentes de infraestrutura que precisariam ser provisionados e configurados.</p> Submit <p>Answer</p> <ol> <li>Servidor/container para RabbitMQ com configura\u00e7\u00f5es de mem\u00f3ria e armazenamento adequadas</li> <li>Banco de dados para armazenar os dados processados dos sensores</li> <li>Servidor de aplica\u00e7\u00e3o para executar o c\u00f3digo de processamento</li> <li>Configura\u00e7\u00f5es de rede para permitir comunica\u00e7\u00e3o entre sensores e RabbitMQ</li> <li>Sistema de monitoramento para acompanhar a sa\u00fade da fila e do processamento</li> <li>Configura\u00e7\u00f5es de seguran\u00e7a como firewalls e controle de acesso</li> <li>Armazenamento persistente para dados hist\u00f3ricos e backup</li> </ol>"},{"location":"classes/05-iac/intro/#desafios-e-consideracoes","title":"Desafios e Considera\u00e7\u00f5es","text":"<p>Embora a Infrastructure as Code traga muitos benef\u00edcios, sua ado\u00e7\u00e3o tamb\u00e9m apresenta alguns desafios que devem ser considerados.</p>"},{"location":"classes/05-iac/intro/#curva-de-aprendizado","title":"Curva de Aprendizado","text":"<p>A transi\u00e7\u00e3o de gerenciamento manual para IaC requer aprendizado de novas ferramentas e conceitos. A equipe precisa se familiarizar com sintaxes espec\u00edficas, conceitos de estado e melhores pr\u00e1ticas de cada ferramenta.</p>"},{"location":"classes/05-iac/intro/#gerenciamento-de-estado","title":"Gerenciamento de Estado","text":"<p>Muitas ferramentas de IaC mant\u00eam um \"estado\" que representa o mapeamento entre a configura\u00e7\u00e3o declarada e os recursos reais. O gerenciamento inadequado deste estado pode causar problemas s\u00e9rios.</p>"},{"location":"classes/05-iac/intro/#seguranca","title":"Seguran\u00e7a","text":"<p>Arquivos de IaC frequentemente cont\u00eam informa\u00e7\u00f5es sens\u00edveis como credenciais e configura\u00e7\u00f5es de acesso. \u00c9 essencial implementar pr\u00e1ticas adequadas de seguran\u00e7a para proteger essas informa\u00e7\u00f5es.</p> <p>Cuidado com Credenciais</p> <p>Nunca inclua senhas, chaves de API ou outras credenciais diretamente nos arquivos de IaC. Use sempre sistemas de gerenciamento de secrets ou vari\u00e1veis de ambiente seguras.</p>"},{"location":"classes/05-iac/intro/#mudancas-destrutivas","title":"Mudan\u00e7as Destrutivas","text":"<p>Algumas altera\u00e7\u00f5es na configura\u00e7\u00e3o podem resultar na destrui\u00e7\u00e3o e recria\u00e7\u00e3o de recursos, causando perda de dados ou interrup\u00e7\u00e3o de servi\u00e7os.</p> <p>Aten\u00e7\u00e3o!</p> <p>\u00c9 fundamental entender as implica\u00e7\u00f5es de cada mudan\u00e7a antes de aplic\u00e1-la.</p> <p>Exercise</p> <p>Imagine que voc\u00ea precisa explicar para um colega por que n\u00e3o devemos incluir senhas diretamente nos arquivos de IaC.</p> <p>Quais argumentos voc\u00ea usaria?</p> Submit <p>Answer</p> <ol> <li>Versionamento: Os arquivos de IaC s\u00e3o versionados em sistemas como Git, o que significaria que as senhas ficariam registradas no hist\u00f3rico do reposit\u00f3rio</li> <li>Colabora\u00e7\u00e3o: Outros desenvolvedores teriam acesso \u00e0s credenciais ao visualizar o c\u00f3digo</li> <li>Seguran\u00e7a: Se o reposit\u00f3rio for comprometido, todas as credenciais seriam expostas</li> <li>Auditoria: Fica dif\u00edcil rastrear quem teve acesso a quais credenciais</li> <li>Rota\u00e7\u00e3o: Mudan\u00e7as de senhas exigiriam altera\u00e7\u00f5es no c\u00f3digo e redeployment</li> <li>Ambientes diferentes: Produ\u00e7\u00e3o e desenvolvimento devem usar credenciais diferentes</li> </ol>"},{"location":"classes/05-iac/intro/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Agora que voc\u00ea compreende os conceitos fundamentais de Infrastructure as Code, est\u00e1 preparado para explorar ferramentas espec\u00edficas e aplic\u00e1-las na pr\u00e1tica.</p> <p>Nas pr\u00f3ximas p\u00e1ginas, vamos explorar o Terraform como ferramenta de IaC.</p>"},{"location":"classes/05-iac/pratica/","title":"Pr\u00e1tica IaC","text":""},{"location":"classes/05-iac/pratica/#pratica-iac","title":"Pr\u00e1tica IaC","text":""},{"location":"classes/05-iac/pratica/#pratica-infrastructure-as-code-com-terraform","title":"Pr\u00e1tica: Infrastructure as Code com Terraform","text":""},{"location":"classes/05-iac/pratica/#fixar-conceito","title":"Fixar conceito","text":"<p>Infrastructure as Code (Infraestrutura como C\u00f3digo) \u00e9 uma abordagem que permite gerenciar e provisionar recursos de infraestrutura usando c\u00f3digo e arquivos de configura\u00e7\u00e3o, ao inv\u00e9s de processos manuais.</p> <p>Com IaC, voc\u00ea define sua infraestrutura em arquivos de texto que podem ser versionados, revisados e reutilizados. Isso traz benef\u00edcios como:</p> <ul> <li>Consist\u00eancia: A mesma configura\u00e7\u00e3o pode ser aplicada m\u00faltiplas vezes</li> <li>Versionamento: Mudan\u00e7as podem ser rastreadas usando Git</li> <li>Reutiliza\u00e7\u00e3o: Configura\u00e7\u00f5es podem ser compartilhadas entre projetos</li> <li>Automa\u00e7\u00e3o: Reduz erros humanos e acelera deployments</li> </ul> <p>Compara\u00e7\u00e3o com a aula anterior</p> <p>Na aula anterior, criamos uma VM manualmente pelo console da AWS e depois fizemos configura\u00e7\u00f5es via SSH.</p> <p>Com Terraform, faremos tudo isso atrav\u00e9s de c\u00f3digo!</p>"},{"location":"classes/05-iac/pratica/#terraform","title":"Terraform","text":"<p>O Terraform \u00e9 uma das ferramentas mais populares para IaC.</p> <p>Desenvolvido pela HashiCorp (2014), ele permite definir infraestrutura usando a linguagem HCL (HashiCorp Configuration Language).</p> <p>O Terraform suporta m\u00faltiplos provedores de nuvem (AWS, Azure, Google Cloud) e servi\u00e7os diversos, permitindo criar desde uma simples VM at\u00e9 arquiteturas complexas.</p> <p>Perigo!</p> <p>Se fizer esta aula pela metade, n\u00e3o deixe VMs em execu\u00e7\u00e3o.</p> <p>Confira, ao final do tutorial, o comando para destruir os recursos criados.</p> <p>Voc\u00ea tamb\u00e9m pode utilizar o console para conferir se sobrou algo (para destruir, prefira o terraform, n\u00e3o \u00e9 boa pr\u00e1tica utilizar o console uma vez que introduzimos IaC).</p> <p>Qualquer d\u00favida, entre em contato com o professor.</p> OpenTofu: Curiosidade! <p>H\u00e1 alguns anos, a HashiCorp decidiu mudar a licen\u00e7a do Terraform para um modelo mais restritivo, o que gerou preocupa\u00e7\u00e3o sobre a liberdade e sustentabilidade do projeto.</p> <p>Como resposta, a comunidade criou um fork aberto para manter a colabora\u00e7\u00e3o e garantir transpar\u00eancia no desenvolvimento.</p> <p>Inicialmente chamado de OpenTF e depois renomeado para OpenTofu, o projeto passou a ser mantido pela Linux Foundation, assegurando governan\u00e7a aberta e compatibilidade com o Terraform.</p> <p>Assim, tornou-se uma alternativa livre e confi\u00e1vel para usu\u00e1rios e empresas que dependem de IaC.</p>"},{"location":"classes/05-iac/pratica/#instalacao-do-terraform","title":"Instala\u00e7\u00e3o do Terraform","text":"<p>Exerc\u00edcio</p> <p>Instale a vers\u00e3o mais recente do Terraform seguindo as instru\u00e7\u00f5es para seu sistema operacional:</p> <p>Extra</p> <p>Caso necess\u00e1rio, acesse mais informa\u00e7\u00f5es diretamente no site do Terraform.</p> Linux (Ubuntu/Debian)macOSWindows <pre><code>$ wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\n$ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(grep -oP '(?&lt;=UBUNTU_CODENAME=).*' /etc/os-release || lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n$ sudo apt update &amp;&amp; sudo apt install terraform\n</code></pre> <pre><code>$ brew tap hashicorp/tap\n$ brew install hashicorp/tap/terraform\n</code></pre> <ol> <li>Baixe o bin\u00e1rio do Terraform em https://developer.hashicorp.com/terraform/install</li> <li>Extraia o arquivo ZIP em um diret\u00f3rio (ex: <code>C:\\terraform</code>)</li> <li>Adicione o diret\u00f3rio ao PATH do sistema Tutorial 1 Tutorial 2</li> </ol> Mark as done <p>Exerc\u00edcio</p> <p>Verifique se a instala\u00e7\u00e3o foi bem-sucedida:</p> <pre><code>$ terraform version\n</code></pre> <p>Voc\u00ea deve ver a vers\u00e3o do Terraform instalada.</p> Mark as done"},{"location":"classes/05-iac/pratica/#extensao-vscode","title":"Extens\u00e3o VSCode","text":"<p>Instale a extens\u00e3o Hashicorp Terraform no VS Code.</p>"},{"location":"classes/05-iac/pratica/#estrutura-do-projeto","title":"Estrutura do projeto","text":"<p>Vamos criar um projeto Terraform que reproduzir\u00e1 o deploy da API de filmes que fizemos manualmente na aula anterior.</p> <p>Exerc\u00edcio</p> <p>Crie um diret\u00f3rio para o projeto e entre nele:</p> <pre><code>$ mkdir movies-api-terraform\n$ cd movies-api-terraform\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>main.tf</code> com a configura\u00e7\u00e3o inicial do provedor AWS:</p> <pre><code>terraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = \"us-east-1\"\n  profile = \"dataeng\"\n}\n</code></pre> Mark as done"},{"location":"classes/05-iac/pratica/#inicializando-o-terraform","title":"Inicializando o Terraform","text":"<p>Exerc\u00edcio</p> <p>Garanta que voc\u00ea est\u00e1 com o perfil <code>dataeng</code> configurado e logado no AWS CLI.</p> <p>Para mais detalhes, acesse a aula passada</p> <p>Caso tenha o perfil configurado, fa\u00e7a login com:</p> <pre><code>$ aws sso login --profile dataeng\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Inicialize o diret\u00f3rio do Terraform:</p> <pre><code>$ terraform init\n</code></pre> <p>Esse comando baixa os plugins necess\u00e1rios para o provedor AWS.</p> Mark as done <p>Exercise</p> <p>O que aconteceu ap\u00f3s executar <code>terraform init</code>?</p> Submit <p>Answer</p> <p>O Terraform criou um diret\u00f3rio <code>.terraform/</code> e baixou o plugin do provedor AWS. Tamb\u00e9m criou um arquivo <code>.terraform.lock.hcl</code> para fixar as vers\u00f5es dos provedores.</p>"},{"location":"classes/05-iac/pratica/#criando-a-instancia-ec2","title":"Criando a inst\u00e2ncia EC2","text":"<p>Vamos come\u00e7ar criando apenas a inst\u00e2ncia EC2 com Terraform, equivalente ao que fizemos manualmente na aula anterior.</p> <p>Chave SSH</p> <p>O c\u00f3digo abaixo assume que voc\u00ea tem uma chave SSH em <code>~/.ssh/id_ed25519.pub</code>. Se n\u00e3o tiver, crie uma com:</p> Linux / Mac / Windows (Git Bash)Windows (Power Shell) <pre><code>$ ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519\n</code></pre> <pre><code>$ ssh-keygen -t ed25519 -f $env:USERPROFILE\\.ssh\\id_ed25519\n</code></pre> <p>Exerc\u00edcio</p> <p>Adicione ao final do arquivo <code>main.tf</code> a configura\u00e7\u00e3o da inst\u00e2ncia EC2:</p> <pre><code># Dados da AMI mais recente do Ubuntu\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"] # Canonical (Ubuntu)\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\"] # A mesma que utilizamos na aula passada!\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\n# Key Pair para acesso SSH\nresource \"aws_key_pair\" \"movies_api_key\" {\n  key_name   = \"movies-api-key\"\n  public_key = file(\"~/.ssh/id_ed25519.pub\") # Ajuste o caminho conforme necess\u00e1rio\n}\n\n# Inst\u00e2ncia EC2\nresource \"aws_instance\" \"movies_api\" {\n  ami             = data.aws_ami.ubuntu.id\n  instance_type   = \"t3a.micro\"\n  key_name        = aws_key_pair.movies_api_key.key_name\n  security_groups = [aws_security_group.movies_api_sg.name]\n\n  tags = {\n    Name = \"movies-api-server\"\n  }\n}\n</code></pre> Mark as done <p>Exercise</p> <p>Qual sistema operacional e vers\u00e3o est\u00e3o sendo utilizados na inst\u00e2ncia EC2?</p> Submit <p>Answer</p> <p>Ubuntu 24.04 LTS</p> <p>Exercise</p> <p>Qual ser\u00e1 a configura\u00e7\u00e3o (RAM e vCPU) da inst\u00e2ncia EC2?</p> Submit <p>Answer</p> <p><code>t3a.micro</code> (2 vCPU, 1 GB RAM).</p> <p>Veja mais em AWS EC2 Instance Types.</p> <p>Dica!</p> <p>Voc\u00ea pode conferir as imagens dispon\u00edveis da canonical com:</p> <p>Info!</p> <p><code>099720109477</code> \u00e9 o ID do dono oficial das imagens Ubuntu na AWS (canonical).</p> <pre><code>$ aws ec2 describe-images --owners 099720109477 --query 'Images[*].[ImageId,Name,CreationDate]' --output table --profile dataeng\n</code></pre> <p>Exerc\u00edcio</p> <p>Adicione tamb\u00e9m a configura\u00e7\u00e3o do Security Group:</p> <pre><code># Security Group\nresource \"aws_security_group\" \"movies_api_sg\" {\n  name_prefix = \"movies-api-sg\"\n\n  # SSH\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  # HTTP para a API (porta 8000)\n  ingress {\n    from_port   = 8000\n    to_port     = 8000\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  # Sa\u00edda para internet\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"movies-api-security-group\"\n  }\n}\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Adicione outputs para exibir informa\u00e7\u00f5es importantes:</p> <pre><code># Outputs\noutput \"instance_id\" {\n  description = \"ID da inst\u00e2ncia EC2\"\n  value       = aws_instance.movies_api.id\n}\n\noutput \"instance_public_ip\" {\n  description = \"IP p\u00fablico da inst\u00e2ncia\"\n  value       = aws_instance.movies_api.public_ip\n}\n\noutput \"instance_public_dns\" {\n  description = \"DNS p\u00fablico da inst\u00e2ncia\"\n  value       = aws_instance.movies_api.public_dns\n}\n\noutput \"ssh_connection\" {\n  description = \"Comando para conectar via SSH\"\n  value       = \"ssh ubuntu@${aws_instance.movies_api.public_ip} -i ~/.ssh/id_ed25519\"\n}\n</code></pre> Mark as done <p>Acesso SSH</p> <p>Perceba que, na aula passada, utilizamos um arquivo <code>.pem</code> para obter acesso \u00e0 VM.</p> <p>Nesta aula, a chave da sua m\u00e1quina \u00e9 um arquivo <code>id_ed25519</code> e estar\u00e1 autorizada a acessar a inst\u00e2ncia.</p>"},{"location":"classes/05-iac/pratica/#planejamento-e-aplicacao","title":"Planejamento e aplica\u00e7\u00e3o","text":"<p>O comando  <code>terraform plan</code> \u00e9 usado no Terraform para gerar e exibir um plano de execu\u00e7\u00e3o, mostrando quais mudan\u00e7as ser\u00e3o feitas na infraestrutura antes de aplic\u00e1-las.</p> <p>Ele compara o estado atual dos recursos (registrado no state file e no provedor) com a configura\u00e7\u00e3o descrita nos arquivos <code>.tf</code> e indica o que ser\u00e1 criado, alterado ou destru\u00eddo.</p> <p>Info</p> <p>O <code>terraform plan</code> serve como uma pr\u00e9via segura para validar se as altera\u00e7\u00f5es desejadas est\u00e3o corretas, permitindo revisar e evitar modifica\u00e7\u00f5es indesejadas na infraestrutura antes de executar o <code>terraform apply</code>.</p> <p>Exerc\u00edcio</p> <p>Execute o planejamento para ver o que ser\u00e1 criado:</p> <pre><code>$ terraform plan\n</code></pre> <p>Analise a sa\u00edda. O que o Terraform pretende criar?</p> Submit <p>Answer</p> <p>Deve retornar algo contendo, dentre outras informa\u00e7\u00f5es:</p> <pre><code># aws_instance.movies_api will be created\n...\n# aws_key_pair.movies_api_key will be created\n...\n# aws_security_group.movies_api_sg will be created\n...\nPlan: 3 to add, 0 to change, 0 to destroy.\n...\n</code></pre> <p>Exerc\u00edcio</p> <p>Aplique a configura\u00e7\u00e3o para criar os recursos:</p> <pre><code>$ terraform apply\n</code></pre> <p>Digite <code>yes</code> quando solicitado.</p> Mark as done <p>Exerc\u00edcio</p> <p>Abra o console da AWS e verifique, no painel da EC2, se a inst\u00e2ncia foi criada.</p> <p>Quantas inst\u00e2ncias voc\u00ea v\u00ea?</p> Submit <p>Answer</p> <p>Duas!</p> <ul> <li>Uma da aula passada (desligada)</li> <li>Uma que acabou de ser criada (ligada)</li> </ul> <p>Exerc\u00edcio</p> <p>Teste o acesso SSH \u00e0 inst\u00e2ncia criada:</p> <p>Aten\u00e7\u00e3o!</p> <p>Substitua <code>IP_PUBLICO</code> pelo IP p\u00fablico mostrado no output do Terraform.</p> <pre><code>$ ssh ubuntu@IP_PUBLICO -i ~/.ssh/id_ed25519\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Na VM, rode o comando:</p> <pre><code>$ cat ~/.ssh/authorized_keys\n</code></pre> <p> </p> <p>Informa\u00e7\u00e3o</p> <p>O conte\u00fado do arquivo <code>~/.ssh/authorized_keys</code> deve incluir a chave p\u00fablica correspondente ao arquivo <code>id_ed25519</code> da sua m\u00e1quina!</p> <p>Ou seja, esta linha que permite voc\u00ea realize SSH no servidor!</p> <p>Cada computador autorizado a realizar SSH no servidor ter\u00e1 uma chave p\u00fablica listada nesse arquivo (uma por linha).</p> Mark as done <p>Exercise</p> <p>Compare o processo de cria\u00e7\u00e3o da VM com Terraform versus a cria\u00e7\u00e3o manual da aula anterior. Quais s\u00e3o as principais diferen\u00e7as?</p> Submit <p>Answer</p> <ul> <li>Velocidade: Terraform cria todos os recursos de uma vez</li> <li>Reprodutibilidade: O mesmo c\u00f3digo pode ser executado v\u00e1rias vezes</li> <li>Documenta\u00e7\u00e3o: A infraestrutura fica documentada no c\u00f3digo</li> <li>Versionamento: Mudan\u00e7as podem ser rastreadas no Git</li> <li>Consist\u00eancia: Reduz erros de configura\u00e7\u00e3o manual</li> </ul>"},{"location":"classes/05-iac/pratica/#automatizando-a-configuracao-da-aplicacao","title":"Automatizando a configura\u00e7\u00e3o da aplica\u00e7\u00e3o","text":"<p>Agora vamos estender nosso Terraform para instalar e configurar a aplica\u00e7\u00e3o automaticamente, eliminando as etapas manuais via SSH.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>user_data.sh</code> com o script de inicializa\u00e7\u00e3o:</p> <p>info</p> <p>Este ser\u00e1 o script de inicializa\u00e7\u00e3o a ser executado quando a VM for criada.</p> <pre><code>#!/bin/bash\nset -euxo pipefail\n\n# Atualizar sistema\napt-get update\napt-get upgrade -y\napt-get install -y git curl build-essential python3-pip\n\n# Criar usu\u00e1rio\nsudo useradd --system --create-home --home-dir /srv/movies-api --shell /usr/sbin/nologin uapi\nsudo mkdir -p /srv/movies-api/app\nsudo chown -R uapi:uapi /srv/movies-api/app\n\n# Clonar Repositorio API\nsudo -u uapi git clone https://github.com/macielcalebe/movies-api-example-01.git /srv/movies-api/app\n\n# Instalar uv system-wide\ncurl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR=/usr/local/bin sh\n/usr/local/bin/uv --version\n\n# Criar ambiente virtual Python 3.12 com uv\nsudo -u uapi /usr/local/bin/uv venv --python 3.12 /srv/movies-api/app/.venv\n\n# Instalar depend\u00eancias\nif [ -f /srv/movies-api/app/requirements.txt ]; then\n    sudo -u uapi /usr/local/bin/uv pip install -r /srv/movies-api/app/requirements.txt --python /srv/movies-api/app/.venv/bin/python\nfi\n\n# Criar arquivo de servi\u00e7o systemd\ncat &gt; /etc/systemd/system/movies-api.service &lt;&lt; 'EOF'\n[Unit]\nDescription=Movies API\nAfter=network.target\n\n[Service]\nType=simple\nUser=uapi\nWorkingDirectory=/srv/movies-api/app\nEnvironment=PATH=/srv/movies-api/app/.venv/bin\nExecStart=/srv/movies-api/app/.venv/bin/fastapi run src/main.py --host 0.0.0.0 --port 8000\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n# Iniciar o servi\u00e7o\nsystemctl daemon-reload\nsystemctl enable movies-api\nsystemctl start movies-api\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>O que \u00e9 <code>set -euxo pipefail</code>? Pesquise sobre!</p> Submit <p>Exerc\u00edcio</p> <p>Modifique o recurso <code>aws_instance</code> no arquivo <code>main.tf</code> para incluir o script user data:</p> <p>Warning</p> <p>A \u00fanica altera\u00e7\u00e3o ser\u00e1 a adi\u00e7\u00e3o da linha: <pre><code>user_data = file(\"user_data.sh\")\n</code></pre></p> <p>Dever\u00e1 ficar assim:</p> <p>Aten\u00e7\u00e3o!</p> <p>Perceba que liberamos o acesso \u00e0 porta <code>22</code> (SSH) de qualquer IP.</p> <p>Por seguran\u00e7a, n\u00e3o \u00e9 uma boa pr\u00e1tica fazer isto em ambientes de produ\u00e7\u00e3o.</p> <pre><code>resource \"aws_instance\" \"movies_api\" {\n  ami             = data.aws_ami.ubuntu.id\n  instance_type   = \"t3a.micro\"\n  key_name        = aws_key_pair.movies_api_key.key_name\n  security_groups = [aws_security_group.movies_api_sg.name]\n\n  # Script de inicializa\u00e7\u00e3o\n  user_data = file(\"user_data.sh\")\n\n  tags = {\n    Name = \"movies-api-server\"\n  }\n}\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Ao final do arquivo, adicione um output para a URL da API:</p> <pre><code>output \"api_url\" {\n  description = \"URL da API de filmes\"\n  value       = \"http://${aws_instance.movies_api.public_ip}:8000\"\n}\n\noutput \"api_docs_url\" {\n  description = \"URL da documenta\u00e7\u00e3o da API\"\n  value       = \"http://${aws_instance.movies_api.public_ip}:8000/docs\"\n}\n</code></pre> Mark as done"},{"location":"classes/05-iac/pratica/#testando-a-infraestrutura-completa","title":"Testando a infraestrutura completa","text":"<p>Exerc\u00edcio</p> <p>Como fizemos mudan\u00e7as na configura\u00e7\u00e3o, vamos recriar a inst\u00e2ncia:</p> <p>Responda <code>yes</code></p> <p>Responda 'yes` para confirmar a destrui\u00e7\u00e3o da inst\u00e2ncia e para sua recria\u00e7\u00e3o!</p> <pre><code>$ terraform destroy\n$ terraform apply\n</code></pre> <p>Digite <code>yes</code> quando solicitado para ambos comandos.</p> Mark as done <p>Dica!</p> <p>Voc\u00ea pode conferir o log de execu\u00e7\u00e3o do script <code>user_data.sh</code> em <code>/var/log/cloud-init-output.log</code>.</p> <p>Fa\u00e7a SSH na VM e rode:</p> <pre><code>$ cat /var/log/cloud-init-output.log\n</code></pre> <p>Exerc\u00edcio</p> <p>Aguarde alguns minutos (3-5 minutos) para que o script de inicializa\u00e7\u00e3o seja executado completamente.</p> <p>Teste a API nos endpoints: - <code>http://&lt;IP_PUBLICO&gt;:8000</code> - <code>http://&lt;IP_PUBLICO&gt;:8000/docs</code> - <code>http://&lt;IP_PUBLICO&gt;:8000/filmes/avatar</code></p> Mark as done <p>Exerc\u00edcio</p> <p>Verifique se o servi\u00e7o est\u00e1 rodando corretamente conectando via SSH:</p> <pre><code>$ ssh ubuntu@IP_PUBLICO -i ~/.ssh/id_ed25519\n$ sudo systemctl status movies-api\n$ sudo journalctl -u movies-api -f\n</code></pre> Mark as done <p>Exercise</p> <p>Quais s\u00e3o as vantagens de usar user data versus configurar tudo manualmente via SSH?</p> Submit <p>Answer</p> <ul> <li>Automa\u00e7\u00e3o completa: A inst\u00e2ncia fica pronta sem interven\u00e7\u00e3o manual</li> <li>Reprodutibilidade: Sempre teremos a mesma configura\u00e7\u00e3o</li> <li>Escalabilidade: Facilita a cria\u00e7\u00e3o de m\u00faltiplas inst\u00e2ncias</li> <li>Documenta\u00e7\u00e3o: Todo processo fica documentado no c\u00f3digo</li> <li>Redu\u00e7\u00e3o de erros: Elimina passos manuais propensos a erro</li> </ul>"},{"location":"classes/05-iac/pratica/#organizando-o-codigo","title":"Organizando o c\u00f3digo","text":"<p>Para projetos maiores, \u00e9 uma boa pr\u00e1tica organizar o c\u00f3digo Terraform em m\u00faltiplos arquivos.</p> <p>Exerc\u00edcio</p> <p>Vamos conferir o status atual!</p> <p>Execute:</p> <pre><code>$ terraform plan\n</code></pre> <p>Como n\u00e3o realizamos nenhuma modifica\u00e7\u00e3o nos arquivos de configura\u00e7\u00e3o, o esperado \u00e9 que obtenhamos:</p> <pre><code>No changes. Your infrastructure matches the configuration.\n</code></pre> <p>Isto indica que n\u00e3o existem mudan\u00e7as a serem aplicadas.</p> Mark as done <p>Assim, prosseguimos para a reorganiza\u00e7\u00e3o do c\u00f3digo!</p> <p>Exerc\u00edcio</p> <p>Reorganize o c\u00f3digo criando os arquivos:</p> <p><code>variables.tf</code>: <pre><code>variable \"region\" {\n  description = \"Regi\u00e3o AWS\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"instance_type\" {\n  description = \"Tipo da inst\u00e2ncia EC2\"\n  type        = string\n  default     = \"t3a.micro\"\n}\n\nvariable \"key_name\" {\n  description = \"Nome da chave SSH\"\n  type        = string\n  default     = \"movies-api-key\"\n}\n\nvariable \"ssh_public_key_path\" {\n  description = \"Caminho para a chave p\u00fablica SSH\"\n  type        = string\n  default     = \"~/.ssh/id_ed25519.pub\"\n}\n</code></pre></p> <p><code>outputs.tf</code>: <pre><code>output \"instance_id\" {\n  description = \"ID da inst\u00e2ncia EC2\"\n  value       = aws_instance.movies_api.id\n}\n\noutput \"instance_public_ip\" {\n  description = \"IP p\u00fablico da inst\u00e2ncia\"\n  value       = aws_instance.movies_api.public_ip\n}\n\noutput \"api_url\" {\n  description = \"URL da API de filmes\"\n  value       = \"http://${aws_instance.movies_api.public_ip}:8000\"\n}\n\noutput \"ssh_connection\" {\n  description = \"Comando para conectar via SSH\"\n  value       = \"ssh ubuntu@${aws_instance.movies_api.public_ip} -i ~/.ssh/id_ed25519\"\n}\n</code></pre></p> Mark as done <p>Exerc\u00edcio</p> <p>Atualize o <code>main.tf</code> para usar as vari\u00e1veis:</p> <pre><code>terraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region  = var.region\n  profile = \"dataeng\"\n}\n\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-*\"] # A mesma que utilizamos na aula passada!\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\nresource \"aws_key_pair\" \"movies_api_key\" {\n  key_name   = var.key_name\n  public_key = file(var.ssh_public_key_path)\n}\n\nresource \"aws_security_group\" \"movies_api_sg\" {\n  name_prefix = \"movies-api-sg\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 8000\n    to_port     = 8000\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"movies-api-security-group\"\n  }\n}\n\nresource \"aws_instance\" \"movies_api\" {\n  ami             = data.aws_ami.ubuntu.id\n  instance_type   = var.instance_type\n  key_name        = aws_key_pair.movies_api_key.key_name\n  security_groups = [aws_security_group.movies_api_sg.name]\n  user_data       = file(\"user_data.sh\")\n\n  tags = {\n    Name = \"movies-api-server\"\n  }\n}\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Teste a reorganiza\u00e7\u00e3o:</p> <pre><code>$ terraform plan\n</code></pre> <p>Se n\u00e3o houver mudan\u00e7as planejadas na infra (apenas nos outputs), a reorganiza\u00e7\u00e3o foi bem-sucedida!</p> <p>Pode aplicar!</p> Mark as done"},{"location":"classes/05-iac/pratica/#limpeza-dos-recursos","title":"Limpeza dos recursos","text":"<p>Importante!</p> <p>Sempre remova recursos que n\u00e3o est\u00e3o em uso para evitar custos desnecess\u00e1rios.</p> <p>Exerc\u00edcio</p> <p>Destrua todos os recursos criados:</p> <pre><code>$ terraform destroy\n</code></pre> <p>Digite <code>yes</code> quando solicitado.</p> Mark as done <p>Exerc\u00edcio</p> <p>Confirme que todos os recursos foram removidos:</p> <pre><code>$ aws ec2 describe-instances --query 'Reservations[*].Instances[*].{ID:InstanceId,State:State.Name}' --output table --profile dataeng\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/intro-cron/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/06-intro-orchestration/intro-cron/#introducao","title":"Introdu\u00e7\u00e3o","text":""},{"location":"classes/06-intro-orchestration/intro-cron/#agendamento","title":"Agendamento","text":"<p>Na aula 03 discutimos sobre processos de ETL / ELT.</p> <p>No exemplo da aula, implementamos uma pipeline de dados que extrai informa\u00e7\u00f5es de um banco de dados PostgreSQL, transforma esses dados e os carrega em um data warehouse Clickhouse.</p> <p>Exerc\u00edcio</p> <p>De quanto em quanto tempo a tarefa de ETL era executada?</p> Submit <p>Answer</p> <p>A cada dois minutos.</p> <p>Um tempo baixo para que consegu\u00edssemos simular uma rotina di\u00e1ria sem esperar tanto.</p> <p>Exerc\u00edcio</p> <p>Como isto foi implementado?</p> Submit <p>Answer</p> <p>De forma rudimentar, esperando <code>120</code> segundos em um loop infinito.</p> <pre><code># Recorte de parte do docker-compose.yml\ncommand: &gt;\n    bash -c \"\n        pip install -r /app/requirements.txt &amp;&amp;\n        echo 'Inicializando banco de dados warehouse...' &amp;&amp;\n        python src/init_database.py &amp;&amp;\n        echo 'Iniciando ETL (executar\u00e1 a cada 2 minutos)...' &amp;&amp;\n        while true; do\n        echo 'Executando ETL...' &amp;&amp;\n        python src/etl_vendas.py &amp;&amp;\n        echo 'ETL conclu\u00eddo. Aguardando 2 minutos...' &amp;&amp;\n        sleep 120\n        done\n    \"\n</code></pre> <p>Exerc\u00edcio</p> <p>V\u00ea algum problema na forma como fizemos?</p> Submit <p>Answer</p> <p>A query de SQL utilizada para extrair os dados extra\u00eda informa\u00e7\u00f5es desde o segundo zero do minuto atual. Entretanto, caso o processo de ETL demorasse mais que um minuto, parte dos dados n\u00e3o seriam ingeridos/tratados.</p> <p>Ainda, o m\u00e1ximo que obt\u00ednhamos de controle era o tempo de espera. N\u00e3o havia como, por exemplo, executar o processo de ETL \u00e0s 3 da manh\u00e3 todos os dias.</p>"},{"location":"classes/06-intro-orchestration/intro-cron/#simulando-a-atividade","title":"Simulando a atividade","text":"<p>Vamos simular a execu\u00e7\u00e3o de um processo que precisa ser agendado. Para isto, vamos partir dos seguintes arquivos:</p> <p>Exercise</p> <p>Antes, crie um pasta para esta parte da aula.</p> <p>Os pr\u00f3ximos arquivos devem ser criados nesta pasta.</p> <pre><code>$ mkdir -p 06-intro-orchestration/01-intro\n$ cd 06-intro-orchestration/01-intro\n</code></pre> Mark as done <p>Agora considere os arquivos:</p> <code>app.py</code> <pre><code>import datetime\n\ndef main():\n    print(f\"Ol\u00e1 do Docker! A hora atual \u00e9: {datetime.datetime.now()}\", flush=True)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <code>Dockerfile</code> <pre><code># Use uma imagem base oficial do Python\nFROM python:3.12-slim\n\n# Defina o diret\u00f3rio de trabalho no container\nWORKDIR /app\n\n# Copie o script Python para o diret\u00f3rio de trabalho\nCOPY app.py .\n\n# Execute o script Python quando o container for iniciado\nCMD [\"python\", \"app.py\"]\n</code></pre> <code>docker-compose.yml</code> <pre><code>services:\n    python-app:\n        build: .\n        container_name: python-scheduler\n        restart: always\n</code></pre> <p>Exercise</p> <p>Crie cada um dos arquivos na pasta <code>06-intro-orchestration/01-intro</code>.</p> Mark as done <p>Exercise</p> <p>Fa\u00e7a a inicializa\u00e7\u00e3o com:</p> <pre><code>$ docker compose build\n$ docker compose up\n</code></pre> <p>De quanto em quanto tempo a tarefa \u00e9 executada (<code>print</code>)?</p> Submit <p>Answer</p> <p>Assim que poss\u00edvel!</p> <p>Ap\u00f3s a execu\u00e7\u00e3o, o processo encerra. Ent\u00e3o, instantaneamente o container \u00e9 reiniciado e ocorre novo <code>print</code>.</p> <p>Exercise</p> <p>Altere o arquivo <code>app.py</code> para que o <code>print</code> ocorra a cada <code>5</code> segundos.</p> <p>Pode ser com <code>time.sleep(5)</code>!</p> Mark as done <p>Answer</p> <pre><code>import datetime\nimport time\n\ndef main():\n    while True:\n        print(f\"Ol\u00e1 do Docker! A hora atual \u00e9: {datetime.datetime.now()}\", flush=True)\n        time.sleep(5)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Anteriormente, j\u00e1 discutimos por que o <code>time.sleep</code> n\u00e3o \u00e9 uma boa solu\u00e7\u00e3o. Vamos buscar uma alternativa melhor!</p>"},{"location":"classes/06-intro-orchestration/intro-cron/#cron","title":"Cron","text":"<p>O cron \u00e9 um agendador de tarefas do Unix que permite executar scripts ou comandos em intervalos regulares.</p> <p>Ele opera em segundo plano como um daemon, verificando a cada minuto um arquivo de configura\u00e7\u00e3o chamado <code>crontab</code> (cron table) para saber quais tarefas devem ser executadas.</p> <p>Daemon</p> <p>Um daemon \u00e9 um tipo especial de programa que roda em segundo plano em sistemas Unix e semelhantes.</p> <p>Ele geralmente inicia junto com o sistema e fica aguardando por eventos ou condi\u00e7\u00f5es espec\u00edficas para executar suas tarefas (Ex: <code>sshd</code>).</p> <p>As tarefas s\u00e3o definidas usando uma sintaxe de cinco campos para especificar o minuto, a hora, o dia do m\u00eas, o m\u00eas e o dia da semana em que o comando deve rodar:</p> <pre><code>* * * * *\n| | | | |\n| | | | ----- Dia da semana (0 - 7, onde 0 ou 7 \u00e9 Domingo)\n| | | ------- M\u00eas (1 - 12)\n| | --------- Dia do m\u00eas (1 - 31)\n| ----------- Hora (0 - 23)\n------------- Minuto (0 - 59)\n</code></pre> <p>Info</p> <ul> <li>Um asterisco (<code>*</code>) em um campo significa \"todos os valores poss\u00edveis\" para aquele campo.</li> <li>Valores espec\u00edficos podem ser listados, separados por v\u00edrgulas (ex: <code>1,15</code> no campo de minutos significa \"no minuto 1 e no minuto 15\").</li> <li>Intervalos podem ser especificados com um h\u00edfen (ex: <code>1-5</code> no campo de horas significa \"da 1h \u00e0s 5h\").</li> <li>Passos podem ser definidos com uma barra (ex: <code>*/10</code> no campo de minutos significa \"a cada 10 minutos\").</li> </ul> <p>Isso torna o cron extremamente \u00fatil para tarefas rotineiras, como backups de banco de dados ou limpeza de arquivos tempor\u00e1rios.</p> <p>Exercise</p> <p>Antes de continuar, encerre a execu\u00e7\u00e3o dos containers e remova-os.</p> <pre><code>$ docker compose down\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/intro-cron/#atualizacao-para-o-uso-do-cron","title":"Atualiza\u00e7\u00e3o para o uso do cron","text":"<p>Vamos utilizar o cron para agendar a execu\u00e7\u00e3o do nossa tarefa (por enquanto, apenas um <code>print</code>).</p> <p>Para isto, considere os arquivos:</p> <p>Exercise</p> <p>Crie uma nova pasta <code>06-intro-orchestration/02-cron</code> e copie os arquivos na sequ\u00eancia:</p> Mark as done <code>app.py</code> <p>Ser\u00e1 igual ao primeiro arquivo <code>app.py</code> proposto nesta aula. <pre><code>    import datetime\n\n    def main():\n        print(f\"Ol\u00e1 do Docker! A hora atual \u00e9: {datetime.datetime.now()}\", flush=True)\n\n    if __name__ == \"__main__\":\n        main()\n</code></pre></p> <code>crontab.txt</code> <pre><code>* * * * * /usr/local/bin/python3 /app/app.py &gt;&gt; /var/log/cron.log 2&gt;&amp;1\n</code></pre> <p>Info</p> <p>A linha acima agenda a execu\u00e7\u00e3o do <code>app.py</code> a cada minuto (de cada hora, de cada dia, de cada m\u00eas, etc.).</p> <p>A sa\u00edda padr\u00e3o e a sa\u00edda de erro s\u00e3o redirecionadas para o arquivo <code>/var/log/cron.log</code>.</p> <p>Aten\u00e7\u00e3o!</p> <p>Deixe uma linha em branco no final do arquivo <code>crontab.txt</code>. O cron exige isto.</p> <code>Dockerfile</code> <p>Atualizaremos para uso do <code>cron</code>: <pre><code>FROM python:3.12-slim\n\n# Instala o cron\nRUN apt-get update &amp;&amp; apt-get install -y cron &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\nCOPY app.py .\nCOPY crontab.txt .\n\n# Adiciona o crontab ao cron do sistema\nRUN crontab crontab.txt\n\n# Cria um arquivo de log para o cron\nRUN touch /var/log/cron.log\n\nCMD [\"cron\", \"-f\"]\n</code></pre></p> <code>docker-compose.yml</code> <p>De importante, apenas a atualiza\u00e7\u00e3o no nome do container! <pre><code>services:\npython-app:\n    build: .\n    environment:\n    - PYTHONUNBUFFERED=1\n    container_name: python-scheduler-cron\n    restart: always\n</code></pre></p> <p>Exercise</p> <p>Fa\u00e7a a reinicializa\u00e7\u00e3o com:</p> <p>Aten\u00e7\u00e3o!</p> <p>Garanta que esteja no diret\u00f3rio <code>06-intro-orchestration/02-cron</code>!</p> <pre><code>$ docker compose build\n$ docker compose up\n</code></pre> Mark as done <p>Se aguardar um minuto ou dois, ir\u00e1 perceber que n\u00e3o ter\u00e1 nenhum feedback. Isto ocorre porque os logs foram redirecionados para o arquivo <code>/var/log/cron.log</code> (dentro do container).</p> <p>Exercise</p> <p>Vamos inspecionar os logs.</p> <p>Com o o container em execu\u00e7\u00e3o, abra outro terminal e execute:</p> <pre><code>$ docker exec python-scheduler-cron cat /var/log/cron.log\n</code></pre> <p>Aproveite e explique o que este comando faz!</p> Submit <p>Answer</p> <p>O comando <code>docker exec python-scheduler-cron cat /var/log/cron.log</code> \u00e9 utilizado para acessar o container em execu\u00e7\u00e3o chamado <code>python-scheduler-cron</code> e executar o comando <code>cat /var/log/cron.log</code> dentro dele.</p> <p>Isso permite visualizar o conte\u00fado do arquivo de log do cron, onde est\u00e3o registrados os resultados das execu\u00e7\u00f5es agendadas.</p> <p>Exercise</p> <p>Funcionou? A tarefa est\u00e1 sendo executada a cada minuto?</p> Sim N\u00e3o Submit <p>Exercise</p> <p>O que voc\u00ea acha de utilizar o <code>cron</code> para agendar tarefas em produ\u00e7\u00e3o?</p> <p>Analise, de forma cr\u00edtica, com foco em engenharia de dados.</p> Submit <p>Answer</p> <p>O cron \u00e9 um agendador simples.</p> <p>Suas limita\u00e7\u00f5es incluem:</p> <ul> <li>Sem depend\u00eancias: N\u00e3o consegue gerenciar depend\u00eancias entre tarefas</li> <li>Sem estado: N\u00e3o rastreia se tarefas anteriores falharam ou tiveram sucesso</li> <li>Sem retry: N\u00e3o tem mecanismos autom\u00e1ticos de reexecu\u00e7\u00e3o em caso de falha</li> <li>Sem paraleliza\u00e7\u00e3o inteligente: Executa tarefas de forma isolada</li> <li>Monitoramento limitado: Logging b\u00e1sico, sem dashboards ou alertas</li> <li>Sem fluxo condicional: N\u00e3o pode executar tarefas baseadas em resultados de outras</li> <li>Sem gest\u00e3o de recursos: N\u00e3o controla uso de CPU, mem\u00f3ria ou concorr\u00eancia</li> </ul> <p>Com o <code>cron</code>, tudo isto precisaria ser implementado pelo script iniciado ou por ferramentas externas.</p> <p>Se quisermos um controle mais avan\u00e7ado sobre a execu\u00e7\u00e3o das tarefas, precisamos considerar outras solu\u00e7\u00f5es.</p> <p>Em ambientes de produ\u00e7\u00e3o, o uso do cron pode n\u00e3o ser a melhor escolha devido \u00e0s suas limita\u00e7\u00f5es. \u00c9 importante avaliar outras op\u00e7\u00f5es que ofere\u00e7am mais robustez e recursos para o agendamento de tarefas.</p> <p>Vamos discutir algumas alternativas na pr\u00f3xima p\u00e1gina!</p> <p>Exercise</p> <p>Antes de continuar, encerre a execu\u00e7\u00e3o dos containers e remova-os.</p> <pre><code>$ docker compose down\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/intro-orchestration/","title":"Orquestra\u00e7\u00e3o","text":""},{"location":"classes/06-intro-orchestration/intro-orchestration/#orquestracao","title":"Orquestra\u00e7\u00e3o","text":""},{"location":"classes/06-intro-orchestration/intro-orchestration/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Na p\u00e1gina anterior exploramos como o cron pode ser utilizado para agendar tarefas, mas tamb\u00e9m identificamos suas limita\u00e7\u00f5es para ambientes de produ\u00e7\u00e3o em engenharia de dados.</p> <p>Quando lidamos com pipelines de dados complexos, precisamos de algo mais robusto que simples agendamento temporal. Imagine um cen\u00e1rio onde voc\u00ea precisa:</p> <ul> <li>Processar dados de vendas ap\u00f3s o backup do banco estar completo</li> <li>Executar transforma\u00e7\u00f5es apenas quando novos arquivos chegarem</li> <li>Reprocessar dados caso uma etapa falhe</li> <li>Monitorar o status de cada etapa do pipeline</li> </ul> <p>Para atender a essas necessidades, surgiu o conceito de orquestra\u00e7\u00e3o de dados.</p> <p>Exerc\u00edcio</p> <p>Com base na sua experi\u00eancia nas aulas anteriores, quais outros problemas voc\u00ea identifica ao usar apenas cron para pipelines de dados complexos?</p> Submit <p>Answer</p> <p>Al\u00e9m dos problemas j\u00e1 mencionados na aula do cron, outros desafios incluem:</p> <ul> <li>Gerenciamento de depend\u00eancias: Como garantir que a tarefa A execute antes da tarefa B (SisHard entrou no chat!)?</li> <li>Tratamento de falhas: O que fazer quando uma etapa falha no meio do pipeline?</li> <li>Monitoramento: Como saber se o pipeline est\u00e1 funcionando corretamente?</li> <li>Versionamento: Como gerenciar diferentes vers\u00f5es do pipeline?</li> <li>Teste: Como testar pipelines complexos antes de coloc\u00e1-los em produ\u00e7\u00e3o?</li> </ul>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#o-que-e-orquestracao-de-dados","title":"O que \u00e9 Orquestra\u00e7\u00e3o de Dados?","text":"<p>A orquestra\u00e7\u00e3o de dados \u00e9 o processo de coordenar e gerenciar automaticamente a execu\u00e7\u00e3o de m\u00faltiplas tarefas e fluxos de trabalho de dados, garantindo que sejam executados na ordem correta, com as depend\u00eancias adequadas e com tratamento robusto de erros.</p> <p>Diferentemente do simples agendamento, a orquestra\u00e7\u00e3o considera:</p> <ul> <li> <p>Depend\u00eancias Entre Tarefas: As tarefas podem depender umas das outras. Por exemplo, um processo de transforma\u00e7\u00e3o s\u00f3 deve iniciar ap\u00f3s a extra\u00e7\u00e3o dos dados estar completa.</p> </li> <li> <p>Gerenciamento de Estado: O orquestrador mant\u00e9m o estado de cada tarefa, sabendo quais foram executadas com sucesso, quais falharam e quais est\u00e3o em execu\u00e7\u00e3o.</p> </li> <li> <p>Tratamento de Falhas: Quando uma tarefa falha, o sistema pode automaticamente retentar, alertar os respons\u00e1veis ou parar o pipeline dependendo da configura\u00e7\u00e3o.</p> </li> <li> <p>Monitoramento e Observabilidade: Fornece visibilidade completa sobre o status do pipeline, m\u00e9tricas de performance e logs detalhados.</p> </li> </ul> <p>Exemplo de Pipeline Orquestrado</p> <p>Considere um pipeline de an\u00e1lise de vendas:</p> <ol> <li>Extra\u00e7\u00e3o: Buscar dados do PostgreSQL</li> <li>Valida\u00e7\u00e3o: Verificar qualidade dos dados extra\u00eddos</li> <li>Transforma\u00e7\u00e3o: Agregar vendas por regi\u00e3o e per\u00edodo</li> <li>Carga: Inserir dados no data warehouse</li> <li>Notifica\u00e7\u00e3o: Enviar email confirmando a atualiza\u00e7\u00e3o</li> </ol> <p>Cada etapa depende da anterior e tem tratamento espec\u00edfico de erro.</p> <p>Orquestra\u00e7\u00e3o</p> <p>Em ess\u00eancia, a orquestra\u00e7\u00e3o \u00e9 como o maestro de uma orquestra de dados: ele n\u00e3o toca nenhum instrumento diretamente, mas coordena cada m\u00fasico (cada job de dados) para que toquem na ordem certa!</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#conceitos-fundamentais","title":"Conceitos Fundamentais","text":""},{"location":"classes/06-intro-orchestration/intro-orchestration/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"<p>Um DAG \u00e9 a representa\u00e7\u00e3o visual e l\u00f3gica de um pipeline de dados.</p> <p>DAG: vertices e arestas</p> <p>Em um DAG, as tarefas s\u00e3o representadas como n\u00f3s e as depend\u00eancias como arestas direcionadas.</p> <ul> <li>Directed: As conex\u00f5es t\u00eam dire\u00e7\u00e3o (A \u2192 B)</li> <li>Acyclic: N\u00e3o h\u00e1 ciclos (A n\u00e3o pode depender de B se B depende de A)</li> <li>Graph: Estrutura que conecta tarefas</li> </ul> <pre><code>graph LR\n    A[Extra\u00e7\u00e3o&lt;br/&gt;PostgreSQL] --&gt; B[Valida\u00e7\u00e3o&lt;br/&gt;Dados]\n    B --&gt; C[Transforma\u00e7\u00e3o&lt;br/&gt;Agrega\u00e7\u00e3o]\n    C --&gt; D[Carga&lt;br/&gt;Data Warehouse]\n    D --&gt; E[Notifica\u00e7\u00e3o&lt;br/&gt;Email]\n\n    style A fill:#2563eb,color:#ffffff\n    style B fill:#7c3aed,color:#ffffff\n    style C fill:#059669,color:#ffffff\n    style D fill:#ea580c,color:#ffffff\n    style E fill:#dc2626,color:#ffffff</code></pre>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#tarefas-tasks","title":"Tarefas (Tasks)","text":"<p>Cada tarefa representa uma unidade de trabalho espec\u00edfica no pipeline, como extrair dados de uma API, executar uma transforma\u00e7\u00e3o ou enviar uma notifica\u00e7\u00e3o.</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#dependencias","title":"Depend\u00eancias","text":"<p>As depend\u00eancias definem a ordem de execu\u00e7\u00e3o das tarefas. Uma tarefa s\u00f3 executa quando todas as suas depend\u00eancias foram conclu\u00eddas com sucesso.</p> <p>No exemplo do gr\u00e1fico, a tarefa de Validar Dados Clientes s\u00f3 pode come\u00e7ar ap\u00f3s a Extra\u00e7\u00e3o da Tabela Clientes ser conclu\u00edda. Da mesma forma, a Transforma\u00e7\u00e3o de Vendas por Cliente s\u00f3 pode ocorrer ap\u00f3s a valida\u00e7\u00e3o dos dados de vendas, clientes e produtos.</p> <p>Veja um outro exemplo de DAG com depend\u00eancias:</p> <pre><code>graph LR\n    A[Extrair&lt;br/&gt;Tabela Clientes] --&gt; D[Validar&lt;br/&gt;Dados Clientes]\n    B[Extrair&lt;br/&gt;Tabela Vendas] --&gt; E[Validar&lt;br/&gt;Dados Vendas]\n    C[Extrair&lt;br/&gt;Tabela Produtos] --&gt; F[Validar&lt;br/&gt;Dados Produtos]\n\n    D --&gt; G[Transformar&lt;br/&gt;Vendas por Cliente]\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H[Carregar&lt;br/&gt;Data Warehouse]\n    H --&gt; I[Atualizar&lt;br/&gt;Dashboard]\n    H --&gt; J[Enviar&lt;br/&gt;Relat\u00f3rio Email]\n\n    style A fill:#2563eb,color:#ffffff\n    style B fill:#2563eb,color:#ffffff\n    style C fill:#2563eb,color:#ffffff\n    style D fill:#7c3aed,color:#ffffff\n    style E fill:#7c3aed,color:#ffffff\n    style F fill:#7c3aed,color:#ffffff\n    style G fill:#059669,color:#ffffff\n    style H fill:#ea580c,color:#ffffff\n    style I fill:#dc2626,color:#ffffff\n    style J fill:#dc2626,color:#ffffff</code></pre> <p>Conceitos</p> <p>Em um DAG, se a Tarefa C depende das Tarefas A e B, qual \u00e9 a ordem correta de execu\u00e7\u00e3o?</p> C \u2192 A \u2192 B A \u2192 C \u2192 B A e B (podem executar em paralelo) \u2192 C B \u2192 A \u2192 C Submit <p>Answer</p> <p>Como C depende de A e B, estas duas tarefas precisam executar primeiro.</p> <p>A e B podem executar em paralelo pois n\u00e3o dependem uma da outra, e C s\u00f3 executa ap\u00f3s ambas terminarem.</p> <p>Lembram do ponto de encontro (RDV, rendezvous) de SisHard?!</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#ferramentas-de-orquestracao","title":"Ferramentas de Orquestra\u00e7\u00e3o","text":"<p>Existem diversas ferramentas de orquestra\u00e7\u00e3o dispon\u00edveis no mercado, cada uma com suas caracter\u00edsticas e casos de uso espec\u00edficos.</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#apache-airflow","title":"Apache Airflow","text":"<p>O Airflow \u00e9 uma das ferramentas mais populares para orquestra\u00e7\u00e3o de dados. Desenvolvido pelo Airbnb e depois doado para a Apache Foundation, permite definir DAGs usando c\u00f3digo Python.</p> <p>Caracter\u00edsticas:</p> <ul> <li>Interface web rica para monitoramento</li> <li>Ampla ado\u00e7\u00e3o no mercado</li> <li>Flexibilidade para integra\u00e7\u00f5es complexas</li> <li>Curva de aprendizado mais acentuada</li> </ul> <p>Iremos explorar o Airflow em aulas futuras.</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#prefect","title":"Prefect","text":"<p>O Prefect \u00e9 uma ferramenta moderna de orquestra\u00e7\u00e3o que busca ser mais developer-friendly que o Airflow, com uma abordagem \"Python-first\".</p> <p>Caracter\u00edsticas:</p> <ul> <li>Sintaxe Python mais limpa e intuitiva</li> <li>Melhor tratamento de erros e retry autom\u00e1tico</li> <li>Interface moderna e responsiva</li> <li>Facilidade para desenvolvimento local</li> </ul>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#outros-orquestradores","title":"Outros Orquestradores","text":"<p>Al\u00e9m do Airflow e Prefect, existem outras ferramentas de orquestra\u00e7\u00e3o. Alguns exemplos:</p> <ul> <li>Dagster: Foco em qualidade de dados</li> <li>Argo Workflows: Para ambientes Kubernetes</li> <li>AWS Step Functions + AWS Glue: Nativos da AWS</li> </ul> <p>Exerc\u00edcio</p> <p>Qual \u00e9 a principal diferen\u00e7a entre usar cron e usar um orquestrador como Prefect para pipelines de dados?</p> Submit <p>Answer</p> <p>A principal diferen\u00e7a \u00e9 que o cron apenas agenda execu\u00e7\u00f5es baseadas em tempo, enquanto orquestradores como Prefect gerenciam depend\u00eancias, estado das tarefas, tratamento de falhas e monitoramento, oferecendo controle completo sobre o fluxo de dados.</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#por-que-usar-orquestracao","title":"Por que Usar Orquestra\u00e7\u00e3o?","text":"<p>A orquestra\u00e7\u00e3o resolve problemas cr\u00edticos em ambientes de produ\u00e7\u00e3o:</p> <ul> <li> <p>Confiabilidade: Sistemas orquestrados s\u00e3o mais resilientes a falhas, com mecanismos autom\u00e1ticos de retry e recupera\u00e7\u00e3o.</p> </li> <li> <p>Visibilidade: Voc\u00ea pode acompanhar o status de cada tarefa em tempo real, identificar gargalos e diagnosticar problemas rapidamente.</p> </li> <li> <p>Escalabilidade: Orquestradores permitem paraleliza\u00e7\u00e3o eficiente de tarefas independentes e distribui\u00e7\u00e3o de cargas de trabalho.</p> </li> <li> <p>Manutenibilidade: O c\u00f3digo fica mais organizado e versionado, facilitando atualiza\u00e7\u00f5es e corre\u00e7\u00f5es.</p> </li> </ul> <p>Complexidade vs Benef\u00edcio</p> <p>A orquestra\u00e7\u00e3o adiciona complexidade ao projeto.</p> <p>Para pipelines muito simples, pode ser over-engineering!</p> <p>Avalie sempre se os benef\u00edcios justificam a complexidade adicional para seu caso espec\u00edfico.</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#quando-usar-orquestracao","title":"Quando Usar Orquestra\u00e7\u00e3o?","text":"<p>A orquestra\u00e7\u00e3o \u00e9 recomendada quando voc\u00ea tem:</p> <ul> <li>M\u00faltiplas tarefas com depend\u00eancias entre si</li> <li>Pipelines cr\u00edticos que n\u00e3o podem falhar</li> <li>Necessidade de reprocessamento de dados hist\u00f3ricos</li> <li>Equipes que precisam colaborar no mesmo pipeline</li> <li>Requisitos de auditoria e rastreabilidade</li> </ul> <p>Cen\u00e1rios de Uso</p> <p>Qual cen\u00e1rio mais se beneficia de um orquestrador de dados?</p> Script simples que roda uma vez por dia para backup Pipeline com cinco etapas que processa dados de vendas e precisa alertar em caso de falha Aplica\u00e7\u00e3o web que consulta dados em tempo real Processo manual executado semanalmente Submit <p>Answer</p> <p>O pipeline com m\u00faltiplas etapas, depend\u00eancias e necessidade de alertas \u00e9 o cen\u00e1rio ideal para orquestra\u00e7\u00e3o, pois requer coordena\u00e7\u00e3o, monitoramento e tratamento de falhas.</p>"},{"location":"classes/06-intro-orchestration/intro-orchestration/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<p>Agora que entendemos os conceitos fundamentais de orquestra\u00e7\u00e3o, vamos partir para a pr\u00e1tica! Na pr\u00f3xima se\u00e7\u00e3o, implementaremos nosso primeiro pipeline usando Prefect, explorando como definir tarefas, depend\u00eancias e monitorar a execu\u00e7\u00e3o.</p> <p>Voc\u00ea ver\u00e1 como a orquestra\u00e7\u00e3o transforma pipelines de dados complexos em fluxos confi\u00e1veis, observ\u00e1veis e mant\u00edveis.</p>"},{"location":"classes/06-intro-orchestration/prefect/","title":"Orquestra\u00e7\u00e3o com Prefect","text":""},{"location":"classes/06-intro-orchestration/prefect/#orquestracao-com-prefect","title":"Orquestra\u00e7\u00e3o com Prefect","text":""},{"location":"classes/06-intro-orchestration/prefect/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Na aula anterior, exploramos os conceitos fundamentais de orquestra\u00e7\u00e3o de dados e identificamos as limita\u00e7\u00f5es do cron para pipelines complexos. Agora \u00e9 hora de colocar a m\u00e3o na massa e experimentar uma ferramenta moderna de orquestra\u00e7\u00e3o.</p> <p>O Prefect se destaca por sua abordagem Python-first, oferecendo uma sintaxe adequada para desenvolvedores que j\u00e1 dominam Python.</p> <p>Info!</p> <p>O Prefect foi projetado para ser amig\u00e1vel ao desenvolvedor, mantendo a simplicidade sem sacrificar a robustez.</p> <p>Nesta aula pr\u00e1tica, iremos:</p> <ul> <li>Configurar um ambiente de orquestra\u00e7\u00e3o local com Prefect</li> <li>Criar e executar nossos primeiros flows e tasks</li> <li>Explorar a interface web para monitoramento</li> <li>Implementar pipelines com depend\u00eancias e tratamento de erros</li> </ul> <p>Exerc\u00edcio</p> <p>Com base no que aprendemos sobre orquestra\u00e7\u00e3o, quais vantagens voc\u00ea espera encontrar ao usar Prefect em compara\u00e7\u00e3o com agendamento tradicional usando cron?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#configurando-o-ambiente","title":"Configurando o Ambiente","text":"<p>Vamos come\u00e7ar configurando um ambiente local do Prefect usando Docker.</p> <p>Isso nos permite experimentar a ferramenta sem a necessidade de instala\u00e7\u00f5es complexas ou configura\u00e7\u00f5es de infraestrutura.</p> <p>Exerc\u00edcio</p> <p>Crie uma nova pasta para esta parte da aula:</p> <pre><code>$ mkdir -p 06-intro-orchestration/03-prefect\n$ cd 06-intro-orchestration/03-prefect\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#docker-compose","title":"Docker Compose","text":"<p>O Prefect oferece uma imagem Docker oficial que facilita a execu\u00e7\u00e3o local. Vamos configurar um servidor Prefect usando Docker Compose.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>docker-compose.yml</code> com o seguinte conte\u00fado:</p> <pre><code>services:\n  prefect-server:\n    image: prefecthq/prefect:3.4-python3.12\n    container_name: prefect_server\n    command: [\"prefect\", \"server\", \"start\"]\n    environment:\n      - PREFECT_SERVER_API_HOST=0.0.0.0\n      - PREFECT_API_URL=http://127.0.0.1:4200/api\n      - PREFECT_API_DATABASE_CONNECTION_URL=sqlite+aiosqlite:///prefect.db\n    ports:\n      - \"4200:4200\"\n    volumes:\n      - prefect_data:/root/.prefect\n    user: \"0:0\"\n\nvolumes:\n  prefect_data:\n</code></pre> <p>Info!</p> <p>Perceba que estamos utilizando sqlite como banco de dados para o Prefect.</p> <p>Isso \u00e9 \u00f3timo para desenvolvimento e testes locais, mas em produ\u00e7\u00e3o, voc\u00ea deve considerar um banco de dados mais robusto.</p> Mark as done <p>Exerc\u00edcio</p> <p>Inicie o servidor Prefect:</p> <pre><code>$ docker compose up -d\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Verifique se o servidor est\u00e1 executando corretamente:</p> <pre><code>$ docker logs prefect_server -f\n</code></pre> <p>Voc\u00ea deve ver logs indicando que o servidor Prefect foi iniciado com sucesso.</p> Mark as done <p>Exerc\u00edcio</p> <p>Acesse a interface web do Prefect em seu navegador: http://localhost:4200</p> <p>Voc\u00ea deve ver o dashboard do Prefect similar \u00e0 imagem abaixo:</p> <p></p> Mark as done <p>Exerc\u00edcio</p> <p>O que voc\u00ea observa na interface inicial do Prefect? Descreva brevemente o que v\u00ea no dashboard.</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#primeiro-flow","title":"Primeiro Flow","text":"<p>Agora vamos criar nosso primeiro flow com Prefect.</p> <p>Um flow \u00e9 a unidade principal de organiza\u00e7\u00e3o no Prefect, representando um conjunto de tarefas (tasks) que devem ser executadas.</p>"},{"location":"classes/06-intro-orchestration/prefect/#instalacao-local","title":"Instala\u00e7\u00e3o Local","text":"<p>Para desenvolver localmente, precisamos instalar o Prefect em nossa m\u00e1quina.</p> <p>Aten\u00e7\u00e3o!</p> <p>Nosso esquema ser\u00e1:</p> <ul> <li>O servidor Prefect rodando em Docker</li> <li>Tarefas enviadas para o servidor via API.<ul> <li>Isto ser\u00e1 rodado localmente (VS Code).</li> <li>As tarefas ser\u00e3o definidas em arquivos Python.</li> <li>Precisaremos de um ambiente virtual!</li> </ul> </li> </ul> <p>Exerc\u00edcio</p> <p>Crie um ambiente virtual utilizando a ferramenta de sua prefer\u00eancia e instale as depend\u00eancias do projeto:</p> <p>Arquivo <code>requirements.txt</code>: <pre><code>prefect==3.4\npython-dotenv==1.1.1\n</code></pre></p> <p>Ambiente virtual com <code>uv</code> <pre><code>$ uv venv --python 3.12 venv\n$ source venv/bin/activate\n$ uv pip install -r requirements.txt\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#codigo-basico","title":"C\u00f3digo B\u00e1sico","text":"<p>Exerc\u00edcio</p> <p>Crie um arquivo <code>primeiro_flow.py</code> com o seguinte conte\u00fado:</p> <pre><code>from prefect import flow, task\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n@task\ndef mensagem_hello():\n    return \"Ol\u00e1 mundo\"\n\n@flow\ndef flow_hello():\n    mensagem()\n\nif __name__ == \"__main__\":\n    flow_hello()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Configure a URL da API do Prefect para conectar ao servidor Docker.</p> <p>Para isto, crie um arquivo <code>.env</code> contendo:</p> <pre><code>PREFECT_API_URL=http://127.0.0.1:4200/api\n</code></pre> <p>Nota</p> <p>Certifique-se de que o arquivo <code>.env</code> est\u00e1 no mesmo diret\u00f3rio que o seu script Python.</p> <p>Perceba que temos a chamada <code>load_dotenv(override=True)</code> no c\u00f3digo para carregar essa vari\u00e1vel de ambiente.</p> Mark as done <p>Exerc\u00edcio</p> <p>Execute o script:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run primeiro_flow.py\n</code></pre> <pre><code>$ python primeiro_flow.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>O que aconteceu no terminal ap\u00f3s executar o script? Descreva a sa\u00edda.</p> Submit <p>Exerc\u00edcio</p> <p>Agora acesse novamente a interface web do Prefect (http://localhost:4200) e navegue at\u00e9 as se\u00e7\u00f5es:</p> <ul> <li>Dashboard</li> <li>Runs</li> <li>Flows</li> </ul> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#explorando-a-interface","title":"Explorando a Interface","text":"<p>A interface web do Prefect oferece visibilidade completa sobre a execu\u00e7\u00e3o dos flows.</p> <p>Uma das principais vantagens da orquestra\u00e7\u00e3o \u00e9 a observabilidade dos processos.</p> <p>Vamos explorar suas funcionalidades principais.</p> <p>Exerc\u00edcio</p> <p>Efetue a corre\u00e7\u00e3o do arquivo <code>primeiro_flow.py</code> com o seguinte conte\u00fado:</p> <pre><code>from prefect import flow, task\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n@task\ndef mensagem_hello():\n    return \"Ol\u00e1 mundo\"\n\n@flow\ndef flow_hello():\n    mensagem_hello() # Alterou apenas aqui!\n\nif __name__ == \"__main__\":\n    flow_hello()\n</code></pre> <p>Execute novamente.</p> <p>Confira as mudan\u00e7as refletidas no terminal e na interface.</p> Mark as done <p>Exerc\u00edcio</p> <p>Na interface do Prefect, no menu Flow, clique no flow run que acabou de ser executado para ver os detalhes.</p> <p>Considere o flow executado com sucesso!</p> Mark as done <p>Exerc\u00edcio</p> <p>Quais informa\u00e7\u00f5es voc\u00ea consegue ver sobre a execu\u00e7\u00e3o do flow?</p> Submit <p>Exerc\u00edcio</p> <p>H\u00e1 alguma representa\u00e7\u00e3o visual do flow (como um grafo/DAG)? Descreva o que observa.</p> Submit <p>Exerc\u00edcio</p> <p>No menu Runs, explore as seguintes se\u00e7\u00f5es:</p> <ul> <li>Flows: Lista todos os flows criados</li> <li>Flow Runs: Hist\u00f3rico de execu\u00e7\u00f5es</li> <li>Task Runs: Detalhes de cada tarefa executada</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Quais m\u00e9tricas e informa\u00e7\u00f5es voc\u00ea considera mais \u00fateis para monitorar um pipeline em produ\u00e7\u00e3o? O Prefect fornece essas informa\u00e7\u00f5es?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#tarefas-com-tempo-de-execucao","title":"Tarefas com Tempo de Execu\u00e7\u00e3o","text":"<p>Vamos agora criar um exemplo mais realista, simulando tarefas que demandam tempo de processamento.</p> <p>Isso nos permitir\u00e1 observar melhor o comportamento do Prefect durante a execu\u00e7\u00e3o.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>flow_processamento.py</code> com o seguinte conte\u00fado:</p> <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\nimport random\n\nload_dotenv(override=True)\n\n@task(task_run_name=\"mensagem(espera={espera}s)\")\ndef mensagem(espera: int):\n    \"\"\"Simula uma tarefa que espera um tempo aleat\u00f3rio\"\"\"\n    time.sleep(espera)\n    # Redireciona logs para o prefect\n    logger = get_run_logger()\n    msg = f\"Esperou {espera} segundos\"\n    logger.info(msg)\n    return msg\n\n@flow\ndef flow_processamento_dummy():\n    mensagem(random.randint(1, 8))\n    mensagem(random.randint(1, 8))\n\nif __name__ == \"__main__\":\n    flow_processamento_dummy()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o novo script. Repita umas duas ou tr\u00eas vezes o comando:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run flow_processamento.py\n</code></pre> <pre><code>$ python flow_processamento.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Quanto tempo demorou para a execu\u00e7\u00e3o completa? Por que as duas tarefas n\u00e3o executaram em paralelo?</p> Submit <p>Exerc\u00edcio</p> <p>Acesse a interface web e examine os detalhes dessa nova execu\u00e7\u00e3o. Observe particularmente:</p> <ul> <li>O tempo total de execu\u00e7\u00e3o</li> <li>O tempo individual de cada tarefa</li> <li>Os logs gerados</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Execute novamente o pipeline e, enquanto ele executa, observe em tempo real:</p> <ul> <li>O status da execu\u00e7\u00e3o na interface web</li> <li>Os logs sendo gerados</li> <li>A progress\u00e3o atrav\u00e9s das tarefas</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Compare a experi\u00eancia de monitoramento no Prefect com o que seria poss\u00edvel usando apenas cron. Quais diferen\u00e7as voc\u00ea identifica?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#paralelizacao-de-tarefas","title":"Paraleliza\u00e7\u00e3o de Tarefas","text":"<p>Uma das grandes vantagens dos orquestradores \u00e9 a capacidade de executar tarefas independentes em paralelo. Vamos modificar nosso flow para demonstrar essa funcionalidade.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>flow_paralelo.py</code>:</p> <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\nimport random\n\nload_dotenv(override=True)\n\n@task(task_run_name=\"processar_dados(id={dados_id})\")\ndef processar_dados(dados_id: int):\n    # Simula processamento com tempo vari\u00e1vel\n    tempo_processamento = random.randint(2, 6)\n    time.sleep(tempo_processamento)\n\n    logger = get_run_logger()\n    logger.info(f\"Processamento {dados_id} finalizado em {tempo_processamento}s\")\n\n    return {\n        \"id\": dados_id,\n        \"tempo_processamento\": tempo_processamento,\n        \"resultado\": f\"dados_{dados_id}_processados\"\n    }\n\n@flow\ndef flow_processamento_paralelo():\n    # Lista para armazenar os futures das tarefas\n    futures = []\n\n    # Submete 4 tarefas para execu\u00e7\u00e3o\n    for i in range(1, 5):\n        future = processar_dados.submit(i)\n        futures.append(future)\n\n    # Aguarda a conclus\u00e3o de todas as tarefas\n    resultados = [future.result() for future in futures]\n\n    logger = get_run_logger()\n    logger.info(f\"Processamento finalizado. Resultados: {resultados}\")\n\n    return resultados\n\nif __name__ == \"__main__\":\n    flow_processamento_paralelo()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o script e observe o tempo total de execu\u00e7\u00e3o:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run flow_paralelo.py\n</code></pre> <pre><code>$ python flow_paralelo.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Qual foi o tempo total de execu\u00e7\u00e3o? Foi menor que a soma dos tempos individuais das tarefas?</p> Submit <p>Exerc\u00edcio</p> <p>Na interface web, examine o gr\u00e1fico de execu\u00e7\u00e3o deste flow. Como as tarefas aparecem representadas visualmente?</p> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#dependencias-entre-tarefas","title":"Depend\u00eancias Entre Tarefas","text":"<p>Vamos agora implementar um pipeline mais realista com depend\u00eancias entre tarefas, simulando um processo de ETL onde cada etapa depende da anterior.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>flow_dependencias.py</code>:</p> <code>flow_dependencias.py</code>: <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\n\nload_dotenv(override=True)\n\n@task\ndef extrair_dados_vendas():\n    logger = get_run_logger()\n    logger.info(\"Iniciando extra\u00e7\u00e3o de dados de vendas\")\n\n    # Simula extra\u00e7\u00e3o de dados\n    time.sleep(2)\n\n    dados = [\n        {\"id\": 1, \"produto\": \"Notebook\", \"valor\": 2500.00, \"quantidade\": 2},\n        {\"id\": 2, \"produto\": \"Mouse\", \"valor\": 50.00, \"quantidade\": 5},\n        {\"id\": 3, \"produto\": \"Teclado\", \"valor\": 150.00, \"quantidade\": 3}\n    ]\n\n    logger.info(f\"Extra\u00eddos {len(dados)} registros de vendas\")\n    return dados\n\n@task\ndef validar_dados(dados):\n    logger = get_run_logger()\n    logger.info(\"Iniciando valida\u00e7\u00e3o de dados\")\n\n    # Simula valida\u00e7\u00e3o\n    time.sleep(1)\n\n    dados_validos = []\n    for item in dados:\n        if item[\"valor\"] &gt; 0 and item[\"quantidade\"] &gt; 0:\n            dados_validos.append(item)\n            logger.info(f\"Registro {item['id']} validado com sucesso\")\n        else:\n            logger.warning(f\"Registro {item['id']} falhou na valida\u00e7\u00e3o\")\n\n    return dados_validos\n\n@task\ndef transformar_dados(dados):\n    logger = get_run_logger()\n    logger.info(\"Iniciando transforma\u00e7\u00e3o de dados\")\n\n    # Simula transforma\u00e7\u00e3o (c\u00e1lculo de total)\n    time.sleep(1)\n\n    for item in dados:\n        item[\"total\"] = item[\"valor\"] * item[\"quantidade\"]\n        logger.info(f\"Produto {item['produto']}: Total = R$ {item['total']:.2f}\")\n\n    return dados\n\n@task\ndef carregar_dados(dados):\n    logger = get_run_logger()\n    logger.info(\"Iniciando carregamento no Data Warehouse\")\n\n    # Simula carregamento\n    time.sleep(2)\n\n    logger.info(f\"Carregados {len(dados)} registros no Data Warehouse\")\n    logger.info(\"Pipeline ETL finalizado com sucesso\")\n\n    return {\"status\": \"success\", \"registros_carregados\": len(dados)}\n\n@flow(name=\"Pipeline ETL Vendas\")\ndef pipeline_etl_vendas():\n    # Definindo o pipeline com depend\u00eancias\n    dados_brutos = extrair_dados_vendas()\n    dados_validos = validar_dados(dados_brutos)\n    dados_transformados = transformar_dados(dados_validos)\n    resultado = carregar_dados(dados_transformados)\n\n    return resultado\n\nif __name__ == \"__main__\":\n    pipeline_etl_vendas()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o pipeline ETL:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run flow_dependencias.py\n</code></pre> <pre><code>$ python flow_dependencias.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Quanto tempo demorou a execu\u00e7\u00e3o total?</p> Submit <p>Exerc\u00edcio</p> <p>Na interface web, explore o flow run rec\u00e9m executado.</p> <p>Examine:</p> <ul> <li>O grafo visual das depend\u00eancias</li> <li>Os logs de cada tarefa individual</li> <li>O tempo de execu\u00e7\u00e3o de cada etapa</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Analisando o grafo visual do pipeline, como o Prefect representa as depend\u00eancias entre as tarefas?</p> <p>Isso facilita o entendimento do flow?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#tratamento-de-erros","title":"Tratamento de Erros","text":"<p>Uma das grandes vantagens dos orquestradores \u00e9 o tratamento robusto de erros. Vamos simular falhas para observar como o Prefect lida com essas situa\u00e7\u00f5es.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>flow_erros.py</code>:</p> <code>flow_erros.py</code> <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\nimport random\n\nload_dotenv(override=True)\n\n@task(retries=3, retry_delay_seconds=2)\ndef tarefa_instavel():\n    logger = get_run_logger()\n\n    # 60% de chance de falha\n    if random.random() &lt; 0.6:\n        logger.error(\"Simulando falha na tarefa\")\n        raise Exception(\"Erro simulado - recurso indispon\u00edvel\")\n\n    logger.info(\"Tarefa executada com sucesso\")\n    return \"dados_processados\"\n\n@task\ndef tarefa_dependente(dados):\n    logger = get_run_logger()\n    logger.info(f\"Processando dados recebidos: {dados}\")\n    time.sleep(1)\n    return f\"resultado_final_{dados}\"\n\n@flow(name=\"Pipeline com Tratamento de Erros\")\ndef pipeline_com_erros():\n    try:\n        dados = tarefa_instavel()\n        resultado = tarefa_dependente(dados)\n        return resultado\n    except Exception as e:\n        logger = get_run_logger()\n        logger.error(f\"Pipeline falhou: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    pipeline_com_erros()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o script algumas vezes para observar o comportamento com falhas:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run flow_erros.py\n</code></pre> <pre><code>$ python flow_erros.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Quantas tentativas (retries) foram feitas antes da tarefa ter sucesso ou falhar definitivamente?</p> Submit <p>Exerc\u00edcio</p> <p>Na interface web, examine as execu\u00e7\u00f5es que falharam. Como o Prefect apresenta as informa\u00e7\u00f5es sobre:</p> <ul> <li>Tentativas de retry</li> <li>Mensagens de erro</li> <li>Status final da execu\u00e7\u00e3o</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Liste algumas situa\u00e7\u00f5es em que retry \u00e9 um recurso interessante.</p> Submit <p>Answer</p> <ul> <li>Intera\u00e7\u00f5es com APIs externas que podem falhar temporariamente.</li> <li>Tarefas que dependem de recursos externos, como bancos de dados ou servi\u00e7os de fila.</li> </ul> <p>Exerc\u00edcio</p> <p>Como voc\u00ea implementaria esse mesmo tratamento de erros usando apenas cron? Quais seriam as dificuldades?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#pipeline-de-dados","title":"Pipeline de Dados","text":"<p>Vamos agora criar um pipeline mais pr\u00f3ximo da realidade, simulando um processo de ETL que poderia ser usado em produ\u00e7\u00e3o.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>pipeline_vendas.py</code>:</p> <code>pipeline_vendas.py</code> <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\nimport random\nfrom datetime import datetime, timedelta\n\nload_dotenv(override=True)\n\n@task\ndef extrair_vendas_database():\n    \"\"\"Simula extra\u00e7\u00e3o de dados do banco de vendas\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Conectando ao banco de dados de vendas\")\n\n    # Simula tempo de extra\u00e7\u00e3o\n    time.sleep(2)\n\n    # Dados simulados\n    vendas = []\n    for i in range(random.randint(10, 50)):\n        venda = {\n            \"id\": i + 1,\n            \"cliente_id\": random.randint(1, 100),\n            \"produto_id\": random.randint(1, 20),\n            \"quantidade\": random.randint(1, 5),\n            \"valor_unitario\": round(random.uniform(10, 1000), 2),\n            \"data_venda\": (datetime.now() - timedelta(days=random.randint(0, 30))).isoformat()\n        }\n        vendas.append(venda)\n\n    logger.info(f\"Extra\u00eddas {len(vendas)} vendas do banco de dados\")\n    return vendas\n\n@task\ndef validar_qualidade_dados(vendas):\n    \"\"\"Valida a qualidade dos dados extra\u00eddos\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Iniciando valida\u00e7\u00e3o de qualidade dos dados\")\n\n    vendas_validas = []\n    problemas = 0\n\n    for venda in vendas:\n        # Valida\u00e7\u00f5es b\u00e1sicas\n        if (venda[\"quantidade\"] &gt; 0 and \n            venda[\"valor_unitario\"] &gt; 0 and \n            venda[\"cliente_id\"] &gt; 0):\n            vendas_validas.append(venda)\n        else:\n            problemas += 1\n            logger.warning(f\"Venda ID {venda['id']} falhou na valida\u00e7\u00e3o\")\n\n    taxa_qualidade = (len(vendas_validas) / len(vendas)) * 100\n    logger.info(f\"Taxa de qualidade: {taxa_qualidade:.1f}% ({problemas} problemas encontrados)\")\n\n    if taxa_qualidade &lt; 90:\n        raise Exception(f\"Taxa de qualidade muito baixa: {taxa_qualidade:.1f}%\")\n\n    return vendas_validas\n\n@task\ndef aplicar_transformacoes(vendas):\n    \"\"\"Aplica transforma\u00e7\u00f5es nos dados\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Aplicando transforma\u00e7\u00f5es nos dados\")\n\n    time.sleep(1)\n\n    for venda in vendas:\n        # Calcula valor total\n        venda[\"valor_total\"] = venda[\"quantidade\"] * venda[\"valor_unitario\"]\n\n        # Categoriza venda por valor\n        if venda[\"valor_total\"] &gt; 500:\n            venda[\"categoria\"] = \"alta\"\n        elif venda[\"valor_total\"] &gt; 100:\n            venda[\"categoria\"] = \"media\"\n        else:\n            venda[\"categoria\"] = \"baixa\"\n\n    logger.info(f\"Transforma\u00e7\u00f5es aplicadas em {len(vendas)} registros\")\n    return vendas\n\n@task\ndef carregar_data_warehouse(vendas):\n    \"\"\"Carrega dados no Data Warehouse\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Iniciando carregamento no Data Warehouse\")\n\n    # Simula carregamento\n    time.sleep(3)\n\n    # Estat\u00edsticas do carregamento\n    total_valor = sum(v[\"valor_total\"] for v in vendas)\n    vendas_por_categoria = {}\n\n    for venda in vendas:\n        cat = venda[\"categoria\"]\n        vendas_por_categoria[cat] = vendas_por_categoria.get(cat, 0) + 1\n\n    logger.info(f\"Carregamento conclu\u00eddo:\")\n    logger.info(f\"- Total de registros: {len(vendas)}\")\n    logger.info(f\"- Valor total: R$ {total_valor:.2f}\")\n    logger.info(f\"- Distribui\u00e7\u00e3o por categoria: {vendas_por_categoria}\")\n\n    return {\n        \"registros_carregados\": len(vendas),\n        \"valor_total\": total_valor,\n        \"distribuicao\": vendas_por_categoria\n    }\n\n@task\ndef gerar_relatorio_execucao(resultado_carregamento):\n    \"\"\"Gera relat\u00f3rio final da execu\u00e7\u00e3o\"\"\"\n    logger = get_run_logger()\n\n    relatorio = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"status\": \"sucesso\",\n        \"metricas\": resultado_carregamento\n    }\n\n    logger.info(\"=== RELAT\u00d3RIO DE EXECU\u00c7\u00c3O ===\")\n    logger.info(f\"Timestamp: {relatorio['timestamp']}\")\n    logger.info(f\"Status: {relatorio['status']}\")\n    logger.info(f\"Registros processados: {resultado_carregamento['registros_carregados']}\")\n\n    return relatorio\n\n@flow(name=\"Pipeline ETL Vendas Completo\", description=\"Pipeline completo de ETL para dados de vendas\")\ndef pipeline_etl_completo():\n    # Extra\u00e7\u00e3o\n    vendas_brutas = extrair_vendas_database()\n\n    # Valida\u00e7\u00e3o\n    vendas_validas = validar_qualidade_dados(vendas_brutas)\n\n    # Transforma\u00e7\u00e3o\n    vendas_transformadas = aplicar_transformacoes(vendas_validas)\n\n    # Carregamento\n    resultado = carregar_data_warehouse(vendas_transformadas)\n\n    # Relat\u00f3rio\n    relatorio = gerar_relatorio_execucao(resultado)\n\n    return relatorio\n\nif __name__ == \"__main__\":\n    pipeline_etl_completo()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o pipeline completo:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run pipeline_vendas.py\n</code></pre> <pre><code>$ python pipeline_vendas.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Quantas tarefas foram executadas no total? Qual foi a dura\u00e7\u00e3o total do pipeline?</p> Submit <p>Exerc\u00edcio</p> <p>Na interface web, analise a execu\u00e7\u00e3o completa:</p> <ul> <li>Examine o DAG gerado</li> <li>Verifique os logs de cada tarefa</li> <li>Observe as m\u00e9tricas de tempo</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Agora que voc\u00ea viu um pipeline completo, quais vantagens da orquestra\u00e7\u00e3o ficaram mais evidentes? Compare com uma implementa\u00e7\u00e3o usando apenas scripts Python e cron.</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#executando-multiplas-vezes","title":"Executando M\u00faltiplas Vezes","text":"<p>Para simular um ambiente mais pr\u00f3ximo da produ\u00e7\u00e3o, vamos executar o pipeline v\u00e1rias vezes e observar o hist\u00f3rico de execu\u00e7\u00f5es.</p> <p>Exerc\u00edcio</p> <p>Execute o pipeline de vendas cinco vezes consecutivas:</p> <pre><code>$ for i in {1..5}; do python pipeline_vendas.py; sleep 10; done\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Na interface web, explore a se\u00e7\u00e3o Flow Runs e examine:</p> <ul> <li>O hist\u00f3rico de todas as execu\u00e7\u00f5es</li> <li>As diferentes dura\u00e7\u00f5es de cada execu\u00e7\u00e3o</li> <li>Poss\u00edveis varia\u00e7\u00f5es nos resultados</li> </ul> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#agendamento","title":"Agendamento","text":"<p>Uma das funcionalidades mais importantes dos orquestradores \u00e9 a capacidade de agendar execu\u00e7\u00f5es regulares de pipelines.</p> <p>O Prefect oferece v\u00e1rias formas de configurar agendamentos, desde intervalos simples at\u00e9 express\u00f5es cron.</p>"},{"location":"classes/06-intro-orchestration/prefect/#deployments","title":"Deployments","text":"<p>No Prefect, o agendamento \u00e9 feito atrav\u00e9s de deployments. Um deployment \u00e9 uma configura\u00e7\u00e3o que define como e quando um flow deve ser executado.</p> <p>Exerc\u00edcio</p> <p>Primeiro, vamos criar um flow simples para agendar. Crie um arquivo <code>flow_para_agendar.py</code>:</p> <code>flow_para_agendar.py</code> <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\nfrom datetime import datetime\n\nload_dotenv(override=True)\n\n@task\ndef processar_dados_diarios():\n    \"\"\"Simula processamento de dados di\u00e1rios\"\"\"\n    logger = get_run_logger()\n    logger.info(f\"Executando processamento di\u00e1rio \u00e0s {datetime.now().strftime('%H:%M:%S')}\")\n\n    # Simula processamento\n    time.sleep(2)\n\n    registros_processados = 100 + int(time.time() % 50)\n    logger.info(f\"Processados {registros_processados} registros\")\n\n    return registros_processados\n\n@flow(name=\"Pipeline Di\u00e1rio\")\ndef pipeline_diario():\n    registros = processar_dados_diarios()\n\n    logger = get_run_logger()\n    logger.info(f\"Pipeline di\u00e1rio executado com sucesso: {registros} registros processados\")\n\n    return {\"timestamp\": datetime.now().isoformat(), \"registros\": registros}\n\nif __name__ == \"__main__\":\n    pipeline_diario()\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute uma vez para testar:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run flow_para_agendar.py\n</code></pre> <pre><code>$ python flow_para_agendar.py\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#criando-um-deployment","title":"Criando um Deployment","text":"<p>Agora vamos criar um deployment que executar\u00e1 nosso flow automaticamente em intervalos regulares.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>deploy_pipeline.py</code> para configurar o agendamento:</p> <pre><code>from prefect import serve\nfrom datetime import timedelta\nfrom flow_para_agendar import pipeline_diario\n\nif __name__ == \"__main__\":\n    # Cria um deployment com agendamento de 30 segundos\n    deployment = pipeline_diario.to_deployment(\n        name=\"pipeline-diario-30s\",\n        interval=timedelta(seconds=30),\n        description=\"Pipeline que executa a cada 30 segundos\"\n    )\n\n    # Inicia o servi\u00e7o que executar\u00e1 o deployment\n    serve(deployment)\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o deployment em background:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run deploy_pipeline.py &amp;\n</code></pre> <pre><code>$ python deploy_pipeline.py &amp;\n</code></pre> <p>Isso iniciar\u00e1 um worker que executar\u00e1 o pipeline a cada 30 segundos.</p> Mark as done <p>Exerc\u00edcio</p> <p>Acesse a interface web do Prefect e observe: - Na se\u00e7\u00e3o Deployments, voc\u00ea deve ver o deployment criado - Na se\u00e7\u00e3o Flow Runs, voc\u00ea ver\u00e1 execu\u00e7\u00f5es sendo criadas automaticamente</p> Mark as done <p>Exerc\u00edcio</p> <p>Quantas execu\u00e7\u00f5es foram criadas ap\u00f3s 2 minutos? O agendamento est\u00e1 funcionando corretamente?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#agendamento-com-cron","title":"Agendamento com Cron","text":"<p>O Prefect tamb\u00e9m suporta express\u00f5es cron tradicionais para agendamentos mais complexos.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>deploy_cron.py</code>:</p> <pre><code>from prefect import serve\nfrom flow_para_agendar import pipeline_diario\n\nif __name__ == \"__main__\":\n    # Deployment que executa a cada 2 minutos usando cron\n    deployment_cron = pipeline_diario.to_deployment(\n        name=\"pipeline-diario-cron\",\n        cron=\"*/2 * * * *\",  # A cada 2 minutos\n        description=\"Pipeline agendado via express\u00e3o cron\"\n    )\n\n    serve(deployment_cron)\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>O que significa a express\u00e3o cron <code>*/2 * * * *</code>? Compare com o que aprendemos sobre cron na aula anterior.</p> Submit <p>Exerc\u00edcio</p> <p>Pare o deployment anterior (Ctrl+C no terminal) e execute o novo:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run deploy_cron.py &amp;\n</code></pre> <pre><code>$ python deploy_cron.py &amp;\n</code></pre> Mark as done"},{"location":"classes/06-intro-orchestration/prefect/#multiplos-agendamentos","title":"M\u00faltiplos Agendamentos","text":"<p>Podemos ter m\u00faltiplos deployments do mesmo flow com agendamentos diferentes.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>deploy_multiplos.py</code>:</p> <pre><code>from prefect import serve\nfrom datetime import timedelta\nfrom flow_para_agendar import pipeline_diario\n\nif __name__ == \"__main__\":\n    # Deployment 1: A cada 45 segundos\n    deployment_rapido = pipeline_diario.to_deployment(\n        name=\"pipeline-rapido\",\n        interval=timedelta(seconds=45),\n        description=\"Execu\u00e7\u00e3o r\u00e1pida para testes\"\n    )\n\n    # Deployment 2: A cada 3 minutos\n    deployment_lento = pipeline_diario.to_deployment(\n        name=\"pipeline-lento\", \n        cron=\"*/3 * * * *\",\n        description=\"Execu\u00e7\u00e3o mais espa\u00e7ada\"\n    )\n\n    # Inicia ambos os deployments\n    serve(deployment_rapido, deployment_lento)\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute os m\u00faltiplos deployments:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run deploy_multiplos.py &amp;\n</code></pre> <pre><code>$ python deploy_multiplos.py &amp;\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Na interface web, observe:</p> <ul> <li>Dois deployments diferentes na se\u00e7\u00e3o Deployments</li> <li>Execu\u00e7\u00f5es intercaladas com frequ\u00eancias diferentes</li> <li>Como o Prefect distingue as execu\u00e7\u00f5es de cada deployment</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Qual deployment executa com mais frequ\u00eancia? Como isso aparece no hist\u00f3rico de execu\u00e7\u00f5es?</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#agendamento-personalizado","title":"Agendamento Personalizado","text":"<p>Para cen\u00e1rios mais complexos, podemos criar agendamentos personalizados.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>deploy_personalizado.py</code>:</p> <pre><code>from prefect import serve\nfrom flow_para_agendar import pipeline_diario\n\nif __name__ == \"__main__\":\n    # Simula agendamento de hor\u00e1rio comercial (9h \u00e0s 18h, segunda a sexta)\n    deployment_comercial = pipeline_diario.to_deployment(\n        name=\"pipeline-horario-comercial\",\n        cron=\"0 9-18 * * 1-5\",  # Todo hora das 9h \u00e0s 18h, seg-sex\n        description=\"Pipeline executado apenas em hor\u00e1rio comercial\"\n    )\n\n    # Para fins de demonstra\u00e7\u00e3o, vamos usar um agendamento mais frequente\n    deployment_demo = pipeline_diario.to_deployment(\n        name=\"pipeline-demo-comercial\",\n        cron=\"*/1 * * * *\",  # A cada minuto para demonstra\u00e7\u00e3o\n        description=\"Demo do pipeline comercial\"\n    )\n\n    serve(deployment_demo, deployment_comercial)\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Como voc\u00ea interpretaria a express\u00e3o cron <code>0 9-18 * * 1-5</code>? Quebre cada campo e explique.</p> Submit <p>Exerc\u00edcio</p> <p>Execute o deployment personalizado:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run deploy_personalizado.py &amp;\n</code></pre> <pre><code>$ python deploy_personalizado.py &amp;\n</code></pre> <p>Observe as execu\u00e7\u00f5es por alguns minutos na interface web.</p> <p>Confira as pr\u00f3ximas execu\u00e7\u00f5es na aba Upcoming.</p> Mark as done <p>Parando Deployments</p> <p>Para parar os deployments em execu\u00e7\u00e3o, use Ctrl+C nos terminais onde est\u00e3o rodando, ou mate os processo no gerenciador de tarefas!</p>"},{"location":"classes/06-intro-orchestration/prefect/#pipeline-incremental-com-agendamento","title":"Pipeline Incremental com Agendamento","text":"<p>Um padr\u00e3o comum em produ\u00e7\u00e3o \u00e9 agendar pipelines que executam regularmente, mas que possuem l\u00f3gica condicional para processar apenas quando h\u00e1 dados novos dispon\u00edveis.</p> <p>Exerc\u00edcio</p> <p>Crie um arquivo <code>pipeline_incremental.py</code>:</p> <pre><code>from dotenv import load_dotenv\nfrom prefect import flow, task, get_run_logger\nimport time\nfrom datetime import datetime\n\nload_dotenv(override=True)\n\n@task\ndef verificar_novos_dados():\n    \"\"\"Verifica se h\u00e1 novos dados para processar\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Verificando disponibilidade de novos dados...\")\n\n    # Simula verifica\u00e7\u00e3o de novos dados\n    time.sleep(1)\n\n    # Simula presen\u00e7a de dados (70% de chance)\n    tem_dados = time.time() % 10 &lt; 7\n\n    if tem_dados:\n        logger.info(\"\u2705 Novos dados detectados - prosseguindo com pipeline\")\n        return True\n    else:\n        logger.info(\"\u2139\ufe0f  Nenhum dado novo encontrado - pulando execu\u00e7\u00e3o\")\n        return False\n\n@task\ndef processar_dados_incrementais():\n    \"\"\"Processa apenas dados novos\"\"\"\n    logger = get_run_logger()\n    logger.info(\"Iniciando processamento de dados incrementais...\")\n\n    time.sleep(3)\n\n    # Simula quantidade vari\u00e1vel de registros\n    registros_novos = int(50 + (time.time() % 100))\n    logger.info(f\"Processados {registros_novos} novos registros\")\n\n    return registros_novos\n\n@task\ndef atualizar_controle():\n    \"\"\"Atualiza controles de \u00faltima execu\u00e7\u00e3o\"\"\"\n    logger = get_run_logger()\n    timestamp = datetime.now().isoformat()\n    logger.info(f\"Controle atualizado: \u00faltima execu\u00e7\u00e3o em {timestamp}\")\n    return timestamp\n\n@flow(name=\"Pipeline Incremental Agendado\")\ndef pipeline_incremental_agendado():\n    \"\"\"Pipeline que executa regularmente mas processa apenas dados novos\"\"\"\n\n    # Sempre verifica se h\u00e1 dados novos\n    tem_dados = verificar_novos_dados()\n\n    if tem_dados:\n        # Processa os dados\n        registros = processar_dados_incrementais()\n\n        # Atualiza controles\n        timestamp_controle = atualizar_controle()\n\n        logger = get_run_logger()\n        logger.info(f\"\u2705 Pipeline executado com sucesso: {registros} registros processados\")\n\n        return {\n            \"status\": \"processado\",\n            \"registros\": registros,\n            \"timestamp\": timestamp_controle\n        }\n    else:\n        logger = get_run_logger()\n        logger.info(\"\u23ed\ufe0f  Pipeline pulado - sem dados novos para processar\")\n\n        return {\n            \"status\": \"pulado\",\n            \"registros\": 0,\n            \"timestamp\": datetime.now().isoformat()\n        }\n\nif __name__ == \"__main__\":\n    resultado = pipeline_incremental_agendado()\n    print(f\"Resultado: {resultado}\")\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Teste o pipeline incremental algumas vezes manualmente:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run pipeline_incremental.py\n</code></pre> <pre><code>$ python pipeline_incremental.py\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Observe as diferentes execu\u00e7\u00f5es. Algumas foram puladas por n\u00e3o haver dados novos? Como isso aparece nos logs?</p> Submit <p>Exerc\u00edcio</p> <p>Agora vamos agendar este pipeline para executar automaticamente. Crie um arquivo <code>deploy_incremental.py</code>:</p> <pre><code>from prefect import serve\nfrom datetime import timedelta\nfrom pipeline_incremental import pipeline_incremental_agendado\n\nif __name__ == \"__main__\":\n    # Deployment que executa a cada 45 segundos\n    deployment_incremental = pipeline_incremental_agendado.to_deployment(\n        name=\"pipeline-incremental-automatico\",\n        interval=timedelta(seconds=45),\n        description=\"Pipeline que verifica e processa dados incrementais automaticamente\"\n    )\n\n    serve(deployment_incremental)\n</code></pre> Mark as done <p>Exerc\u00edcio</p> <p>Execute o deployment incremental:</p> Com <code>uv</code>Com <code>python</code> <pre><code>$ uv run deploy_incremental.py\n</code></pre> <pre><code>$ python deploy_incremental.py\n</code></pre> <p>Deixe executar por alguns minutos e observe na interface web.</p> <p>Confira tamb\u00e9m a aba Upcoming.</p> Mark as done <p>Exerc\u00edcio</p> <p>Na interface web, analise o comportamento do pipeline incremental:</p> <ul> <li>Quantas execu\u00e7\u00f5es foram \"processadas\" vs \"puladas\"?</li> <li>Como os logs diferenciam os dois cen\u00e1rios?</li> <li>Observe os diferentes tempos de execu\u00e7\u00e3o</li> </ul> Mark as done <p>Dica!</p> <p>Este padr\u00e3o de \"pipeline agendado com l\u00f3gica condicional\" \u00e9 muito comum em produ\u00e7\u00e3o.</p> <p>Exerc\u00edcio</p> <p>Agora que experimentamos o Prefect, em que cen\u00e1rios voc\u00ea ainda consideraria usar cron ao inv\u00e9s de um orquestrador como Prefect? Explique seu racioc\u00ednio.</p> Submit"},{"location":"classes/06-intro-orchestration/prefect/#experimentacao-livre","title":"Experimenta\u00e7\u00e3o Livre","text":"<p>Agora \u00e9 sua vez de experimentar e criar algo \u00fanico!</p> <p>Exerc\u00edcio</p> <p>Escolha uma das op\u00e7\u00f5es abaixo ou crie algo pr\u00f3prio:</p> <p>Op\u00e7\u00e3o 1: Pipeline de An\u00e1lise de Logs</p> <ul> <li>Criar tarefas que simulem leitura de arquivos de log</li> <li>Implementar valida\u00e7\u00e3o e limpeza dos dados</li> <li>Gerar estat\u00edsticas e alertas</li> </ul> <p>Op\u00e7\u00e3o 2: Pipeline de Machine Learning</p> <ul> <li>Simular carregamento de dados de treinamento</li> <li>Implementar pr\u00e9-processamento</li> <li>Simular treinamento de modelo</li> <li>Validar resultados</li> </ul> <p>Op\u00e7\u00e3o 3: Pipeline de Monitoramento</p> <ul> <li>Verificar status de servi\u00e7os externos (APIs)</li> <li>Coletar m\u00e9tricas de performance</li> <li>Gerar alertas baseados em thresholds</li> </ul> Mark as done <p>Exerc\u00edcio</p> <p>Ap\u00f3s implementar seu pipeline customizado, descreva:</p> <ul> <li>Qual problema seu pipeline resolve</li> <li>Como voc\u00ea estruturou as depend\u00eancias</li> <li>Que tipo de tratamento de erro implementou</li> <li>Quais melhorias faria para uso em produ\u00e7\u00e3o</li> </ul> Submit"},{"location":"classes/06-intro-orchestration/prefect/#limpeza","title":"Limpeza","text":"<p>Para finalizar a aula, vamos fazer a limpeza do ambiente.</p> <p>Exerc\u00edcio</p> <p>Pare e remova os containers do Prefect:</p> <p>Aten\u00e7\u00e3o!</p> <p>A op\u00e7\u00e3o <code>-v</code> remove tamb\u00e9m os volumes, apagando todos os dados salvos (hist\u00f3rico do Prefect).</p> <p>Como estamos em uma aula, para poupar espa\u00e7o do seu disco, \u00e9 ok apagar!</p> <pre><code>$ docker compose down -v\n</code></pre> Mark as done <p>Por hoje \u00e9 s\u00f3!</p> <p>Na pr\u00f3xima aula, exploraremos como integrar essas ferramentas em arquiteturas de dados mais complexas.</p>"},{"location":"classes/06-intro-orchestration/references/","title":"Refer\u00eancias","text":"<ul> <li>DDA. Serra, James. Deciphering Data Architectures. \" O'Reilly Media, Inc.\", 2024.</li> </ul>"},{"location":"classes/07-data-modeling/intro-data-modeling/","title":"Modelagem de Dados","text":""},{"location":"classes/07-data-modeling/intro-data-modeling/#modelagem-de-dados","title":"Modelagem de Dados","text":""},{"location":"classes/07-data-modeling/intro-data-modeling/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Modelagem de dados \u00e9 o processo de design utilizado para estruturar e representar informa\u00e7\u00f5es de forma l\u00f3gica antes de sua implementa\u00e7\u00e3o em um sistema de armazenamento.</p> <p>Isso envolve definir quais dados s\u00e3o importantes, como se relacionam entre si e de que maneira podem ser organizados (por exemplo, em entidades, atributos e relacionamentos).</p> <p>A modelagem apoia a constru\u00e7\u00e3o de bancos de dados mais consistentes e f\u00e1ceis de usar, auxiliando para que as \u00e1reas t\u00e9cnicas possam atender melhor \u00e0s necessidades de neg\u00f3cio.</p> <p>Importante!</p> <p>Uma modelagem adequada pode reduzir redund\u00e2ncia, ou propiciar melhor desempenho em analytics!</p> <p>Existem diversas t\u00e9cnicas de modelagem de dados. Nesta aula, exploraremos algumas delas!</p>"},{"location":"classes/07-data-modeling/intro-data-modeling/#modelagem-relacional","title":"Modelagem relacional","text":"<p>A modelagem relacional foi desenvolvida por Edgar F. Codd em 1970 e representa uma das abordagens fundamentais para organizar dados em sistemas de informa\u00e7\u00e3o.</p> <p>Esta t\u00e9cnica estrutura os dados em tabelas interconectadas, onde cada tabela \u00e9 composta por linhas (registros ou tuplas) e colunas (campos ou atributos).</p> <p>O processo de modelagem relacional geralmente \u00e9 iniciado pela cria\u00e7\u00e3o de um diagrama entidade-relacionamento (ER). Este diagrama fornece uma representa\u00e7\u00e3o visual de alto n\u00edvel da estrutura do banco de dados, mostrando as entidades (dados) e os relacionamentos entre elas.</p> <p>Uma vez conclu\u00eddo o diagrama ER, voc\u00ea pode construir um modelo relacional mais detalhado que represente as tabelas e colunas reais do banco de dados.</p> <p>Como exemplo, considere a representa\u00e7\u00e3o do diagrama do modelo relacional para o banco de dados de <code>vendas</code>, utilizado na aula 03:</p> <p>Info!</p> <p>Na aula 03 vimos como estruturar um Data Warehouse.</p> <p>Nossa fonte foi um banco de dados fict\u00edcio de vendas, representado no modelo relacional abaixo.</p> <p></p>"},{"location":"classes/07-data-modeling/intro-data-modeling/#normalizacao-e-formas-normais","title":"Normaliza\u00e7\u00e3o e Formas Normais","text":"<p>Um modelo relacional geralmente utiliza um esquema de banco de dados normalizado, onde os dados s\u00e3o armazenados em um local \u00fanico e organizados em m\u00faltiplas tabelas com relacionamentos definidos entre elas.</p> <p>A normaliza\u00e7\u00e3o \u00e9 um conjunto de regras aplicadas para decompor um banco de dados complexo em tabelas menores e mais simples. O objetivo \u00e9 minimizar redund\u00e2ncia, melhorar a integridade dos dados e tornar o banco de dados mais eficiente e f\u00e1cil de manter.</p> <p>A normaliza\u00e7\u00e3o \u00e9 aplicada em n\u00edveis progressivos, conhecidos como formas normais. Vamos explorar as tr\u00eas primeiras formas normais, que s\u00e3o as mais comuns e amplamente utilizadas.</p>"},{"location":"classes/07-data-modeling/intro-data-modeling/#primeira-forma-normal-1nf","title":"Primeira Forma Normal (<code>1NF</code>)","text":"<p>Uma tabela est\u00e1 na primeira forma normal se satisfaz a seguinte condi\u00e7\u00e3o:</p> <ul> <li>Cada atributo cont\u00e9m um valor \u00fanico: Cada coluna deve armazenar apenas um valor, n\u00e3o uma lista de valores</li> </ul> <p>Info!</p> <p>Os valores devem ser at\u00f4micos, ou seja, indivis\u00edveis!</p> <p>Question</p> <p>Considere uma tabela <code>pedidos</code> com as colunas: <code>id_pedido</code>, <code>cliente</code>, <code>produtos</code> (onde <code>produtos</code> cont\u00e9m \"Produto A, Produto B, Produto C\"). Esta tabela est\u00e1 na primeira forma normal?</p> Sim, pois possui uma chave prim\u00e1ria N\u00e3o, pois a coluna <code>produtos</code> viola a atomicidade Sim, pois n\u00e3o h\u00e1 grupos repetitivos Depende do tipo de dados da coluna <code>produtos</code> Submit <p>Answer</p> <p>A tabela n\u00e3o est\u00e1 na 1NF porque a coluna <code>produtos</code> cont\u00e9m m\u00faltiplos valores em uma \u00fanica c\u00e9lula, violando o princ\u00edpio da atomicidade. Cada produto deveria estar em uma linha separada.</p>"},{"location":"classes/07-data-modeling/intro-data-modeling/#segunda-forma-normal-2nf","title":"Segunda Forma Normal (<code>2NF</code>)","text":"<p>Uma tabela est\u00e1 na segunda forma normal se satisfaz todas as condi\u00e7\u00f5es da 1NF, al\u00e9m de:</p> <ul> <li>Depend\u00eancia funcional total: Todo atributo n\u00e3o-chave deve depender completamente da chave prim\u00e1ria, n\u00e3o apenas de parte dela.</li> </ul> <p>Considere a seguinte tabela (<code>funcionario_departamento</code>) que registra a aloca\u00e7\u00e3o de funcion\u00e1rios em departamentos dentro de uma empresa, bem como o respectivo sal\u00e1rio acordado:</p> id_funcionario (PK) id_departamento (PK) nome_funcionario nome_departamento salario 101 501 Alice Engenharia 9000.00 101 502 Alice Pesquisa 9500.00 102 501 Bruno Engenharia 8800.00 <p>O objetivo dessa tabela \u00e9 registrar em quais departamentos cada funcion\u00e1rio atua e qual o sal\u00e1rio acordado para aquela aloca\u00e7\u00e3o espec\u00edfica.  </p> <p>Info!</p> <p>Perceba que um funcion\u00e1rio pode estar associado a mais de um departamento, com sal\u00e1rios diferentes.</p> <p>Question</p> <p>A tabela <code>funcionario_departamento</code> est\u00e1 na 1NF?</p> Sim N\u00e3o Submit <p>Answer</p> <p>Todos os atributos possuem valores at\u00f4micos, n\u00e3o existem arrays ou campos multivalorados.</p> <p>Question</p> <p>A tabela <code>funcionario_departamento</code> est\u00e1 na 2NF?</p> Sim N\u00e3o Submit <p>Answer</p> <p>A chave prim\u00e1ria \u00e9 composta por (<code>id_funcionario</code>, <code>id_departamento</code>)</p> <ul> <li>A depend\u00eancia \\(\\{\\text{id_funcionario}, \\text{id_departamento}\\} \\rightarrow \\{\\text{salario}\\}\\) est\u00e1 correta.  </li> <li>Entretanto, observamos depend\u00eancias parciais:<ul> <li>\\(\\{\\text{id_funcionario}\\} \\rightarrow \\{\\text{nome_funcionario}\\}\\) </li> <li>\\(\\{\\text{id_departamento}\\} \\rightarrow \\{\\text{nome_departamento}\\}\\) </li> </ul> </li> </ul> <p>Como essas depend\u00eancias n\u00e3o dependem da chave composta inteira, ent\u00e3o viola a 2NF.</p>"},{"location":"classes/07-data-modeling/intro-data-modeling/#terceira-forma-normal-3nf","title":"Terceira Forma Normal (<code>3NF</code>)","text":"<p>Uma tabela est\u00e1 na terceira forma normal se satisfaz todas as condi\u00e7\u00f5es da 1NF e 2NF, al\u00e9m de:</p> <ul> <li>Elimina\u00e7\u00e3o de depend\u00eancias transitivas: Todo atributo n\u00e3o-chave deve relacionar-se diretamente com a chave prim\u00e1ria, n\u00e3o atrav\u00e9s de outro atributo</li> </ul> <p>Considere a seguinte tabela  (<code>emprestimo</code>) que registra empr\u00e9stimos de livros em uma biblioteca:</p> id_emprestimo (PK) data_emprestimo id_livro titulo_livro autor 2001 2025-09-01 301 Dom Casmurro Machado A. 2002 2025-09-02 302 A Hora da Estrela Clarice L. 2003 2025-09-03 301 Dom Casmurro Machado A. <p>Info!</p> <p>O objetivo \u00e9 registrar cada empr\u00e9stimo realizado, considerando um livro por empr\u00e9stimo.</p> <p>Question</p> <p>A tabela <code>emprestimo</code> est\u00e1 na 1NF?</p> Sim N\u00e3o Submit <p>Question</p> <p>A tabela <code>emprestimo</code> est\u00e1 na 2NF?</p> Sim N\u00e3o Submit <p>Question</p> <p>A tabela <code>emprestimo</code> est\u00e1 na 3NF?</p> Sim N\u00e3o Submit <p>Answer</p> <p>N\u00e3o!</p> <p>A tabela <code>emprestimo</code> possui depend\u00eancias transitivas:</p> <ul> <li>\\(\\{\\text{id_livro}\\} \\rightarrow \\{\\text{titulo_livro}, \\text{autor}\\}\\)</li> </ul> <p>Isso significa que atributos n\u00e3o-chave dependem de outros atributos n\u00e3o-chave, violando a 3NF.</p> <p>Question</p> <p>Como voc\u00ea faria para normalizar a tabela <code>emprestimo</code> para que esteja na 3NF?</p> Submit <p>Answer</p> <p>Podemos separar a tabela <code>emprestimo</code> em duas tabelas:</p> <p>Tabela 1 \u2014 <code>emprestimo</code></p> id_emprestimo (PK) data_emprestimo id_livro (FK) 2001 2025-09-01 301 2002 2025-09-02 302 2003 2025-09-03 301 <p>Tabela 2 \u2014 <code>livro</code></p> id_livro (PK) titulo_livro autor 301 Dom Casmurro Machado A. 302 A Hora da Estrela Clarice L. <p>A normaliza\u00e7\u00e3o busca garantir a integridade dos dados. Esta caracter\u00edstica torna os modelos relacionais normalizados especialmente adequados para sistemas OLTP (Online Transaction Processing), onde a consist\u00eancia e integridade dos dados s\u00e3o priorit\u00e1rias em rela\u00e7\u00e3o \u00e0 velocidade de consultas anal\u00edticas complexas.</p> <p>Por\u00e9m, manter uma base normalizada na 3NF pode tornar as consultas mais demoradas, pois frequentemente \u00e9 necess\u00e1rio unir (JOIN) m\u00faltiplas tabelas para recuperar os dados desejados. Na pr\u00f3xima p\u00e1gina, exploraremos alternativas mais adequadas, especialmente para OLAP.</p> <p>Exercise</p> <p>Por que a normaliza\u00e7\u00e3o at\u00e9 a terceira forma normal (3NF) \u00e9 considerada o objetivo padr\u00e3o para bancos OLTP?</p> <p>Quais s\u00e3o os trade-offs envolvidos?</p> Submit <p>Answer</p> <p>A 3NF \u00e9 o padr\u00e3o para OLTP porque:</p> <ul> <li>Integridade: Elimina redund\u00e2ncia e inconsist\u00eancias</li> <li>Manutenibilidade: Facilita atualiza\u00e7\u00f5es sem anomalias</li> <li>Consist\u00eancia: Garante que altera\u00e7\u00f5es sejam feitas em um \u00fanico local</li> </ul> <p>Trade-offs:</p> <ul> <li>Performance: Pode exigir mais JOINs para consultas complexas</li> <li>Complexidade: Aumenta o n\u00famero de tabelas no modelo</li> </ul> <p>Exercise</p> <p>Voc\u00ea consegue pensar em um cen\u00e1rio onde a normaliza\u00e7\u00e3o at\u00e9 a terceira forma normal (3NF) n\u00e3o seria ben\u00e9fica, mesmo em bancos OLTP?</p> Submit"},{"location":"classes/07-data-modeling/obt/","title":"One Big Table (OBT)","text":""},{"location":"classes/07-data-modeling/obt/#one-big-table-obt","title":"One Big Table (OBT)","text":""},{"location":"classes/07-data-modeling/obt/#relembrando-a-aula-03","title":"Relembrando a aula 03","text":"<p>Na aula 03, quando fizemos a implementa\u00e7\u00e3o do nosso Data Warehouse (DW), foi preciso decidir como organizar os dados no DW.</p> <p>Exercise</p> <p>Como definimos o modelo de dados do DW?</p> <p>Como ele se diferenciava do modelo de dados operacional (PostgreSQL OLTP)?</p> Submit <p>Answer</p> <p>N\u00f3s removemos as chaves estrangeiras e a obrigatoriedade de alguns atributos.</p> <p>Ademais, a estrutura foi mantida.</p> <p>Exercise</p> <p>O que voc\u00ea acha de mantermos a estrutura do DW quase id\u00eantia \u00e0 do banco operacional?</p> Submit <p>Answer</p> <p>O padr\u00e3o de acesso OLTP geralmente envolve transa\u00e7\u00f5es r\u00e1pidas e de curto prazo, envolvendo poucas linhas.</p> <p>Por outro lado, o DW \u00e9 otimizado para OLAP, envolvendo consultas complexas, acesso colunar e com grande volume de dados.</p> <p>Portanto, manter a estrutura id\u00eantica pode n\u00e3o ser a melhor abordagem para atender a essas diferentes necessidades.</p> <p>Conforme os dados forem disponibilizados para an\u00e1lise no DW, queries complexas podem demandar a jun\u00e7\u00e3o de diversas tabelas, as quais podem conter grande volume de dados. Isto ser\u00e1 computacionalmente custoso, especialmente quando o DW crescer em volume de dados e complexidade estrutural.</p> <p>Uma solu\u00e7\u00e3o poss\u00edvel \u00e9 o uso do padr\u00e3o One Big Table (OBT).</p>"},{"location":"classes/07-data-modeling/obt/#o-que-e-one-big-table-obt","title":"O que \u00e9 One Big Table (OBT)?","text":"<p>A abordagem One Big Table (OBT) representa uma estrat\u00e9gia de modelagem onde todos os dados relevantes para an\u00e1lise s\u00e3o consolidados em uma \u00fanica tabela desnormalizada.</p> <p>Info</p> <p>Desnormaliza\u00e7\u00e3o \u00e9 o processo de, intencionalmente, adicionar redund\u00e2ncia a um banco de dados previamente normalizado para melhorar o desempenho de leitura, atrav\u00e9s da combina\u00e7\u00e3o de tabelas ou duplica\u00e7\u00e3o de dados!</p> <p>Esta t\u00e9cnica surgiu como uma resposta \u00e0s limita\u00e7\u00f5es de performance em consultas anal\u00edticas que envolvem m\u00faltiplos <code>JOINs</code> entre tabelas normalizadas.</p> <p>Resumo!</p> <p>Em um modelo OBT, ao inv\u00e9s de manter dados distribu\u00eddos em v\u00e1rias tabelas relacionais, todas as informa\u00e7\u00f5es necess\u00e1rias s\u00e3o combinadas em uma \u00fanica estrutura tabular.</p> <p>Isso elimina a necessidade de opera\u00e7\u00f5es custosas de <code>JOIN</code> durante as consultas anal\u00edticas.</p>"},{"location":"classes/07-data-modeling/obt/#exemplo","title":"Exemplo","text":"<p>Vamos ilustrar o conceito de OBT com um exemplo de dados hospitalares.</p>"},{"location":"classes/07-data-modeling/obt/#tabelas-originais","title":"Tabelas Originais","text":""},{"location":"classes/07-data-modeling/obt/#tabela-paciente","title":"Tabela: <code>paciente</code>","text":"id_paciente nome uf 1 Carla Dias SP 2 Lucas Prado RJ"},{"location":"classes/07-data-modeling/obt/#tabela-medico","title":"Tabela: <code>medico</code>","text":"id_medico nome_medico especialidade 10 Dr. Silva Cardiologia 20 Dra. Souza Pediatria"},{"location":"classes/07-data-modeling/obt/#tabela-atendimento","title":"Tabela: <code>atendimento</code>","text":"id_atendimento id_paciente id_medico data procedimento 1001 1 10 2025-08-01 Consulta 1002 2 20 2025-08-02 Exame 1003 1 20 2025-08-03 Vacina"},{"location":"classes/07-data-modeling/obt/#tabela-obt-resultante","title":"Tabela OBT Resultante","text":"id_atendimento data nome_paciente uf nome_medico especialidade procedimento 1001 2025-08-01 Carla Dias SP Dr. Silva Cardiologia Consulta 1002 2025-08-02 Lucas Prado RJ Dra. Souza Pediatria Exame 1003 2025-08-03 Carla Dias SP Dra. Souza Pediatria Vacina <p>Assim, todas as informa\u00e7\u00f5es relevantes para an\u00e1lise de atendimentos est\u00e3o consolidadas em uma \u00fanica tabela, eliminando a necessidade de JOINs.</p> <p>Conceito Fundamental</p> <p>O OBT prioriza performance de leitura sobre otimiza\u00e7\u00e3o de armazenamento, sendo especialmente \u00fatil em cen\u00e1rios de OLAP onde a velocidade das consultas \u00e9 cr\u00edtica.</p>"},{"location":"classes/07-data-modeling/obt/#contexto-historico","title":"Contexto Hist\u00f3rico","text":"<p>O conceito de OBT ganhou popularidade com o advento do Big Data e ferramentas de processamento distribu\u00eddo como Hadoop e Spark. Com a queda do custo de armazenamento e a necessidade crescente de an\u00e1lises em tempo real, a redund\u00e2ncia de dados tornou-se um trade-off aceit\u00e1vel em muitos cen\u00e1rios.</p> <p>A abordagem tamb\u00e9m se tornou vi\u00e1vel com o desenvolvimento de sistemas de armazenamento colunar como Parquet, que consegue comprimir dados redundantes de forma eficiente, minimizando o impacto do aumento no volume de dados.</p>"},{"location":"classes/07-data-modeling/obt/#vantagens-da-abordagem-obt","title":"Vantagens da Abordagem OBT","text":"<p>Dentre as vantagens do uso de OBT, podemos citar:</p> <ul> <li> <p>Performance de Consultas: A principal vantagem do OBT \u00e9 a elimina\u00e7\u00e3o de JOINs complexos. Em um DW tradicional, uma consulta simples pode requerer jun\u00e7\u00f5es entre m\u00faltiplas tabelas, criando gargalos de performance significativos.</p> </li> <li> <p>Simplicidade de Consultas: Analistas e cientistas de dados podem escrever consultas mais diretas, sem precisar compreender rela\u00e7\u00f5es complexas entre tabelas ou memorizar esquemas de jun\u00e7\u00e3o.</p> </li> <li> <p>Otimiza\u00e7\u00e3o para Ferramentas de BI: Muitas ferramentas Business Intelligence trabalham melhor com estruturas desnormalizadas, aproveitando otimiza\u00e7\u00f5es internas quando os dados est\u00e3o em formato flat.</p> </li> <li> <p>Paraleliza\u00e7\u00e3o: Em ambientes distribu\u00eddos, consultas em uma \u00fanica tabela podem ser facilmente paralelizadas e distribu\u00eddas entre m\u00faltiplos n\u00f3s de processamento.</p> </li> </ul> <p>Question</p> <p>Qual \u00e9 a principal vantagem de performance do modelo OBT em rela\u00e7\u00e3o a modelos normalizados?</p> Menor uso de mem\u00f3ria Elimina\u00e7\u00e3o de opera\u00e7\u00f5es JOIN Menor volume de dados Maior seguran\u00e7a dos dados Submit"},{"location":"classes/07-data-modeling/obt/#desvantagens-da-abordagem-obt","title":"Desvantagens da Abordagem OBT","text":"<p>Mas o uso de OBT tamb\u00e9m apresenta desvantagens que devem ser consideradas. Dentre elas:</p> <ul> <li> <p>Redund\u00e2ncia de Dados: A desnormaliza\u00e7\u00e3o resulta em duplica\u00e7\u00e3o massiva de informa\u00e7\u00f5es. Dados que antes apareciam uma \u00fanica vez em tabelas normalizadas agora s\u00e3o replicados em cada linha da OBT.</p> </li> <li> <p>Maior Consumo de Armazenamento: O volume de dados pode crescer significativamente, resultando em maiores custos de armazenamento e backup.</p> </li> <li> <p>Maior Complexidade de Atualiza\u00e7\u00e3o: Quando um dado mestre \u00e9 alterado (como o nome de um cliente), essa mudan\u00e7a precisa ser propagada para todas as linhas da OBT onde esse cliente aparece, tornando as atualiza\u00e7\u00f5es mais complexas e custosas.</p> </li> </ul> <p>Info!</p> <p>Em muitos casos, a OBT \u00e9 append-only.</p> <ul> <li> <p>Potencial Inconsist\u00eancia: A redund\u00e2ncia aumenta o risco de inconsist\u00eancias, especialmente se o processo de ETL n\u00e3o for bem projetado ou se houver falhas na sincroniza\u00e7\u00e3o dos dados.</p> </li> <li> <p>Gest\u00e3o de Migra\u00e7\u00f5es: Altera\u00e7\u00f5es no modelo de dados podem exigir migra\u00e7\u00f5es complexas, uma vez que a estrutura desnormalizada pode n\u00e3o se adaptar facilmente a mudan\u00e7as.</p> </li> </ul>"},{"location":"classes/07-data-modeling/obt/#quando-usar-obt","title":"Quando Usar OBT?","text":"<p>A decis\u00e3o de implementar um OBT deve considerar diversos fatores:</p> <ul> <li> <p>Cen\u00e1rios Adequados:</p> <ul> <li>An\u00e1lises frequentes que sempre envolvem os mesmos conjuntos de dados</li> <li>Relat\u00f3rios padronizados com consultas previs\u00edveis</li> <li>Ambientes onde a performance de leitura \u00e9 mais cr\u00edtica que efici\u00eancia de armazenamento</li> <li>Data marts espec\u00edficos para \u00e1reas de neg\u00f3cio</li> </ul> </li> <li> <p>Cen\u00e1rios Inadequados:</p> <ul> <li>Dados com alta frequ\u00eancia de atualiza\u00e7\u00e3o</li> <li>Ambientes com recursos limitados de armazenamento</li> <li>Casos onde a consist\u00eancia transacional \u00e9 cr\u00edtica</li> <li>Sistemas principalmente OLTP</li> </ul> </li> </ul>"},{"location":"classes/07-data-modeling/obt/#implementando-obt","title":"Implementando OBT","text":"<p>Vamos trabalhar com a transforma\u00e7\u00e3o do nosso modelo de vendas normalizado em uma OBT. Primeiro, vamos relembrar a estrutura original:</p> <p></p>"},{"location":"classes/07-data-modeling/obt/#projetando-a-obt","title":"Projetando a OBT","text":"<p>Para criar nossa OBT, precisamos identificar qual ser\u00e1 o gr\u00e3o (granularidade) da tabela.</p> <p>No nosso caso, o gr\u00e3o natural seria cada item vendido, pois representa o n\u00edvel mais detalhado de informa\u00e7\u00e3o dispon\u00edvel.</p> <p>Exercise</p> <p>Por que escolhemos \"item vendido\" como gr\u00e3o da OBT ao inv\u00e9s de \"venda\"?</p> Submit <p>Answer</p> <p>Porque uma venda pode conter m\u00faltiplos produtos.</p> <p>Se us\u00e1ssemos \"venda\" como gr\u00e3o, perderiamos o detalhamento por produto!</p>"},{"location":"classes/07-data-modeling/obt/#campos-da-obt-de-vendas","title":"Campos da OBT de Vendas","text":"<p>Nossa OBT incluir\u00e1 todos os campos necess\u00e1rios para an\u00e1lises de vendas, eliminando a necessidade de JOINs:</p> <ul> <li> <p>Informa\u00e7\u00f5es da Venda: <code>id_venda</code>, <code>data_venda</code>, <code>entregue</code>, <code>valor_total_venda</code></p> </li> <li> <p>Informa\u00e7\u00f5es do Item: <code>id_item</code>, <code>quantidade</code>, <code>valor_unitario</code>, <code>valor_total_item</code></p> </li> <li> <p>Informa\u00e7\u00f5es do Produto: <code>id_produto</code>, <code>nome_produto</code>, <code>descricao_produto</code>, <code>preco_base</code></p> </li> <li> <p>Informa\u00e7\u00f5es do Cliente: <code>id_cliente</code>, <code>nome_cliente</code>, <code>data_nasc</code>, <code>data_cad</code>, <code>cpf</code></p> </li> <li> <p>Informa\u00e7\u00f5es Geogr\u00e1ficas: <code>cidade</code>, <code>uf</code>, <code>pais</code></p> </li> </ul> <p>Question</p> <p>Quantas vezes os dados do cliente \"Jo\u00e3o Silva\" aparecer\u00e3o na OBT se ele fez 5 compras com 3 itens cada?</p> 1 vez 5 vezes 3 vezes 15 vezes Depende dos produtos Submit"},{"location":"classes/07-data-modeling/obt/#criando-a-obt-na-pratica","title":"Criando a OBT na Pr\u00e1tica","text":"<p>A transforma\u00e7\u00e3o do modelo normalizado para OBT, usando SQL, pode ser representada pela seguinte query:</p> <p>Aten\u00e7\u00e3o</p> <p>Esta \u00e9 uma vers\u00e3o refatorada, o nome dos atributos (colunas) pode n\u00e3o bater com o utilizado na aula 03.</p> <pre><code>    -- Cria\u00e7\u00e3o da OBT de Vendas\n    CREATE TABLE obt_vendas AS\n    SELECT \n        -- Informa\u00e7\u00f5es da venda\n        v.id_venda,\n        v.data as data_venda,\n        v.entregue,\n        v.valor_total as valor_total_venda,\n\n        -- Informa\u00e7\u00f5es do item\n        iv.id_item,\n        iv.quantidade,\n        iv.valor_unitario,\n        iv.valor_total as valor_total_item,\n\n        -- Informa\u00e7\u00f5es do produto\n        p.id_produto,\n        p.nome as nome_produto,\n        p.descricao as descricao_produto,\n        p.preco_base,\n        p.ativo as produto_ativo,\n\n        -- Informa\u00e7\u00f5es do cliente\n        c.id_cliente,\n        c.nome as nome_cliente,\n        c.data_nasc,\n        c.data_cad,\n        c.cpf,\n\n        -- Informa\u00e7\u00f5es geogr\u00e1ficas\n        cid.cidade_desc as cidade,\n        cid.cidade_uf as uf,\n        pa.pais\n\n    FROM item_venda iv\n    JOIN venda v ON iv.id_venda = v.id_venda\n    JOIN produto p ON iv.id_produto = p.id_produto\n    JOIN cliente c ON v.id_cliente = c.id_cliente\n    JOIN cidade cid ON c.id_cidade = cid.id_cidade\n    JOIN pais pa ON cid.id_pais = pa.id_pais;\n</code></pre>"},{"location":"classes/07-data-modeling/obt/#consultas-normalizado-vs-obt","title":"Consultas: Normalizado vs OBT","text":"<p>Podemos comparar como seria a estrutura b\u00e1sica de consultas no modelo normalizado e na OBT.</p> <p>Por exemplo, para obter as vendas por pa\u00eds no \u00faltimo m\u00eas, ter\u00edamos:</p> NormalizadoOBT <pre><code>-- Vendas por pa\u00eds no \u00faltimo m\u00eas\nSELECT \n    pa.pais,\n    COUNT(*) as total_vendas,\n    SUM(v.valor_total) as receita_total\nFROM venda v\nJOIN cliente c ON v.id_cliente = c.id_cliente\nJOIN cidade cid ON c.id_cidade = cid.id_cidade\nJOIN pais pa ON cid.id_pais = pa.id_pais\nWHERE v.data &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)\nGROUP BY pa.pais\nORDER BY receita_total DESC;\n</code></pre> <pre><code>-- Vendas por pa\u00eds no \u00faltimo m\u00eas\nSELECT \n    pais,\n    COUNT(DISTINCT id_venda) as total_vendas,\n    SUM(valor_total_venda) as receita_total\nFROM obt_vendas\nWHERE data_venda &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)\nGROUP BY pais\nORDER BY receita_total DESC;\n</code></pre> <p>Exercise</p> <p>Compare as duas consultas acima. Quais diferen\u00e7as voc\u00ea observa em termos de complexidade e potencial performance?</p> Submit <p>Answer</p> <p>A consulta OBT \u00e9 mais simples (sem JOINs), potencialmente mais r\u00e1pida, mas requer aten\u00e7\u00e3o especial ao usar <code>COUNT(DISTINCT id_venda)</code> para evitar contar a mesma venda m\u00faltiplas vezes devido aos m\u00faltiplos itens.</p> <p>Otimiza\u00e7\u00f5es!</p> <p>Como a OBT ser\u00e1 consultada frequentemente, \u00e9 fundamental criar \u00edndices nas colunas mais utilizadas em filtros e agrupamentos:</p> <p>Explos\u00e3o de Cardinalidade!</p> <p>Em relacionamentos muitos-para-muitos, a OBT pode crescer exponencialmente.</p> <p>\u00c9 importante monitorar o tamanho da tabela resultante.</p> <p>Cuidado!</p> <p>Ao fazer agrega\u00e7\u00f5es na OBT, tenha cuidado para n\u00e3o contar duplicatas:</p> <pre><code>-- INCORRETO: conta o mesmo cliente m\u00faltiplas vezes\nSELECT COUNT(*) as total_clientes FROM obt_vendas;\n\n-- CORRETO: conta clientes \u00fanicos\nSELECT COUNT(DISTINCT id_cliente) as total_clientes FROM obt_vendas;\n</code></pre> <p>Manuten\u00e7\u00e3o de Consist\u00eancia</p> <p>A redund\u00e2ncia exige processos robustos de ETL para garantir que todas as c\u00f3pias dos dados permane\u00e7am consistentes.</p> <p>Exercise</p> <p>Que estrat\u00e9gias voc\u00ea implementaria para detectar e corrigir inconsist\u00eancias em uma OBT?</p> Submit <p>Answer</p> <ul> <li>Monitoramento cont\u00ednuo de dados para identificar anomalias.</li> <li>Implementa\u00e7\u00e3o de testes automatizados para validar a integridade dos dados.</li> <li>Processos de reconcilia\u00e7\u00e3o peri\u00f3dicos entre a OBT e as fontes de dados originais.</li> </ul>"},{"location":"classes/07-data-modeling/references/","title":"Refer\u00eancias","text":"<ul> <li>DDA. Serra, James. Deciphering Data Architectures. \" O'Reilly Media, Inc.\", 2024.</li> <li>DDIA. Kleppmann, M. (2017). Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. Estados Unidos: O'Reilly Media.</li> <li>FDE. Reis, J., Housley, M. (2022). Fundamentals of Data Engineering: Plan and Build Robust Data Systems. Estados Unidos: O'Reilly Media.</li> </ul>"},{"location":"classes/07-data-modeling/snowflake/","title":"Snowflake Schema","text":""},{"location":"classes/07-data-modeling/snowflake/#snowflake-schema","title":"Snowflake Schema","text":""},{"location":"classes/07-data-modeling/snowflake/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Ap\u00f3s explorarmos o Star Schema na se\u00e7\u00e3o anterior, vamos conhecer uma varia\u00e7\u00e3o que busca equilibrar ainda mais performance e normaliza\u00e7\u00e3o: o Snowflake Schema.</p> <p>O Snowflake Schema surge da necessidade de reduzir a redund\u00e2ncia presente nas dimens\u00f5es do Star Schema, especialmente quando lidamos com hierarquias complexas e m\u00faltiplos n\u00edveis de relacionamento.</p> <p>Exercise</p> <p>Pensando nas dimens\u00f5es do Star Schema que implementamos (especialmente a dimens\u00e3o cliente que inclu\u00eda cidade, UF e pa\u00eds), que problemas de redund\u00e2ncia voc\u00ea identifica?</p> Submit"},{"location":"classes/07-data-modeling/snowflake/#o-que-e-snowflake-schema","title":"O que \u00e9 Snowflake Schema?","text":"<p>O Snowflake Schema \u00e9 uma extens\u00e3o do Star Schema onde as tabelas dimens\u00e3o s\u00e3o normalizadas, eliminando redund\u00e2ncias atrav\u00e9s da cria\u00e7\u00e3o de tabelas de hierarquia.</p> <p>Ao inv\u00e9s de manter todas as informa\u00e7\u00f5es de uma dimens\u00e3o em uma \u00fanica tabela desnormalizada, o Snowflake Schema separa os diferentes n\u00edveis hier\u00e1rquicos em tabelas distintas.</p> <pre><code>graph LR\n    subgraph \"Snowflake Schema - Vendas\"\n        Pais[\ud83c\udf0d dim_pais&lt;br/&gt;id_pais&lt;br/&gt;nome_pais]\n        Estado[\ud83d\uddfa\ufe0f dim_estado&lt;br/&gt;id_estado&lt;br/&gt;uf&lt;br/&gt;nome_estado&lt;br/&gt;id_pais]\n        Cidade[\ud83c\udfd9\ufe0f dim_cidade&lt;br/&gt;id_cidade&lt;br/&gt;nome_cidade&lt;br/&gt;id_estado]\n\n        Cliente[\ud83d\udc64 dim_cliente&lt;br/&gt;id_cliente&lt;br/&gt;nome&lt;br/&gt;data_nasc&lt;br/&gt;cpf&lt;br/&gt;id_cidade]\n\n        Categoria[\ud83d\udcc2 dim_categoria&lt;br/&gt;id_categoria&lt;br/&gt;nome_categoria]\n        Produto[\ud83d\udce6 dim_produto&lt;br/&gt;id_produto&lt;br/&gt;nome&lt;br/&gt;preco_base&lt;br/&gt;id_categoria]\n\n        Tempo[\ud83d\udcc5 dim_tempo&lt;br/&gt;data&lt;br/&gt;ano&lt;br/&gt;mes&lt;br/&gt;trimestre]\n\n        Fato[\ud83c\udfaf fato_vendas&lt;br/&gt;id_venda&lt;br/&gt;id_cliente&lt;br/&gt;id_produto&lt;br/&gt;data&lt;br/&gt;quantidade&lt;br/&gt;valor_total]\n\n        Pais --&gt; Estado\n        Estado --&gt; Cidade\n        Cidade --&gt; Cliente\n        Categoria --&gt; Produto\n\n        Cliente --&gt; Fato\n        Produto --&gt; Fato\n        Tempo --&gt; Fato\n    end</code></pre> <p>Dentre as motivac\u00f5es para as altera\u00e7\u00f5es propostas pelo Snowflake, podemos citar a redu\u00e7\u00e3o de redund\u00e2ncia e gest\u00e3o da atualiza\u00e7\u00e3o dos dados.</p> <p>No Star Schema, informa\u00e7\u00f5es como pa\u00eds e estado s\u00e3o repetidas para cada cidade:</p> id_cliente nome cidade uf pais 1 Jo\u00e3o S\u00e3o Paulo SP Brasil 2 Maria Campinas SP Brasil 3 Pedro Santos SP Brasil <p>Inconsist\u00eancias Potenciais</p> <p>A redund\u00e2ncia aumenta o risco de inconsist\u00eancias.</p> <p>Por exemplo, algu\u00e9m poderia inadvertidamente cadastrar \"S\u00e3o Paulo, RJ, Brasil\".</p> <p>Ainda, mudan\u00e7as em dados hier\u00e1rquicos (como renomear um estado) requerem atualiza\u00e7\u00f5es em m\u00faltiplas linhas.</p>"},{"location":"classes/07-data-modeling/snowflake/#vantagens-do-snowflake-schema","title":"Vantagens do Snowflake Schema","text":"<ul> <li> <p>Elimina\u00e7\u00e3o de Redund\u00e2ncia: Cada informa\u00e7\u00e3o \u00e9 armazenada apenas uma vez, reduzindo significativamente o espa\u00e7o de armazenamento em dimens\u00f5es com hierarquias profundas.</p> </li> <li> <p>Integridade Referencial: A normaliza\u00e7\u00e3o garante consist\u00eancia de dados hier\u00e1rquicos. \u00c9 imposs\u00edvel ter inconsist\u00eancias como \"S\u00e3o Paulo, RJ, Brasil\".</p> </li> <li> <p>Facilidade de Manuten\u00e7\u00e3o: Mudan\u00e7as em dados mestres (como renomear um estado) requerem atualiza\u00e7\u00f5es em apenas uma tabela.</p> </li> <li> <p>Flexibilidade Hier\u00e1rquica: Novas hierarquias podem ser facilmente adicionadas sem impacto nas estruturas existentes.</p> </li> </ul>"},{"location":"classes/07-data-modeling/snowflake/#desvantagens-do-snowflake-schema","title":"Desvantagens do Snowflake Schema","text":"<ul> <li> <p>Complexidade de Consultas: Consultas anal\u00edticas requerem mais JOINs, tornando-se mais complexas de escrever e entender.</p> </li> <li> <p>Performance de Consultas: O maior n\u00famero de JOINs pode impactar negativamente a performance, especialmente em sistemas com grandes volumes de dados.</p> </li> <li> <p>Menor Compatibilidade com Ferramentas BI: Algumas ferramentas de Business Intelligence s\u00e3o otimizadas para Star Schema ou OBT e podem n\u00e3o funcionar otimamente com estruturas muito normalizadas.</p> </li> </ul> <p>Quando Usar Snowflake Schema?</p> <p>Cen\u00e1rios Adequados:</p> <ul> <li>Hierarquias complexas e profundas (geografia, organizacional, produtos)</li> <li>Dimens\u00f5es com alta cardinalidade e redund\u00e2ncia significativa</li> <li>Ambientes onde consist\u00eancia de dados \u00e9 cr\u00edtica</li> <li>Recursos limitados de armazenamento</li> </ul> <p>Cen\u00e1rios Inadequados:</p> <ul> <li>Performance cr\u00edtica em consultas frequentes e simples</li> <li>Ambientes com ferramentas BI legadas que n\u00e3o suportam m\u00faltiplos JOINs</li> <li>Hierarquias rasas onde a redund\u00e2ncia \u00e9 m\u00ednima</li> <li>Equipes com pouca experi\u00eancia em SQL complexo</li> </ul>"},{"location":"classes/07-data-modeling/snowflake/#comparacao-final-escolhendo-o-modelo-adequado","title":"Compara\u00e7\u00e3o Final: Escolhendo o Modelo Adequado","text":"Crit\u00e9rio Normalizado Snowflake Star OBT Complexidade de consulta Muito Alta Alta M\u00e9dia Baixa Performance de leitura Baixa M\u00e9dia Boa Excelente Redund\u00e2ncia de dados Nenhuma Baixa M\u00e9dia Alta Integridade de dados Excelente Boa M\u00e9dia Baixa Facilidade para analistas Baixa M\u00e9dia Alta Muito Alta Custo de armazenamento Baixo Baixo-M\u00e9dio M\u00e9dio Alto Complexidade de manuten\u00e7\u00e3o Alta M\u00e9dia M\u00e9dia-Baixa Baixa <p>Question</p> <p>Para um dashboard executivo que \u00e9 consultado poucas vezes por dia mas precisa de alta performance, qual modelo seria ideal?</p> Modelo normalizado para garantir consist\u00eancia Snowflake Schema para economia de espa\u00e7o Star Schema para equilibrar performance e manuten\u00e7\u00e3o OBT para m\u00e1xima performance Submit"},{"location":"classes/07-data-modeling/snowflake/#abordagem-pratica","title":"Abordagem pr\u00e1tica","text":"<p>N\u00e3o \u00e9 tarefa f\u00e1cil escolher o modelo ideal e n\u00e3o existe resposta m\u00e1gica.</p> <p>Do ponto de vista de evolu\u00e7\u00e3o e matura\u00e7\u00e3o das \u00e1reas de analytics das empresas, \u00e9 comum que iniciem com a replica\u00e7\u00e3o do modelo relacional.</p> <p>Quando as primeiras dores de performance e usabilidade aparecem, migra-se para um star schema.</p> <p>Em algumas empresas mais preocupadas com governan\u00e7a ou padroniza\u00e7\u00e3o, o star schema evolui para snowflake (dimens\u00f5es mais normalizadas). Casos comuns para isto incluem:</p> <ul> <li>Quando o dom\u00ednio \u00e9 muito grande (por exemplo, varejo global com hierarquias complexas).</li> <li>H\u00e1 necessidade de evitar redund\u00e2ncia em dimens\u00f5es.</li> </ul> <p>Muitas vezes, uma abordagem h\u00edbrida \u00e9 adotada, onde diferentes partes do data warehouse utilizam modelos distintos conforme suas necessidades espec\u00edficas (star schema + OBT)!</p> <p>Ainda, podemos considerar a utiliza\u00e7\u00e3o de views materializadas ou caches para otimizar o desempenho de consultas em modelos mais complexos. Neste cen\u00e1rio, o star schema pode ser escolhido como modelo central, e o OBT, implementado como views materializadas, para \u00e1reas cr\u00edticas de desempenho.</p>"},{"location":"classes/07-data-modeling/star-schema/","title":"Star Schema","text":""},{"location":"classes/07-data-modeling/star-schema/#star-schema","title":"Star Schema","text":""},{"location":"classes/07-data-modeling/star-schema/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Na se\u00e7\u00e3o anterior, exploramos o conceito de One Big Table (OBT), que prop\u00f5e uma estrat\u00e9gia de desnormaliza\u00e7\u00e3o completa.</p> <p>Agora, vamos estudar uma abordagem mais equilibrada: o Star Schema!</p> <p>O Star Schema representa um meio-termo entre a normaliza\u00e7\u00e3o completa dos sistemas OLTP e a desnormaliza\u00e7\u00e3o total da OBT, oferecendo uma estrutura otimizada para consultas anal\u00edticas.</p> <p>Exercise</p> <p>Considerando as limita\u00e7\u00f5es da OBT discutidas na aula anterior (como redund\u00e2ncia e complexidade de atualiza\u00e7\u00e3o), como voc\u00ea imagina que um modelo \"meio-termo\" poderia resolver alguns desses problemas?</p> Submit <p>Answer</p> <p>Um modelo \"meio-termo\" poderia resolver alguns dos problemas da OBT ao:</p> <ul> <li>Reduzir Redund\u00e2ncia: Separando parte dos dados em tabelas, evitando duplica\u00e7\u00e3o excessiva.</li> <li>Facilitar Atualiza\u00e7\u00f5es: Com menos redund\u00e2ncia, atualiza\u00e7\u00f5es em dados seriam mais simples e menos propensas a erros.</li> <li>Otimizar Consultas: Estruturando os dados para consultas anal\u00edticas comuns, melhorando a performance sem sacrificar a integridade.</li> </ul>"},{"location":"classes/07-data-modeling/star-schema/#o-que-e-star-schema","title":"O que \u00e9 Star Schema?","text":"<p>O Star Schema \u00e9 um modelo de dados dimensional que organiza informa\u00e7\u00f5es em duas categorias principais:</p> <ul> <li>Tabela Fato (Fact Table): cont\u00e9m as m\u00e9tricas e medidas num\u00e9ricas do neg\u00f3cio</li> <li>Tabelas Dimens\u00e3o (Dimension Tables): cont\u00eam os atributos descritivos e contextuais</li> </ul> <p>A estrutura recebe esse nome porque, quando visualizada em um diagrama, lembra uma estrela: a tabela fato no centro conectada \u00e0s dimens\u00f5es ao redor:</p> <pre><code>graph LR\n    subgraph \"Star Schema - Vendas\"\n        Produto[\ud83d\udce6 dim_produto&lt;br/&gt;id_produto&lt;br/&gt;nome&lt;br/&gt;categoria&lt;br/&gt;preco_base] \n        Cliente[\ud83d\udc64 dim_cliente&lt;br/&gt;id_cliente&lt;br/&gt;nome&lt;br/&gt;idade&lt;br/&gt;cidade&lt;br/&gt;uf&lt;br/&gt;pais]\n        Tempo[\ud83d\udcc5 dim_tempo&lt;br/&gt;data&lt;br/&gt;ano&lt;br/&gt;mes&lt;br/&gt;trimestre&lt;br/&gt;dia_semana]\n\n        Fato[\ud83c\udfaf fato_vendas&lt;br/&gt;id_venda&lt;br/&gt;id_cliente&lt;br/&gt;id_produto&lt;br/&gt;data&lt;br/&gt;quantidade&lt;br/&gt;valor_unitario&lt;br/&gt;valor_total]\n\n        Cliente --&gt; Fato\n        Produto --&gt; Fato\n        Tempo --&gt; Fato\n    end</code></pre>"},{"location":"classes/07-data-modeling/star-schema/#contexto-historico","title":"Contexto Hist\u00f3rico","text":"<p>O Star Schema foi formalizado por Ralph Kimball na d\u00e9cada de 1990 como parte da metodologia Kimball para DW.</p> <p>Kimball observou que as consultas anal\u00edticas t\u00edpicas seguiam padr\u00f5es previs\u00edveis: geralmente envolviam agrega\u00e7\u00f5es num\u00e9ricas (vendas, quantidades, valores) agrupadas por atributos descritivos (tempo, geografia, produtos).</p> <p>O que vs como!</p> <p>Esta observa\u00e7\u00e3o levou ao desenvolvimento de uma estrutura que separa claramente \"o que medir\" (fatos) de \"como agrupar\" (dimens\u00f5es), otimizando ambos para seus respectivos prop\u00f3sitos.</p> <p>A abordagem dimensional de Kimball contrastava com a metodologia Inmon, que defendia estruturas mais normalizadas. Esta diferen\u00e7a gerou o famoso debate \"Kimball vs Inmon\" na comunidade de DW.</p> <p>Metodologias Cl\u00e1ssicas</p> <ul> <li>Kimball (dimensional): foco na facilidade de uso para analistas</li> <li>Inmon (normalizada): foco na integridade e consist\u00eancia dos dados</li> </ul> <p>Vamos explorar um pouco mais os conceitos de tabelas fato e tabelas dimens\u00e3o.</p>"},{"location":"classes/07-data-modeling/star-schema/#tabela-fato","title":"Tabela Fato","text":"<p>A tabela fato \u00e9 o cora\u00e7\u00e3o do Star Schema. Ela cont\u00e9m:</p> <ul> <li>Chaves estrangeiras para as dimens\u00f5es</li> <li>Medidas num\u00e9ricas (m\u00e9tricas de neg\u00f3cio)</li> <li>Chaves compostas quando necess\u00e1rio</li> </ul> <p>No nosso exemplo de vendas, a tabela fato conteria:</p> <pre><code>CREATE TABLE fato_vendas (\n    id_venda INT,\n    id_item INT,\n    id_cliente INT,\n    id_produto INT,\n    data_venda DATE,\n\n    -- Medidas (sempre num\u00e9ricas)\n    quantidade INT,\n    valor_unitario DECIMAL(10,2),\n    valor_total DECIMAL(10,2),\n\n    -- Chave prim\u00e1ria composta\n    PRIMARY KEY (id_venda, id_item)\n);\n</code></pre>"},{"location":"classes/07-data-modeling/star-schema/#tabelas-dimensao","title":"Tabelas Dimens\u00e3o","text":"<p>As tabelas dimens\u00e3o fornecem contexto descritivo para as medidas. Elas s\u00e3o tipicamente desnormalizadas para facilitar consultas.</p> <p>Por exemplo:</p> Dimens\u00e3o ClienteDimens\u00e3o ProdutoDimens\u00e3o Tempo <pre><code>CREATE TABLE dim_cliente (\n    id_cliente INT PRIMARY KEY,\n    nome VARCHAR(60),\n    data_nasc DATE,\n    data_cad DATE,\n    cpf VARCHAR(15),\n    idade_atual INT,\n    faixa_etaria VARCHAR(20),\n\n    -- Informa\u00e7\u00f5es geogr\u00e1ficas desnormalizadas\n    cidade VARCHAR(60),\n    uf VARCHAR(2),\n    pais VARCHAR(60)\n);\n</code></pre> <pre><code>CREATE TABLE dim_produto (\n    id_produto INT PRIMARY KEY,\n    nome VARCHAR(100),\n    descricao TEXT,\n    preco_base DECIMAL(10,2),\n    ativo BOOLEAN,\n\n    -- Categoriza\u00e7\u00e3o\n    categoria VARCHAR(50),\n    subcategoria VARCHAR(50)\n);\n</code></pre> <p>A dimens\u00e3o tempo \u00e9 fundamental em praticamente todos os data warehouses:</p> <pre><code>CREATE TABLE dim_tempo (\n    data DATE PRIMARY KEY,\n    ano INT,\n    mes INT,\n    mes_nome VARCHAR(20),\n    trimestre INT,\n    semestre INT,\n    dia_mes INT,\n    dia_ano INT,\n    dia_semana INT,\n    dia_semana_nome VARCHAR(20),\n    eh_feriado BOOLEAN,\n    eh_fim_semana BOOLEAN\n);\n</code></pre> <p>Question</p> <p>Por que a dimens\u00e3o tempo geralmente \u00e9 pr\u00e9-populada com todos os dias do ano ao inv\u00e9s de ser populada conforme as vendas ocorrem?</p> Para economizar espa\u00e7o Para facilitar an\u00e1lises de per\u00edodos sem vendas Por quest\u00f5es de performance \u00c9 uma obriga\u00e7\u00e3o do modelo dimensional Submit"},{"location":"classes/07-data-modeling/star-schema/#vantagens-do-star-schema","title":"Vantagens do Star Schema","text":"<p>Dentre as vantagens do Star Schema, podemos destacar:</p> <ul> <li> <p>Equilibrio Entre Performance e Manutenibilidade: O Star Schema oferece boa performance para consultas anal\u00edticas (menos JOINs que modelos normalizados) mantendo facilidade de manuten\u00e7\u00e3o (menos redund\u00e2ncia que OBT).</p> </li> <li> <p>Intuitividade para Analistas: A estrutura \u00e9 intuitiva para usu\u00e1rios de neg\u00f3cio, separando claramente \"medidas\" (o que medir) de \"dimens\u00f5es\" (como agrupar).</p> </li> <li> <p>Compatibilidade com Ferramentas de BI: A maioria das ferramentas de OLAP \u00e9 otimizada para trabalhar com Star Schemas, oferecendo recursos autom\u00e1ticos de drill-down e roll-up.</p> </li> <li> <p>Flexibilidade Anal\u00edtica: Novas an\u00e1lises podem ser facilmente implementadas combinando diferentes dimens\u00f5es sem modificar a estrutura b\u00e1sica.</p> </li> </ul> <p>Exercise</p> <p>Compare as vantagens do Star Schema com as da OBT. Em que situa\u00e7\u00f5es cada abordagem seria mais adequada?</p> Submit <p>Exercise</p> <p>Quais s\u00e3o as principais diferen\u00e7as em termos de redund\u00e2ncia de dados entre Star Schema, modelo normalizado (3NF) e OBT?</p> Submit"},{"location":"classes/07-data-modeling/star-schema/#implementando-star-schema-para-vendas","title":"Implementando Star Schema para Vendas","text":""},{"location":"classes/07-data-modeling/star-schema/#tabelas-dimensao_1","title":"Tabelas Dimens\u00e3o","text":"<p>Aten\u00e7\u00e3o</p> <p>Esta \u00e9 uma vers\u00e3o refatorada, o nome dos atributos (colunas) pode n\u00e3o bater com o utilizado na aula 03.</p> <p>A cria\u00e7\u00e3o da dimens\u00e3o cliente, incluindo campos calculados \u00fateis para an\u00e1lise, poderia ser realizada pelo uso de uma query semelhante a:</p> <pre><code>CREATE TABLE dim_cliente AS\nSELECT \n    c.id_cliente,\n    c.nome,\n    c.data_nasc,\n    c.data_cad,\n    c.cpf,\n\n    -- Campos calculados\n    TIMESTAMPDIFF(YEAR, c.data_nasc, CURRENT_DATE) as idade_atual,\n    CASE \n        WHEN TIMESTAMPDIFF(YEAR, c.data_nasc, CURRENT_DATE) &lt; 25 THEN 'Jovem'\n        WHEN TIMESTAMPDIFF(YEAR, c.data_nasc, CURRENT_DATE) &lt; 45 THEN 'Adulto'\n        WHEN TIMESTAMPDIFF(YEAR, c.data_nasc, CURRENT_DATE) &lt; 65 THEN 'Meia-idade'\n        ELSE 'Senior'\n    END as faixa_etaria,\n\n    -- Informa\u00e7\u00f5es geogr\u00e1ficas (desnormalizadas)\n    cid.cidade_desc as cidade,\n    cid.cidade_uf as uf,\n    pa.pais,\n\n    -- Auditoria\n    c.created_at,\n    c.updated_at\n\nFROM cliente c\nJOIN cidade cid ON c.id_cidade = cid.id_cidade\nJOIN pais pa ON cid.id_pais = pa.id_pais;\n</code></pre> <p>A dimens\u00e3o produto, com categoriza\u00e7\u00e3o adequada:</p> <pre><code>CREATE TABLE dim_produto AS\nSELECT \n    p.id_produto,\n    p.nome,\n    p.descricao,\n    p.preco_base,\n    p.ativo,\n\n    -- Categoriza\u00e7\u00e3o baseada no nome (exemplo simples)\n    CASE \n        WHEN LOWER(p.nome) LIKE '%eletr\u00f4nico%' THEN 'Eletr\u00f4nicos'\n        WHEN LOWER(p.nome) LIKE '%livro%' THEN 'Livros'\n        WHEN LOWER(p.nome) LIKE '%roupa%' THEN 'Vestu\u00e1rio'\n        ELSE 'Outros'\n    END as categoria,\n\n    -- Classifica\u00e7\u00e3o por faixa de pre\u00e7o\n    CASE \n        WHEN p.preco_base &lt; 50 THEN 'Baixo'\n        WHEN p.preco_base &lt; 200 THEN 'M\u00e9dio'\n        ELSE 'Alto'\n    END as faixa_preco,\n\n    p.created_at,\n    p.updated_at\n\nFROM produto p;\n</code></pre> <p>E a dimens\u00e3o tempo:</p> <pre><code>-- Criar dimens\u00e3o tempo para os \u00faltimos 5 anos\nCREATE TABLE dim_tempo AS\nWITH RECURSIVE date_range AS (\n    SELECT DATE('2020-01-01') as data\n    UNION ALL\n    SELECT DATE_ADD(data, INTERVAL 1 DAY)\n    FROM date_range\n    WHERE data &lt; DATE('2025-12-31')\n)\nSELECT \n    data,\n    YEAR(data) as ano,\n    MONTH(data) as mes,\n    MONTHNAME(data) as mes_nome,\n    QUARTER(data) as trimestre,\n    CASE WHEN MONTH(data) &lt;= 6 THEN 1 ELSE 2 END as semestre,\n    DAY(data) as dia_mes,\n    DAYOFYEAR(data) as dia_ano,\n    WEEKDAY(data) + 1 as dia_semana,\n    DAYNAME(data) as dia_semana_nome,\n    CASE WHEN WEEKDAY(data) &gt;= 5 THEN TRUE ELSE FALSE END as eh_fim_semana\nFROM date_range;\n</code></pre>"},{"location":"classes/07-data-modeling/star-schema/#tabela-fato_1","title":"Tabela Fato","text":"<p>Para a tabela <code>fato_vendas</code>, a cria\u00e7\u00e3o poderia ser realizada com a seguinte query:</p> <pre><code>CREATE TABLE fato_vendas AS\nSELECT \n    -- Chaves para dimens\u00f5es\n    v.id_venda,\n    iv.id_item,\n    v.id_cliente,\n    iv.id_produto,\n    v.data as data_venda,\n\n    -- Medidas\n    iv.quantidade,\n    iv.valor_unitario,\n    iv.valor_total,\n    v.valor_total as valor_total_venda,\n\n    -- Medidas derivadas\n    (iv.valor_unitario * iv.quantidade) as receita_item,\n\n    -- Status\n    v.entregue,\n\n    -- Auditoria\n    v.created_at as venda_created_at\n\nFROM item_venda iv\nJOIN venda v ON iv.id_venda = v.id_venda;\n</code></pre>"},{"location":"classes/07-data-modeling/star-schema/#consultando-o-star-schema","title":"Consultando o Star Schema","text":"<p>Com o Star Schema implementado, as consultas anal\u00edticas tornam-se mais diretas:</p> <pre><code>-- Vendas por pa\u00eds e trimestre\nSELECT \n    dc.pais,\n    dt.ano,\n    dt.trimestre,\n    SUM(fv.valor_total) as receita_total,\n    COUNT(DISTINCT fv.id_venda) as total_vendas\nFROM fato_vendas fv\nJOIN dim_cliente dc ON fv.id_cliente = dc.id_cliente\nJOIN dim_tempo dt ON fv.data_venda = dt.data\nGROUP BY dc.pais, dt.ano, dt.trimestre\nORDER BY dt.ano, dt.trimestre, receita_total DESC;\n</code></pre> <pre><code>-- An\u00e1lise de produtos por faixa et\u00e1ria\nSELECT \n    dp.categoria,\n    dc.faixa_etaria,\n    SUM(fv.quantidade) as quantidade_total,\n    SUM(fv.valor_total) as receita_total,\n    AVG(fv.valor_unitario) as preco_medio\nFROM fato_vendas fv\nJOIN dim_produto dp ON fv.id_produto = dp.id_produto\nJOIN dim_cliente dc ON fv.id_cliente = dc.id_cliente\nGROUP BY dp.categoria, dc.faixa_etaria\nORDER BY receita_total DESC;\n</code></pre> <p>Question</p> <p>Em um Star Schema, onde geralmente ficam os campos que s\u00e3o usados em filtros <code>WHERE</code> das consultas anal\u00edticas?</p> Apenas na tabela fato Principalmente nas tabelas dimens\u00e3o Distribu\u00eddos igualmente Apenas em \u00edndices Submit"},{"location":"classes/07-data-modeling/star-schema/#desvantagens","title":"Desvantagens","text":"<p>Como desvantagens do Star Schema, podemos citar:</p> <ul> <li> <p>Desnormaliza\u00e7\u00e3o Parcial: Embora menor que na OBT, ainda existe redund\u00e2ncia nas tabelas dimens\u00e3o.</p> </li> <li> <p>Complexidade de Manuten\u00e7\u00e3o das Dimens\u00f5es: Mudan\u00e7as em dados mestres (como altera\u00e7\u00e3o do nome de um cliente) ainda requerem atualiza\u00e7\u00f5es nas dimens\u00f5es, embora em menor escala que na OBT.</p> </li> <li> <p>Limita\u00e7\u00f5es em Hierarquias Complexas: Hierarquias muito profundas podem tornar as tabelas dimens\u00e3o extensas e dif\u00edceis de manter.</p> </li> </ul>"},{"location":"classes/07-data-modeling/star-schema/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>Um desafio importante no Star Schema \u00e9 como lidar com mudan\u00e7as nas dimens\u00f5es ao longo do tempo. Por exemplo, o que acontece quando um cliente muda de cidade?</p> <ul> <li> <p>SCD Tipo 1: Sobrescrever. O registro \u00e9 atualizado ou sobrescrito. <pre><code>-- Cliente mudou de cidade: atualiza e perde hist\u00f3rico\nUPDATE dim_cliente \nSET cidade = 'Nova Cidade', uf = 'SP' \nWHERE id_cliente = 123;\n</code></pre></p> </li> <li> <p>SCD Tipo 2: Versionamento. A linha anterior \u00e9 inativada e uma nova linha \u00e9 criada. <pre><code>-- Mant\u00e9m hist\u00f3rico criando nova vers\u00e3o\nINSERT INTO dim_cliente (\n    id_cliente_original, nome, cidade, uf, \n    data_inicio, data_fim, versao_atual\n) VALUES (\n    123, 'Jo\u00e3o Silva', 'Nova Cidade', 'SP', \n    '2025-01-01', '9999-12-31', TRUE\n);\n\n-- Inativa vers\u00e3o anterior\nUPDATE dim_cliente \nSET data_fim = '2024-12-31', versao_atual = FALSE \nWHERE id_cliente_original = 123 AND versao_atual = TRUE;\n</code></pre></p> </li> </ul> <p>Question</p> <p>Para an\u00e1lises hist\u00f3ricas de vendas, qual tipo de SCD seria mais apropriado para a dimens\u00e3o cliente?</p> SCD Tipo 1, pela simplicidade SCD Tipo 2, para manter hist\u00f3rico Depende do volume de dados Qualquer um serve Submit"},{"location":"classes/07-data-modeling/star-schema/#star-schema-vs-outras-abordagens","title":"Star Schema vs Outras Abordagens","text":""},{"location":"classes/07-data-modeling/star-schema/#comparacao-resumida","title":"Compara\u00e7\u00e3o Resumida","text":"Aspecto Modelo Normalizado Star Schema OBT JOINs em consultas Muitos (5-8) Poucos (2-4) Nenhum Redund\u00e2ncia M\u00ednima Moderada M\u00e1xima Performance de leitura Lenta Boa Excelente Facilidade de atualiza\u00e7\u00e3o Alta Moderada Dif\u00edcil Uso de espa\u00e7o M\u00ednimo Moderado M\u00e1ximo Intuitividade para analistas Baixa Alta Muito Alta <p>Question</p> <p>Para um DW que precisa ser atualizado diariamente com milh\u00f5es de transa\u00e7\u00f5es, qual abordagem seria mais adequada?</p> Modelo completamente normalizado Star Schema com SCD apropriado OBT atualizada integralmente H\u00edbrido de acordo com a consulta Submit"},{"location":"classes/08-interview-01/faq/","title":"FAQ - Entrevista 01","text":""},{"location":"classes/08-interview-01/faq/#faq","title":"FAQ","text":"<ol> <li> <p>Posso fazer a entrevista de casa?</p> <p>N\u00e3o. As entrevistas s\u00e3o presenciais e s\u00edncronas. Devem ser feitas durante o hor\u00e1rio da aula.</p> </li> <li> <p>E se eu n\u00e3o puder comparecer \u00e0 aula?</p> <p>Veja as regras para atividades n\u00e3o entregues Aqui.</p> </li> <li> <p>Como ser\u00e1 a din\u00e2mica das entrevistas?</p> <p>As entrevistas ser\u00e3o em duplas. O entrevistador far\u00e1 perguntas e o entrevistado deve respond\u00ea-las.</p> </li> <li> <p>Quem ser\u00e1 o entrevistador?</p> <p>Voc\u00ea e seu parceiro v\u00e3o alternar, \u00e0s vezes sendo o entrevistador, \u00e0s vezes sendo o entrevistado.</p> </li> <li> <p>O entrevistador deve criar perguntas na hora?</p> <p>N\u00e3o. Haver\u00e1 um material de apoio que ir\u00e1 guiar as entrevistas, contendo: etapas simples (como cumprimentar), perguntas do entrevistador e uma rubrica para avaliar as respostas.</p> </li> <li> <p>Quem ir\u00e1 avaliar as respostas?</p> <p>Quem estiver desempenhando o papel de entrevistador.</p> </li> <li> <p>Vou precisar escrever as respostas?</p> <p>A princ\u00edpio, n\u00e3o. Talvez voc\u00ea precise desenhar no quadro (diagramas, pseudo-c\u00f3digo)!</p> </li> <li> <p>Poderei pesquisar no Google durante a entrevista?</p> <p>N\u00e3o.</p> </li> <li> <p>Como posso me preparar para as entrevistas?</p> <p>Utilize os materiais das aulas e fa\u00e7a pesquisas extras para se aprofundar nos temas.</p> </li> </ol>"},{"location":"classes/08-interview-01/faq/#tema-da-entrevista","title":"Tema da entrevista","text":"<p>Vamos focar principalmente nos assuntos estudados nas aulas anteriores. Por exemplo, a Entrevista 01 ir\u00e1 focar em:</p> <ul> <li>Introdu\u00e7\u00e3o \u00e0 Engenharia de dados</li> <li>Docker e filas</li> <li>Data Warehousing</li> <li>Computa\u00e7\u00e3o em nuvem</li> <li>Infraestrutura como c\u00f3digo</li> <li>Orquestra\u00e7\u00e3o de dados</li> </ul> <p>Al\u00e9m disso, algumas perguntas da entrevista v\u00e3o al\u00e9m das habilidades praticadas nas aulas de Engenharia de Dados, para que voc\u00ea pratique habilidades adquiridas em outros cursos.</p> <p>Por exemplo, no projeto final de Ci\u00eancia dos Dados voc\u00ea usou bastante habilidades de comunica\u00e7\u00e3o. Pode haver perguntas que exijam, por exemplo, explicar um conceito para um gestor.</p>"},{"location":"classes/08-interview-01/int01/","title":"Interview 01","text":"<p>Leia o FAQ antes de prosseguir.</p> <p>Depois, encontre um colega para fazer a entrevista com voc\u00ea.</p> <p>Decida com seu colega quem ser\u00e1 o Entrevistador 1 ou o Entrevistador 2.</p> <p>Depois, acesse o formul\u00e1rio correspondente ao seu papel!</p> <ul> <li> <p>Entrevistador Um: Acesse este formul\u00e1rio se voc\u00ea for o Entrevistador Um</p> </li> <li> <p>Entrevistador Dois: Acesse este formul\u00e1rio se voc\u00ea for o Entrevistador Dois</p> </li> </ul>"},{"location":"classes/09-data-lakes/aws-glue/","title":"AWS Glue","text":""},{"location":"classes/09-data-lakes/aws-glue/#aws-glue","title":"AWS Glue","text":""},{"location":"classes/09-data-lakes/aws-glue/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Para praticarmos com Data Lakes e ETL na nuvem, usaremos o AWS Glue. Ele \u00e9 um servi\u00e7o serverless e totalmente gerenciado de ETL (Extract, Transform, Load) que automatiza descoberta, preparo, transforma\u00e7\u00e3o e carregamento de dados a partir de m\u00faltiplas fontes.</p> <p>Prepare o bolso!</p> <p>Voc\u00ea foca na l\u00f3gica de neg\u00f3cio, a AWS cuida de infraestrutura (escalonamento, gerenciamento de recursos, monitora\u00e7\u00e3o).</p> <p>Em alto n\u00edvel, o Glue funciona como a espinha dorsal de prepara\u00e7\u00e3o de dados em um ecossistema anal\u00edtico: descobre dados, organiza metadados no Data Catalog, gera ou executa jobs Spark (Python ou Scala), orquestra depend\u00eancias e integra-se com servi\u00e7os anal\u00edticos como Athena, Redshift, EMR, QuickSight (servi\u00e7os que ainda n\u00e3o foram explorados no curso).</p>"},{"location":"classes/09-data-lakes/aws-glue/#estrutura-basica-s3","title":"Estrutura b\u00e1sica S3","text":"<p>Para explorarmos as caracter\u00edsticas principais do Glue, vamos criar um Data Lake simples em S3 e usar o Glue para catalogar, transformar e preparar dados para an\u00e1lise.</p> <p>Vimos que um Data Lake geralmente \u00e9 organizado em camadas para facilitar a gest\u00e3o e o processamento dos dados. A estrutura b\u00e1sica do bucket ser\u00e1 a seguinte:</p> <pre><code>meu-data-lake-&lt;INSPER_USERNAME&gt;/\n  raw/\n  processed/\n</code></pre> <p>Exercise</p> <p>Antes de prosseguir, fa\u00e7a login SSO na AWS com o perfil <code>dataeng</code>:</p> <pre><code>$ aws sso login --profile dataeng\n</code></pre> Mark as done <p>Exercise</p> <p>Crie um bucket no S3 com o nome <code>meu-data-lake-&lt;INSPER_USERNAME&gt;</code></p> <p>Aten\u00e7\u00e3o</p> <p>Defina corretamente a vari\u00e1vel de ambiente <code>INSPER_USERNAME</code> com seu usu\u00e1rio insper, pois o nome do bucket deve ser globalmente \u00fanico!</p> Linux/MacPowershellCMD <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3api create-bucket \\\n    --bucket meu-data-lake-$INSPER_USERNAME \\\n    --region us-east-1 \\\n    --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n\n$ aws s3api create-bucket `\n    --bucket \"meu-data-lake-$($env:INSPER_USERNAME)\" `\n    --region us-east-1 `\n    --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n\n$ aws s3api create-bucket ^\n    --bucket meu-data-lake-%INSPER_USERNAME% ^\n    --region us-east-1 ^\n    --profile dataeng\n</code></pre> Mark as done <p>Exercise</p> <p>Dentro do bucket, crie as seguintes pastas (prefixos):</p> <ul> <li><code>raw/</code>: para dados brutos, diretamente da fonte.</li> <li><code>processed/</code>: para dados limpos e transformados.</li> </ul> <p>Aten\u00e7\u00e3o</p> <p>Caso esteja na mesma sess\u00e3o do terminal, n\u00e3o precisa redefinir a vari\u00e1vel de ambiente <code>$INSPER_USERNAME</code>.</p> Linux/MacPowershellCMD <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object \\\n    --bucket meu-data-lake-$INSPER_USERNAME \\\n    --key raw/ \\\n    --profile dataeng\n</code></pre> <p> </p> <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object \\\n    --bucket meu-data-lake-$INSPER_USERNAME \\\n    --key processed/ \\\n    --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws s3api put-object `\n    --bucket \"meu-data-lake-$($env:INSPER_USERNAME)\" `\n    --key raw/ `\n    --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws s3api put-object `\n    --bucket \"meu-data-lake-$($env:INSPER_USERNAME)\" `\n    --key processed/ `\n    --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object ^\n    --bucket meu-data-lake-%INSPER_USERNAME% ^\n    --key raw/ ^\n    --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object ^\n    --bucket meu-data-lake-%INSPER_USERNAME% ^\n    --key processed/ ^\n    --profile dataeng\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique se as pastas foram criadas corretamente.</p> Linux/MacPowershellCMD <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3 ls meu-data-lake-$INSPER_USERNAME --recursive --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws s3 ls \"meu-data-lake-$($env:INSPER_USERNAME)\" --recursive --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws s3 ls meu-data-lake-%INSPER_USERNAME% --recursive --profile dataeng\n</code></pre> Mark as done"},{"location":"classes/09-data-lakes/aws-glue/#base-de-dados","title":"Base de Dados","text":"<p>Iremos utilizar a base de dados Brazilian E-Commerce Public Dataset by Olist, dispon\u00edvel no Kaggle</p> <p> Fonte da imagem: Kaggle</p> <p>Exercise</p> <p>Fa\u00e7a o download da base de dados. Crie uma pasta para a aula e descompacte os arquivos nesta pasta.</p> Mark as done <p>Agora vamos subir a base de dados de itens vendidos (<code>olist_order_items</code>):</p> <p>Exercise</p> <p>Fa\u00e7a o upload do arquivo <code>olist_order_items_dataset.csv</code> para a pasta <code>raw/olist_order_items/olist_order_items_dataset.csv</code></p> <p>Aten\u00e7\u00e3o</p> <p>Caso esteja na mesma sess\u00e3o do terminal, n\u00e3o precisa redefinir a vari\u00e1vel de ambiente <code>$INSPER_USERNAME</code>.</p> Linux/MacPowershellCMD <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object \\\n    --bucket meu-data-lake-$INSPER_USERNAME \\\n    --key raw/olist_order_items/olist_order_items_dataset.csv \\\n    --body olist_order_items_dataset.csv \\\n    --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws s3api put-object `\n    --bucket \"meu-data-lake-$($env:INSPER_USERNAME)\" `\n    --key \"raw/olist_order_items/olist_order_items_dataset.csv\" `\n    --body \"olist_order_items_dataset.csv\" `\n    --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object ^\n    --bucket meu-data-lake-%INSPER_USERNAME% ^\n    --key raw/olist_order_items/olist_order_items_dataset.csv ^\n    --body olist_order_items_dataset.csv ^\n    --profile dataeng\n</code></pre> Mark as done <p>Agora vamos subir a base de dados de products (<code>olist_products</code>):</p> <p>Exercise</p> <p>Fa\u00e7a o upload do arquivo <code>olist_products_dataset.csv</code> para a pasta <code>raw/olist_products/olist_products_dataset.csv</code></p> <p>Aten\u00e7\u00e3o</p> <p>Caso esteja na mesma sess\u00e3o do terminal, n\u00e3o precisa redefinir a vari\u00e1vel de ambiente <code>$INSPER_USERNAME</code>.</p> Linux/MacPowershellCMD <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object \\\n    --bucket meu-data-lake-$INSPER_USERNAME \\\n    --key raw/olist_products/olist_products_dataset.csv \\\n    --body olist_products_dataset.csv \\\n    --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws s3api put-object `\n    --bucket \"meu-data-lake-$($env:INSPER_USERNAME)\" `\n    --key \"raw/olist_products/olist_products_dataset.csv\" `\n    --body \"olist_products_dataset.csv\" `\n    --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws s3api put-object ^\n    --bucket meu-data-lake-%INSPER_USERNAME% ^\n    --key raw/olist_products/olist_products_dataset.csv ^\n    --body olist_products_dataset.csv ^\n    --profile dataeng\n</code></pre> Mark as done <p>Exercise</p> <p>Verifique se os arquivos foram criados corretamente.</p> Linux/MacPowershellCMD <pre><code>$ INSPER_USERNAME=seu-usuario-insper\n$ aws s3 ls meu-data-lake-$INSPER_USERNAME --recursive --profile dataeng\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws s3 ls \"meu-data-lake-$($env:INSPER_USERNAME)\" --recursive --profile dataeng\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws s3 ls meu-data-lake-%INSPER_USERNAME% --recursive --profile dataeng\n</code></pre> Mark as done"},{"location":"classes/09-data-lakes/aws-glue/#glue-caracteristicas-principais","title":"Glue: Caracter\u00edsticas Principais","text":"<p>Os componentes mais usados do Glue s\u00e3o:</p>"},{"location":"classes/09-data-lakes/aws-glue/#data-catalog-metadados","title":"Data Catalog &amp; Metadados","text":"<p>Reposit\u00f3rio centralizado de metadados (tabelas, bancos l\u00f3gicos, parti\u00e7\u00f5es, esquemas, localiza\u00e7\u00e3o no S3, formatos, propriedades). Dentre outras caracter\u00edsticas, ele:</p> <ul> <li>Permite descoberta de dados por m\u00faltiplos servi\u00e7os (Athena, Redshift Spectrum, EMR, Glue Jobs).</li> <li>Armazena evolu\u00e7\u00e3o de esquemas (schema versioning) e suporta altera\u00e7\u00e3o incremental.</li> </ul> <p>Info!</p> <p>Ao registrar tabelas no Data Catalog, elas tornam-se imediatamente consult\u00e1veis por Athena (SQL serverless) ou via Redshift Spectrum, evitando duplica\u00e7\u00e3o de metadados.</p>"},{"location":"classes/09-data-lakes/aws-glue/#crawlers","title":"Crawlers","text":"<p>Processos que inspecionam fontes (S3, JDBC, etc.) para inferir automaticamente esquemas e criar/atualizar entradas no Data Catalog. Benef\u00edcios:</p> <ul> <li>Automatizam descoberta e atualiza\u00e7\u00e3o de parti\u00e7\u00f5es (por exemplo, dados particionados por <code>ano=2025/mes=09/</code>).</li> <li>Reduzem erro humano em defini\u00e7\u00e3o manual de tipos.</li> <li>Podem ser agendados ou executados sob demanda.</li> </ul>"},{"location":"classes/09-data-lakes/aws-glue/#criar-um-crawler","title":"Criar um Crawler","text":"<p>Vamos criar um crawler para descobrir e catalogar a tabela <code>olist_order_items</code> que acabamos de subir para o S3.</p> <p>Inicialmente, criaremos um database no Glue Data Catalog para organizar nossas tabelas:</p> <p>Exercise</p> <p>Crie Database no Glue Data Catalog com o nome <code>meu-db-olist</code>. Execute este comando no terminal:</p> Linux/MacPowershellCMD <pre><code>aws glue create-database \\\n    --profile dataeng \\\n    --region us-east-1 \\\n    --database-input '{\n        \"Name\": \"meu-db-olist\",\n        \"Description\": \"Database de exemplo para Glue com base Olist\",\n        \"Parameters\": {\n        \"CreatedBy\": \"AWS CLI\"\n        }\n    }'\n</code></pre> <p>Crie um arquivo <code>db.json</code> com o seguinte conte\u00fado:</p> <pre><code>{\n    \"Name\": \"meu-db-olistasd\",\n    \"Description\": \"Database de exemplo para Glue com base Olist\",\n    \"Parameters\": { \"CreatedBy\": \"AWS CLI\" }\n}\n</code></pre> <p>Rode:</p> <pre><code>$ aws glue create-database `\n    --profile dataeng `\n    --region us-east-1 `\n    --database-input file://db.json\n</code></pre> <p>Crie um arquivo <code>db.json</code> com o seguinte conte\u00fado:</p> <pre><code>{\n    \"Name\": \"meu-db-olistasd\",\n    \"Description\": \"Database de exemplo para Glue com base Olist\",\n    \"Parameters\": { \"CreatedBy\": \"AWS CLI\" }\n}\n</code></pre> <p>Rode:</p> <pre><code>$ aws glue create-database ^\n    --profile dataeng ^\n    --region us-east-1 ^\n    --database-input file://db.json\n</code></pre> Mark as done <p>Exercise</p> <p>Acesse o console do AWS Glue e verifique se o database <code>meu-db-olist</code> foi criado corretamente.</p> <p>Utilize o menu esquerdo Data catalog &gt; Databases.</p> Mark as done <p>Agora precisamos criar uma role IAM para o Glue ter permiss\u00e3o de ler os dados do S3 e escrever no Data Catalog.</p> <p>Exercise</p> <p>Crie os arquivos <code>glue_etl_crawler_s3_role.json</code> e <code>glue_etl_crawler_s3_policy.json</code></p> <code>glue_etl_crawler_s3_role.json</code> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n        \"Service\": \"glue.amazonaws.com\"\n        },\n        \"Action\": \"sts:AssumeRole\"\n    }\n    ]\n}\n</code></pre> <p>Aten\u00e7\u00e3o</p> <p>Atualize o arquivo <code>glue_etl_crawler_s3_policy.json</code>, substituindo <code>meu-data-lake-xxx</code> pelo nome do seu bucket criado anteriormente.</p> <code>glue_etl_crawler_s3_policy.json</code> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"glue:*\",\n            \"s3:GetBucketLocation\",\n            \"s3:ListBucket\",\n            \"s3:ListAllMyBuckets\",\n            \"s3:GetBucketAcl\",\n            \"ec2:DescribeVpcEndpoints\",\n            \"ec2:DescribeRouteTables\",\n            \"ec2:CreateNetworkInterface\",\n            \"ec2:DeleteNetworkInterface\",\n            \"ec2:DescribeNetworkInterfaces\",\n            \"ec2:DescribeSecurityGroups\",\n            \"ec2:DescribeSubnets\",\n            \"ec2:DescribeVpcAttribute\",\n            \"iam:ListRolePolicies\",\n            \"iam:GetRole\",\n            \"iam:GetRolePolicy\",\n            \"cloudwatch:PutMetricData\"\n        ],\n        \"Resource\": \"*\"\n        },\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:CreateBucket\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::aws-glue-*\"\n        ]\n        },\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\",\n            \"s3:DeleteObject\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::aws-glue-*/*\",\n            \"arn:aws:s3:::*/*aws-glue-*/*\"\n        ]\n        },\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::crawler-public*\",\n            \"arn:aws:s3:::aws-glue-*\"\n        ]\n        },\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"logs:CreateLogGroup\",\n            \"logs:CreateLogStream\",\n            \"logs:PutLogEvents\"\n        ],\n        \"Resource\": [\n            \"arn:aws:logs:*:*:*:/aws-glue/*\"\n        ]\n        },\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ec2:CreateTags\",\n            \"ec2:DeleteTags\"\n        ],\n        \"Condition\": {\n            \"ForAllValues:StringEquals\": {\n            \"aws:TagKeys\": [\n                \"aws-glue-service-resource\"\n            ]\n            }\n        },\n        \"Resource\": [\n            \"arn:aws:ec2:*:*:network-interface/*\",\n            \"arn:aws:ec2:*:*:security-group/*\",\n            \"arn:aws:ec2:*:*:instance/*\"\n        ]\n        },\n        {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"s3:GetObject\",\n            \"s3:PutObject\"\n        ],\n        \"Resource\": [\n            \"arn:aws:s3:::meu-data-lake-xxx/*\"\n        ]\n        }\n    ]\n    }\n</code></pre> <p>Analise a pol\u00edtica e liste algumas permiss\u00f5es concedidas.</p> Submit <p>Exercise</p> <p>Crie a role <code>role-glue-meu-data-lake-s3</code> com a pol\u00edtica anexada <code>policy-glue-meu-data-lake-s3</code>.</p> <pre><code>$ aws iam create-role \\\n    --role-name role-glue-meu-data-lake-s3 \\\n    --assume-role-policy-document file://glue_etl_crawler_s3_role.json \\\n    --profile dataeng\n</code></pre> <p> </p> <pre><code>$ aws iam put-role-policy \\\n    --role-name role-glue-meu-data-lake-s3 \\\n    --policy-name policy-glue-meu-data-lake-s3 \\\n    --policy-document file://glue_etl_crawler_s3_policy.json \\\n    --profile dataeng\n</code></pre> Mark as done <p>Exercise</p> <p>Para criar o crawler, utilize:</p> Linux/MacPowershellCMD <pre><code>$ aws glue create-crawler \\\n    --profile dataeng \\\n    --region us-east-1 \\\n    --name meu-crawler-olist \\\n    --role role-glue-meu-data-lake-s3 \\\n    --database-name meu-db-olist \\\n    --targets \"S3Targets=[{Path=\\\"s3://meu-data-lake-$INSPER_USERNAME/raw/\\\"}]\" \\\n    --schema-change-policy UpdateBehavior=UPDATE_IN_DATABASE,DeleteBehavior=DEPRECATE_IN_DATABASE\n</code></pre> <pre><code>$ $env:INSPER_USERNAME = \"seu-usuario-insper\"\n$ aws glue create-crawler `\n    --profile dataeng `\n    --region us-east-1 `\n    --name meu-crawler-olist `\n    --role role-glue-meu-data-lake-s3 `\n    --database-name meu-db-olist `\n    --targets \"S3Targets=[{Path='s3://meu-data-lake-$($env:INSPER_USERNAME)/raw/'}]\" `\n    --schema-change-policy UpdateBehavior=UPDATE_IN_DATABASE,DeleteBehavior=DEPRECATE_IN_DATABASE\n</code></pre> <pre><code>$ set INSPER_USERNAME=seu-usuario-insper\n$ aws glue create-crawler ^\n    --profile dataeng ^\n    --region us-east-1 ^\n    --name meu-crawler-olist ^\n    --role role-glue-meu-data-lake-s3 ^\n    --database-name meu-db-olist ^\n    --targets \"S3Targets=[{Path=\\\"s3://meu-data-lake-%INSPER_USERNAME%/raw/\\\"}]\" ^\n    --schema-change-policy UpdateBehavior=UPDATE_IN_DATABASE,DeleteBehavior=DEPRECATE_IN_DATABASE\n</code></pre> Mark as done <p>Exercise</p> <p>Acesse o console do AWS Glue e verifique se o crawler <code>meu-crawler-olist</code> foi criado corretamente.</p> <p>Utilize o menu esquerdo Data catalog &gt; Crawlers.</p> Mark as done <p>Exercise</p> <p>No menu esquerdo Data catalog &gt; Crawlers, selecione seu crawler <code>meu-crawler-olist</code> e clique no bot\u00e3o Run crawler.</p> <p>Dica!</p> <p>Caso queira fazer por AWS CLI, utilize:</p> <pre><code>$ aws glue start-crawler \\\n    --profile dataeng \\\n    --region us-east-1 \\\n    --name meu-crawler-olist\n</code></pre> Mark as done <p>Exercise</p> <p>Espere o t\u00e9rmino da execu\u00e7\u00e3o do crawler. Atualize a p\u00e1gina e verifique se o status mudou para Ready.</p> Mark as done <p>Exercise</p> <p>No menu esquerdo Data catalog &gt; Databases &gt; Tables, verifique se as tabelas foram criadas corretamente.</p> <p>Aten\u00e7\u00e3o</p> <p>Caso n\u00e3o obtenha sucesso, nos Crawler runs, verifique os logs (AWS Cloudwatch) para entender o que ocorreu.</p> Mark as done"},{"location":"classes/09-data-lakes/aws-glue/#athena","title":"Athena","text":"<p>Como desejamos consultar os dados catalogados, vamos acessar o Athena.</p> <p>Exercise</p> <p>Acesse o console do AWS Athena e clique no bot\u00e3o Iniciar consulta.</p> <p></p> Mark as done <p>Em seguida, no menu esquerdo, escolha o database <code>meu-db-olist</code> criado anteriormente.</p> <p></p> <p>No editor de consultas, teste algumas consultas envolvendo as duas tabelas.</p> <p>Query result location</p> <p>Caso o Athena solicite uma pasta para salvar os resultados das consultas, informe a pasta <code>athena-results/</code> dentro do seu bucket.</p> <p>Por exemplo: <code>s3://meu-data-lake-&lt;INSPER_USERNAME&gt;/athena-results/</code></p> <pre><code>SELECT COUNT(*) AS qtde_itens_vendidos,\n      SUM(price) AS vlr_total_vendido\nFROM olist_order_items;\n\nSELECT * FROM olist_order_items LIMIT 5;\n</code></pre> <p>Dica!</p> <p>Caso digite m\u00faltiplas queries, selecione a query que deseja executar e clique no bot\u00e3o Run again ou aperte Ctrl + Enter.</p> <p></p> <p>Pronto! Desta maneira, conseguimos catalogar e consultar dados brutos no S3 usando o Glue e o Athena.</p> <p>Exercise</p> <p>Quantas linhas tem cada uma das duas tabelas catalogadas?</p> Submit <p>Answer</p> <ul> <li><code>olist_order_items</code>: 112650 linhas</li> <li><code>olist_products</code>: 32951 linhas</li> </ul> <pre><code>SELECT COUNT(*) FROM olist_order_items; -- 112650\nSELECT COUNT(*) FROM olist_products; -- 32951\n</code></pre> <p>Exercise</p> <p>Quais as cinco categorias de produtos mais vendidas (maior quantidade de itens vendidos)?</p> Submit <p>Answer</p> <pre><code>SELECT p.product_category_name, COUNT(*) AS qtde_itens_vendidos\nFROM olist_order_items oi\nJOIN olist_products p ON oi.product_id = p.product_id\nGROUP BY p.product_category_name\nORDER BY qtde_itens_vendidos DESC\nLIMIT 5;\n</code></pre> product_category_name qtde_itens_vendidos cama_mesa_banho 11115 beleza_saude 9670 esporte_lazer 8641 moveis_decoracao 8334 informatica_acessorios 7827"},{"location":"classes/09-data-lakes/aws-glue/#jobs-de-etl","title":"Jobs de ETL","text":"<p>Os jobs de ETL no AWS Glue s\u00e3o processos automatizados que permitem extrair dados de diferentes fontes, realizar transforma\u00e7\u00f5es necess\u00e1rias e carregar os resultados em destinos apropriados, como data lakes ou data warehouses. Eles s\u00e3o essenciais para organizar, limpar e preparar grandes volumes de dados de forma eficiente, sem a necessidade de gerenciar infraestrutura manualmente.</p> <p>Utilizamos jobs de ETL do Glue quando precisamos integrar dados de m\u00faltiplos sistemas, padronizar formatos, enriquecer informa\u00e7\u00f5es ou criar pipelines anal\u00edticos que exigem escalabilidade e automa\u00e7\u00e3o. Cen\u00e1rios comuns incluem a convers\u00e3o de arquivos brutos em formatos otimizados para an\u00e1lise, a atualiza\u00e7\u00e3o incremental de tabelas, ou a prepara\u00e7\u00e3o de dados para visualiza\u00e7\u00e3o e explora\u00e7\u00e3o por ferramentas como Athena ou Redshift.</p> <p>Vendedor da AWS!</p> <p>O Glue facilita o trabalho de engenharia de dados, tornando o processo mais \u00e1gil, seguro e integrado ao ecossistema AWS.</p> <p>Os jobs de ETL no Glue leem, transformam e escrevem dados. S\u00e3o programados em Python (PySpark) ou Scala.</p> <p>Vamos criar um job* que l\u00ea a tabela <code>olist_order_items</code> e grava o resultado em formato Parquet na pasta <code>processed/</code>.</p> <p>Este passo ser\u00e1 realizado de forma manual, pelo editor visual do Glue Studio.</p> <p>Exercise</p> <p>Acesse o console do Glue e clique em ETL jobs &gt; Visual ETL.</p> <p>A direta, clique em Create job (Visual ETL).</p> Mark as done <p></p> <p>Exercise</p> <p>Clique no l\u00e1pis para editar o nome do job</p> Mark as done <p>As sources representam as fontes de dados de entrada que o job ir\u00e1 processar.</p> <p>Exercise</p> <p>Clique no + (Add nodes) e explore as op\u00e7\u00f5es de configura\u00e7\u00e3o de sources do job.</p> <p>Cite uma ou duas que chamaram sua aten\u00e7\u00e3o!</p> Submit <p>Exercise</p> <p>Explore as demais abas (Transforms, Targets) do Add nodes.</p> Mark as done <p>Exercise</p> <p>Adicione uma source do tipo S3.</p> <p>D\u00ea dois cliques na caixa que representa a source para editar suas propriedades.</p> <p>Configure a origem para ler a tabela <code>olist_order_items</code> do Data Catalog:</p> <p></p> Mark as done <p>Exercise</p> <p>Em seguida, adicione um Transform - Change schema.</p> <p>Configure tipos adequados para as colunas e remova a coluna <code>seller_id</code> (marque a op\u00e7\u00e3o \"Drop column\" para simular que ela n\u00e3o \u00e9 relevante para nossa an\u00e1lise).</p> Mark as done <p>Exercise</p> <p>Em seguida, adicione um Transform - Filter.</p> <p>Configure o filtro para manter apenas os itens com <code>price &gt; 100</code> (filtragem pushdown predicate para otimizar leitura).</p> <p></p> Mark as done <p>Exercise</p> <p>Adicione tamb\u00e9m um Data Target - S3 Bucket.</p> <p>Configure o destino para gravar os resultados na pasta <code>processed/</code> em formato Parquet.</p> <p>Aten\u00e7\u00e3o</p> <p>Lembre-se de substituir <code>&lt;INSPER_USERNAME&gt;</code> pelo seu usu\u00e1rio do Insper.</p> <p>S3 Target Location: <code>s3://meu-data-lake-&lt;INSPER_USERNAME&gt;/processed/order-items/</code></p> <p></p> Mark as done <p>Exercise</p> <p>Na aba Job details, selecione a IAM role criada anteriormente.</p> <p>Pode utilizar a mesma role criada para o crawler.</p> Mark as done <p>Exercise</p> <p>Na aba Job details, marque a op\u00e7\u00e3o Automatically scale the number of workers.</p> <p>Configure o menor Worker type poss\u00edvel: <code>G.1X</code>.</p> <p>Delimite o n\u00famero m\u00e1ximo de workers para <code>2</code>.</p> <p>Configure o Job timeout (minutes) para <code>10</code>.</p> <p></p> Mark as done <p>Este \u00e9 o grafo que representa o fluxo de dados do job: </p> <p>Exercise</p> <p>Salve e execute o job!</p> <p>Acompanhe sua execu\u00e7\u00e3o para verificar se tudo ocorreu bem.</p> Mark as done <p>Exercise</p> <p>Confira no Athena. Os novos dados em Parquet j\u00e1 est\u00e3o dispon\u00edveis para consulta?</p> Sim N\u00e3o Submit <p>Answer</p> <p>N\u00e3o. Precisamos catalogar os novos dados. Mas antes, vamos atualizar nosso ETL.</p> <p>Exercise</p> <p>Crie um novo DB para conter as tabelas processadas.</p> Mark as done <p>Exercise</p> <p>Volte ao editor visual do job e adicione o ETL para a tabela de products (<code>olist_products</code>).</p> <p>Adicione um JOIN entre as duas tabelas e salve o resultado do JOIN no S3 em <code>processed/order-items-products/</code>.</p> <p>Aten\u00e7\u00e3o</p> <p>Perceba que o JOIN deve ser feito entre as colunas <code>product_id</code> de ambas as tabelas.</p> <p>O JOIN deve ser do tipo Inner Join e ser realizado antes do filtro de <code>price &gt; 100</code>.</p> <p>Aten\u00e7\u00e3o</p> <p>N\u00e3o execute o job antes de ter feito o pr\u00f3ximo exerc\u00edcio.</p> <p></p> Mark as done <p>Exercise</p> <p>Agora, em cada n\u00f3 Target - S3 Bucket, altere o Data Catalog database para o novo DB, criado anteriormente.</p> <p>Escolha a op\u00e7\u00e3o para criar uma nova tabela, selecione o DB adequado e defina nomes apropriados para as tabelas.</p> <p></p> Mark as done <p>Exercise</p> <p>Rode o job e verifique se as novas tabelas foram criadas corretamente.</p> Mark as done <p>Exercise</p> <p>No Athena, consulte a nova tabela que cont\u00e9m o resultado do JOIN.</p> Mark as done <p>Answer</p> <pre><code>SELECT COUNT(*) FROM order_items_products;\n</code></pre> <p>Exercise</p> <p>Resuma, em suas palavras, os principais objetivos do uso de AWS Glue.</p> Submit <p>Por hoje \u00e9 s\u00f3!</p>"},{"location":"classes/09-data-lakes/intro-data-lakes/","title":"Introdu\u00e7\u00e3o Data Lakes","text":""},{"location":"classes/09-data-lakes/intro-data-lakes/#introducao-data-lakes","title":"Introdu\u00e7\u00e3o Data Lakes","text":""},{"location":"classes/09-data-lakes/intro-data-lakes/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>No cen\u00e1rio atual, o engenheiro de dados nem sempre ir\u00e1 ingerir tabelas bem definidas. Logs de aplica\u00e7\u00e3o, eventos de sensores, cliques, imagens e textos competem por espa\u00e7o nas arquiteturas de dados. Uma outra alternativa para armazenar esses dados \u00e9 o Data Lake.</p> <p>Um Data Lake \u00e9 um reposit\u00f3rio de armazenamento de grande escala que mant\u00e9m dados brutos no formato nativo.</p> <p>Sua populariza\u00e7\u00e3o cresceu mais significativamente por volta de 2010, quando data warehouses relacionais passaram a sofrer com volume, variedade e velocidade de dados semi ou n\u00e3o estruturados.</p> <p>A pergunta que motivou o surgimento do Data Lake foi:</p> <p>Como guardar tudo isso agora, mesmo sem saber ainda todos os usos futuros?</p> <p>Met\u00e1fora</p> <p>Assim como um lago recebe diferentes afluentes sem tentar \"padronizar\" a \u00e1gua, o Data Lake aceita m\u00faltiplos formatos (estruturado, semi-estruturado, n\u00e3o estruturado) sem impor um esquema antecipado.</p> <p>Um Data Lake funciona como um reposit\u00f3rio centralizado que pode conter tanto c\u00f3pias n\u00e3o processadas de dados de sistemas de origem, (sensores, logs, sistemas), quanto informa\u00e7\u00f5es j\u00e1 transformadas para uso em dashboards, an\u00e1lises avan\u00e7adas ou treinamento de modelos.</p>"},{"location":"classes/09-data-lakes/intro-data-lakes/#por-que-nao-so-data-warehouse","title":"Por que n\u00e3o s\u00f3 Data Warehouse?","text":"<p>O warehouse tradicional parte de um modelo pr\u00e9-definido (schema-on-write). Isso exige decidir colunas, tipos e regras antes de carregar. Para dados novos e ca\u00f3ticos, esse atrito gera atraso. O Data Lake inverte essa l\u00f3gica: armazena agora, interpreta depois (schema-on-read). Assim, a modelagem e padroniza\u00e7\u00e3o ficam pr\u00f3ximas do momento de consumo ou de etapas de refinamento.</p> <p>Exercise</p> <p>Liste dois exemplos de fontes de dados que ficariam dif\u00edceis (ou custosos) de colocar diretamente em um data warehouse relacional sem um pr\u00e9-processamento pesado.</p> Submit <p>Answer</p> <ul> <li>Logs de servidores web, que podem ter formatos variados e mudan\u00e7as frequentes.</li> <li>Dados de sensores IoT, que geram grandes volumes de dados em formatos n\u00e3o estruturados.</li> </ul>"},{"location":"classes/09-data-lakes/intro-data-lakes/#caracteristicas","title":"Caracter\u00edsticas","text":"<p>Como caracter\u00edsticas essenciais, um Data Lake geralmente apresenta:</p> <ol> <li>Store first: ingest\u00e3o r\u00e1pida copiando arquivos (ou dados em fluxo) quase como chegam.</li> <li>Escalabilidade horizontal: usando armazenamento de objetos (ex.: S3) com custo baixo, ou seja, foco em armazenar grandes volumes de dados de forma econ\u00f4mica.</li> <li>Neutralidade de computa\u00e7\u00e3o: tem capacidade de processar e analisar dados usando diferentes engines de computa\u00e7\u00e3o e ferramentas anal\u00edticas, sem ficar restrito a uma \u00fanica tecnologia.</li> <li>Reten\u00e7\u00e3o hist\u00f3rica: raramente se apaga; mant\u00e9m trilha de eventos brutos.</li> </ol> <p>Observe que nenhuma dessas caracter\u00edsticas garante valor por si s\u00f3. Sem contexto e organiza\u00e7\u00e3o, vira apenas um reposit\u00f3rio barato.</p> <p>Exercise</p> <p>Por que reter dados brutos pode ajudar a corrigir um erro descoberto meses depois em uma transforma\u00e7\u00e3o de ETL?</p> Submit <p>Answer</p> <p>Reter dados brutos permite reprocessar os dados originais com as corre\u00e7\u00f5es necess\u00e1rias, garantindo que qualquer erro na transforma\u00e7\u00e3o anterior possa ser corrigido sem perda de informa\u00e7\u00e3o.</p>"},{"location":"classes/09-data-lakes/intro-data-lakes/#camadas-logicas","title":"Camadas L\u00f3gicas","text":"<p>Para evitar que o Data Lake se torne um p\u00e2ntano (data swamp), \u00e9 necess\u00e1rio prover uma organiza\u00e7\u00e3o m\u00ednima.</p> <p>Isto pode ser obtido ao aplicar uma separa\u00e7\u00e3o simples de camadas.</p> <p>Uma primeira divis\u00e3o poss\u00edvel \u00e9 separar dados brutos de dados processados.</p> Camada Objetivo Exemplo de conte\u00fado <code>raw</code> Captura dos dados JSON, CSV original, logs, imagens <code>processed</code> Limpeza, padroniza\u00e7\u00e3o, integra\u00e7\u00e3o Colunas tipadas, normaliza\u00e7\u00e3o de datas, arquivos de formato colunar (Parquet) <p>Uma melhor organiza\u00e7\u00e3o pode ser obtida com a separa\u00e7\u00e3o em tr\u00eas camadas.</p> Camada Objetivo Exemplo de conte\u00fado <code>raw</code> (Bronze) Captura integral e imut\u00e1vel JSON de eventos, CSV original, logs, imagens <code>cleansed</code> (Silver) Limpeza, padroniza\u00e7\u00e3o, integra\u00e7\u00e3o Colunas tipadas, normaliza\u00e7\u00e3o de datas, arquivos de formato colunar (Parquet) <code>presentation</code> (Gold) L\u00f3gica de neg\u00f3cio pronta para consumo Tabelas derivadas, fatos e dimens\u00f5es, conjuntos agregados <p>Info!</p> <p>Apesar de n\u00e3o ser a organiza\u00e7\u00e3o formal, a organiza\u00e7\u00e3o em tr\u00eas camadas (Bronze, Silver, Gold) \u00e9 um padr\u00e3o emergente, e bastante utilizada na pr\u00e1tica.</p> <p>Na pr\u00e1tica voc\u00eas ver\u00e3o essas camadas mapeadas em prefixos de caminhos (ex.: <code>s3://meu-data-lake/raw/...</code>).</p> <p>Arquitetura Medallion</p> <p>Este padr\u00e3o de camadas (Bronze, Silver, Gold) \u00e9 conhecido como Medallion Architecture e foi popularizado pela Databricks.</p> <p> Fonte: Adaptado de Databricks</p> <p>Caso queira se aprofundar, veja este artigo da Databricks</p> <p>Exercise</p> <p>D\u00ea um exemplo de transforma\u00e7\u00e3o t\u00edpica que ocorre ao mover um dataset de <code>raw</code> para <code>cleansed</code>.</p> Submit <p>Answer</p> <ul> <li>Convers\u00e3o de tipos de dados (ex.: transformar strings em datas ou n\u00fameros).</li> <li>Remo\u00e7\u00e3o de duplicatas e tratamento de valores nulos.</li> <li>Normaliza\u00e7\u00e3o de formatos (ex.: padronizar datas para um formato \u00fanico).</li> </ul> <p>Vantagem x Risco</p> <p>Um Data Lake geralmente oferece custo baixo por terabyte e flexibilidade para novos usos. Entretanto, sem governan\u00e7a m\u00ednima (nomenclatura, controle de vers\u00f5es de schema, sem metadados ricos, pol\u00edticas de reten\u00e7\u00e3o  e padr\u00f5es de qualidade) ele degrada em um conjunto ca\u00f3tico de pastas!</p> <p>Data Swamp</p> <p>Usu\u00e1rios deixam de confiar e criam c\u00f3pias paralelas.</p>"},{"location":"classes/09-data-lakes/intro-data-lakes/#checagem-rapida","title":"Checagem R\u00e1pida","text":"<p>Exercise</p> <p>Em poucas frases, compare schema-on-write e schema-on-read, destacando impacto no tempo de disponibiliza\u00e7\u00e3o dos dados.</p> Submit <p>Answer</p> <ul> <li>Schema-on-write exige definir o esquema antes de armazenar os dados, o que pode atrasar a ingest\u00e3o, mas garante consist\u00eancia imediata.</li> <li>Schema-on-read permite armazenar dados brutos rapidamente, adiando a defini\u00e7\u00e3o do esquema para o momento da leitura, o que acelera a disponibiliza\u00e7\u00e3o inicial dos dados.</li> </ul> <p>Question</p> <p>Em um Data Lake baseado em S3, n\u00e3o \u00e9 permitido utilizar Parquets, dado que arquivos Parquets possuem schema embutido.</p> Verdadeiro Falso Submit <p>Answer</p> <p>Falso. Arquivos Parquet s\u00e3o amplamente utilizados em Data Lakes por serem eficientes em termos de armazenamento e consulta.</p> <p>Ao contr\u00e1rio dos bancos de dados tradicionais que imp\u00f5em um schema fixo na escrita (schema-on-write), os Data Lakes operam com um modelo de schema-on-read. Isso significa que a estrutura dos dados (o schema) \u00e9 definida e aplicada apenas no momento em que voc\u00ea l\u00ea e processa os dados.</p> <p>Ou seja, voc\u00ea ir\u00e1 conseguir escrever o arquivo Parquet no S3 normalmente!</p> <p>Obs: dever\u00e1 haver organiza\u00e7\u00e3o m\u00ednima (camadas, cat\u00e1logo) para evitar que o lago vire um p\u00e2ntano!</p> <p>Exercise</p> <p>Proponha um conjunto m\u00ednimo de pastas para iniciar um Data Lake em S3 alinhado \u00e0s camadas discutidas.</p> Submit <p>Answer</p> <p>Um conjunto m\u00ednimo de pastas para iniciar um Data Lake em S3 poderia ser:</p> <pre><code>s3://meu-data-lake/\n    |-- raw/\n    |-- silver/\n    |-- gold/\n</code></pre> <p>Nesta introdu\u00e7\u00e3o, definimos Data Lake, justificamos sua exist\u00eancia, diferenciamos de um warehouse, discutimos camadas e riscos. Com isso, voc\u00eas est\u00e3o prontos para experimentar na pr\u00e1tica! Siga para a pr\u00f3xima se\u00e7\u00e3o.</p>"},{"location":"project/project/","title":"Projeto Final","text":"<p>To be released...</p>"}]}